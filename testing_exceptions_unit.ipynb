{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert block alert-info alert\">\n",
    "\n",
    "# <center> Scientific Programming in Python\n",
    "\n",
    "## <center>Karl N. Kirschner<br>Bonn-Rhein-Sieg University of Applied Sciences<br>Sankt Augustin, Germany\n",
    "\n",
    "# <center> Testing Code\n",
    "\n",
    "<!-- <br><br> -->\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "**Note**: All user-defined functions in the notebook do not include document strings (i.e. block comments) or internal checks. This is purposely done to focus on the teaching aspects of the lecture. **A full and proper user-defined function would include these.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context: The Hidden Cost of Untested Code\n",
    "\n",
    "\n",
    "**The \"I Know It Works\" Illusion**\n",
    "- Testing code mentally or with a few temporary `print()` statements:\n",
    "    - Works **today** and for this **one case**,\n",
    "    - But tomorrow may be different: a colleague changes a dependency, or the code is refactored.\n",
    "        - (That small, manual `print()` statement check is lost or bypassed.)\n",
    "\n",
    "**Take-Home Message**\n",
    "- Untested code is problematic:\n",
    "    - Reduced confidence in it\n",
    "    - Results in fear of making changes\n",
    "    - Costly debugging\n",
    "\n",
    "<br>\n",
    "\n",
    "- Testing <b>flips</b> this dynamic\n",
    "    - Gives you <b>confidence</b> to <b>refactor</b>, upgrade, and scale your code\n",
    "        - If you break something, the tests will catch it\n",
    "    - <b>Debugging</b> is <b>faster</b> and <b>easier</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Unit Tests\n",
    "\n",
    "- An <b>'automated'</b> procedure to verify a **small**, **isolated** code piece (i.e., a **unit**).\n",
    "    - Make sure that the unit works as intended under different conditions.\n",
    " \n",
    "## Unit Tests Characteristics\n",
    "\n",
    "- <font color='DodgerBlue'>**Isolation**</font>\n",
    "    - Focuses on the <b>smallest</b> testable <b>part</b> of the <b>code</b> (e.g., a single function).\n",
    "    - It must be <b>run</b> in <b>isolation</b> (i.e., it doesn't rely on external factors like databases).\n",
    "\n",
    "- <font color='DodgerBlue'>**Assertion / Raise Statements**</font>\n",
    "    - Every unit test contains one or more <b>assertions/raise statements</b>.\n",
    "        - An assertion is a conditional check (e.g., `assert calculated_value == expected_value`).\n",
    "        - If it fails, the <b>test stops</b> and <b>reports an error</b>.\n",
    "\n",
    "- <font color='DodgerBlue'>Automation</font>\n",
    "    - The test is code itself.\n",
    "    - It can be run instantly and repeatedly by a testing framework (e.g., <b>pytest</b>, unittest).\n",
    "    - This is crucial for speed and consistency.\n",
    "\n",
    "- <font color='DodgerBlue'>Speed</font>\n",
    "    - Unit tests must execute very quickly (milliseconds).\n",
    "        - They will be run constantly by developers (or automated build systems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assert vs. Raise\n",
    "\n",
    "- Comparing <font color='DodgerBlue'>two floats</font> to be within a <font color='DodgerBlue'>tolerance</font> range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 0.001\n",
    "\n",
    "calculated_number = 0.115\n",
    "expected_number = 0.117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard `assert`: raises an AssertionError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition\n",
    "- `abs(calculated_number - expected_number)` must be **less than** the `tolerance` value\n",
    "- If `True`, then continue.\n",
    "- If `False`, then **raise error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(calculated_number - expected_number) <= tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert abs(calculated_number - expected_number) <= tolerance, \"Values are too far apart.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom `raise`: Checks the condition, explicitly raises an exception on failure\n",
    "\n",
    "- `abs(calculated_number - expected_number)` is **greater than** the `tolerance` value\n",
    "- If `True`, then **raise error**:\n",
    "- If `False`, then continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(calculated_number - expected_number) >= tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if abs(calculated_number - expected_number) >= tolerance:\n",
    "    raise AssertionError(f\"Error: Expected value was {expected_number:.3f}, but obtained {calculated_number:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grade(score, max_score=100):\n",
    "    \"\"\" Calculates the percentage grade from a score. \"\"\"\n",
    "\n",
    "    percentage = (score / max_score) * 100\n",
    "\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_grade(80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look correct - great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Test - Raising Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_calculate_grade():\n",
    "    \"\"\" Verifies the function's output for known, good data. \"\"\"\n",
    "\n",
    "    # 1. Standard case check (80/100 should be 80.0)\n",
    "    assert calculate_grade(80) == 80.0, \"The calculate_grade function does not work for when max_score is 100.\"\n",
    "\n",
    "    # 2. Edge case check (max_score is not 100)\n",
    "    assert calculate_grade(25, max_score=50) == 50.0, \"The calculate_grade function does not work for when max_score is not 100.\"\n",
    "\n",
    "    print(\"Test Passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_calculate_grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add a `try`-`except` statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_calculate_grade()\n",
    "except AssertionError as error:\n",
    "    print(\"TEST FAILED! The test caught a bug that the simple manual check missed.\")\n",
    "    print(f\"Assert Failure Details: {error}\")\n",
    "    print(\"This is the power of a Unit Test: immediate feedback and regression prevention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario\n",
    "Now, let's say in 10 months a colleague moves the 100 to a global position (i.e. outside the user-defined function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grade(score, max_score=100):\n",
    "    \"\"\" Buggy - calculates the percentage grade from a score. \"\"\"\n",
    "\n",
    "    # BUG: Forgot to multiply by 100\n",
    "    percentage = (score / max_score) \n",
    "\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will still use our `test_calculate_grade()` function, but now it will test the <b>new buggy</b> `calculate_grade()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_calculate_grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will introduce the `try-except` statement for code <b>error handling</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_calculate_grade()\n",
    "except AssertionError as error:\n",
    "    print(\"TEST FAILED! The test caught a bug that the simple manual check missed.\")\n",
    "    print(f\"Assert Failure Details: {error}\")\n",
    "    print(\"This is the power of a Unit Test: immediate feedback and regression prevention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The `try`-`except` Statement - to Handle Errors\n",
    "- Tells your code to try something, and\n",
    "- then tells it what to do if it fails based on an <b>exception type</b>\n",
    "\n",
    " \n",
    "#### Strengths:\n",
    "1. The code will <font color='DodgerBlue'>**continue**</font> to run, even when it encounters a problem.\n",
    "\n",
    "   - This prevents the program from crashing (e.g., if a test fails).<br><br>\n",
    "\n",
    "2. <font color='DodgerBlue'>**Faster** than if statements</font> for when <font color='DodgerBlue'>majority of the planned tasks are **expected** to be **successful**</font> (i.e., they don't encounter an exception)\n",
    "\n",
    "<br><br>\n",
    "**Simple Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(5/0)\n",
    "except ZeroDivisionError:\n",
    "    print(f\"Error: You can't have a zero in the denominator.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A More Sophisticated Example**\n",
    "\n",
    "Let's set up a <font color='DodgerBlue'>division calculator</font> that allows users to input numbers and quit at any time using while and if loops (to demonstrate via a comparison of code).\n",
    "\n",
    "1. First, set something up without `try`-`except` in order to see its advantage later.\n",
    "2. Second, do the same thing with `try`-`except`\n",
    "\n",
    "<font color='DodgerBlue'>Demonstrate the following</font>:\n",
    "1. normal operation\n",
    "2. exiting by typing 'q'\n",
    "3. <font color='Red'>O</font> (i.e., a capital alphabet letter \"O\", as in \"O\"ktoberfest (also demonstrates traceback error))\n",
    "\n",
    "Without `try`-`except` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print('Type two numbers that you want to be divided.')\n",
    "print(\"Type 'q' to quit.\")\n",
    "print()\n",
    "\n",
    "while True:\n",
    "    numerator = input('Numerator = ')\n",
    "    if numerator == 'q':\n",
    "        break\n",
    "\n",
    "    denominator = input('Denominator = ')\n",
    "    if denominator == 'q':\n",
    "        break\n",
    "\n",
    "    if denominator == '0':\n",
    "        print(\"You can't have a zero in the denominator.\")\n",
    "        break\n",
    "\n",
    "    answer = float(numerator)/float(denominator)\n",
    "    print(f'Answer for {numerator}/{denominator} = {answer}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the above code to use a **`try`-`except` statement**, and try it with <font color='Red'>O</font>.\n",
    "\n",
    "**Note**: <font color='DodgerBlue'>Multiple `except` conditions via a **tuple**</font>:<br>\n",
    "`except (ZeroDivisionError, ValueError):`\n",
    "- `ZeroDivisionError` when the denominator is zero\n",
    "- `ValueError` for when a string is given as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Type two numbers that you want to be divided.')\n",
    "print(\"Type 'q' to quit.\")\n",
    "print()\n",
    "\n",
    "while True:\n",
    "    numerator = input('Numerator = ')\n",
    "    if numerator == 'q':\n",
    "        break\n",
    "\n",
    "    denominator = input('Denominator = ')\n",
    "    if denominator == 'q':\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        answer = float(numerator)/float(denominator)\n",
    "        print(f'Answer for {numerator}/{denominator} = {answer}\\n')\n",
    "\n",
    "    except (ZeroDivisionError, ValueError):\n",
    "        print('Your input was either not a number, or you are dividing by a zero.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>Now the code continues to run, even though an error was raised!</font>\n",
    "\n",
    "### Warning - Be Careful when Using `try-except` Statements\n",
    "- They will mask or ignore potential bugs\n",
    "\n",
    "<b>Scenario</b>: Creating a dungeon crawler game\n",
    "\n",
    "#### Bad Practice: Demonstrates Error Hiding and False Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = {1: None, 2: None, 3: None, 4: None, 5: None}\n",
    "equipment_list = ['ax', 'potion', 'wand', 'dust', 'sword']\n",
    "\n",
    "index = 0\n",
    "\n",
    "while index <= 5:\n",
    "    index += 1\n",
    "    try:\n",
    "        players[index + 1] = equipment_list[index] \n",
    "    except IndexError:\n",
    "        break  # Stop the loop, hiding the fact that not all players were assigned.\n",
    "\n",
    "# The data is corrupt/incomplete, but the program claims success.\n",
    "print(\"Program finished execution.\")\n",
    "print(\"Success: Program finished execution with all players assigned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Player data (corrupt):\\n{players}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Notice</b>: <b>Player 1</b> has <b>no</b> equipment assigned.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Better Practice: Allows logical errors to stop the algorithm.\n",
    "\n",
    "- Allow the program to error, thus notifying the programmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = {1: None, 2: None, 3: None, 4: None, 5: None}\n",
    "equipment_list = ['ax', 'potion', 'wand', 'dust', 'sword']\n",
    "\n",
    "index = 0\n",
    "\n",
    "while index <= 5:\n",
    "    # Logical Bug: index will reach 5, which is out of range for equipment_list.\n",
    "    # The assignment below will fail when index = 5.\n",
    "    index += 1\n",
    "    players[index + 1] = equipment_list[index] \n",
    "    \n",
    "print(\"Program finished execution.\")\n",
    "print(\"Success: Program finished execution with all players assigned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Best Solution: correct, clear, and robust\n",
    "\n",
    "To show how the problem could be coded well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = {1: None, 2: None, 3: None, 4: None, 5: None}\n",
    "equipment_list = ['ax', 'potion', 'wand', 'dust', 'sword']\n",
    "\n",
    "for index in range(len(equipment_list)): # Correctly iterates from 0 to 4\n",
    "    players[index + 1] = equipment_list[index] \n",
    "\n",
    "# Verification\n",
    "if None in players.values():\n",
    "    print(\"Problem: Program finished execution with UNASSIGNED players.\")\n",
    "else:\n",
    "    print(\"Program finished execution.\")\n",
    "    print(\"Success: Program finished execution with all players assigned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Player data (uncorrupted):\\n{players}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "<h1 align='center'>Test Driven Development\n",
    "    \n",
    "<h2 align='center'> a.k.a. Unit Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/library/unittest.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Driven Development - writing tests before you write your production code\n",
    "1. Ensures proper and directed functionality of your code\n",
    " - creating **concise** code that does a **single** thing (e.g., user-defined functions)\n",
    "2. Helps you plan your code - what do you **actually want** to do (critical thinking)\n",
    "3. Reduces **errors**\n",
    "4. Ensures **reproducibility**\n",
    "5. Helps to ensure a code's **long life**\n",
    "\n",
    "## The Workflow Concept\n",
    "1. Write a failing test\n",
    "2. Run and ensure failure\n",
    "3. Write code to pass\n",
    "4. Run and ensure passing\n",
    "5. Refactor (i.e., restructure/clean up code without changing it final result)\n",
    "6. Redo steps 1-5\n",
    "\n",
    "## Scientific and Data Research\n",
    "It is **CRITICAL** that:\n",
    "1. you get the correct results\n",
    "2. you make it generate reproducible results, especially as the code becomes bigger (and changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assert statements that can be used in unittest library\n",
    "\n",
    "https://docs.python.org/3/library/unittest.html#module-unittest\n",
    "\n",
    "\n",
    "| Method | Checks that | New in |\n",
    "|:------|:-:|:-:|\n",
    "| assertEqual(a, b) | a == b | |\n",
    "| assertNotEqual(a, b)| a != b | |\n",
    "| assertTrue(x) | bool(x) is True | |\n",
    "| assertFalse(x) | bool(x) is False | |\n",
    "| assertIs(a, b) | a is b | 3.1 |\n",
    "| assertIsNot(a, b) | a is not b | 3.1 |\n",
    "| assertIsNone(x) | x is None | 3.1 |\n",
    "| assertIsNotNone(x) | x is not None | 3.1 |\n",
    "| assertIn(a, b) | a in b | 3.1 |\n",
    "| assertNotIn(a, b) | a not in b | 3.1 |\n",
    "| assertIsInstance(a, b) | isinstance(a, b) | 3.2 |\n",
    "| assertNotIsInstance(a, b) | not isinstance(a, b) | 3.2 |\n",
    "| | | |\n",
    "| | | |\n",
    "| assertAlmostEqual(a, b) | round(a-b, 7) == 0 | |\n",
    "| assertNotAlmostEqual(a, b) | round(a-b, 7) != 0 | |\n",
    "| assertGreater(a, b) | a > b | 3.1 |\n",
    "| assertGreaterEqual(a, b) | a >= b | 3.1 |\n",
    "| assertLess(a, b) | a < b | 3.1 |\n",
    "| assertLessEqual(a, b) | a <= b | 3.1 |\n",
    "| assertRegex(s, r) | r.search(s) | 3.1 |\n",
    "| assertNotRegex(s, r) | not r.search(s) | 3.2 |\n",
    "| assertCountEqual(a, b) | a and b have the same elements in the same number, regardless of their order. | 3.2 |\n",
    "\n",
    "\n",
    "| Method | Used to compare | New in|\n",
    "|:------|:-:|:-:|\n",
    "| assertMultiLineEqual(a, b) | strings | 3.1 |\n",
    "| assertSequenceEqual(a, b) | sequences | 3.1 |\n",
    "| assertListEqual(a, b) | lists | 3.1 |\n",
    "| assertTupleEqual(a, b) | tuples | 3.1 |\n",
    "| assertSetEqual(a, b) | sets or frozensets | 3.1 |\n",
    "| assertDictEqual(a, b) | dicts | 3.1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate the following two scenarios:\n",
    "1. Scenario 1: the unit test runs with everything correct\n",
    "2. Scenario 2: the unit test runs, but with errors\n",
    "     - A new `assertEqual` is added\n",
    "\n",
    "**Note**: We will include <font color='DodgerBlue'>additional assert statements just to demonstrate how the output of a unit test looks like</font>, even though it is not relevant to our user-defined function.\n",
    "\n",
    "**Scenario 1**: the unit test runs with everything <font color='DodgerBlue'>correctly</font>\n",
    "\n",
    "Define a user-defined function to demo how that is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello_world():\n",
    "    return 'hello world'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: `isupper()`\n",
    "\n",
    "- `str.isupper()`: Return `True` if all characters in a given string are uppercase, otherwise it is `False`.\n",
    "    \n",
    "    - https://docs.python.org/3/library/stdtypes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class MyFirstUniTTests(unittest.TestCase):\n",
    "\n",
    "    def test_isEqual(self):\n",
    "        self.assertEqual(hello_world(), 'hello world')\n",
    "\n",
    "    def test_isLess(self):\n",
    "        self.assertLess(5, 10)\n",
    "\n",
    "    def test_isLessEqual(self):\n",
    "        self.assertLessEqual(10, 10)\n",
    "\n",
    "    def test_isUpperTrue(self):\n",
    "        self.assertTrue('FOO'.isupper())\n",
    "\n",
    "    def test_isUpperFalse(self):\n",
    "        self.assertFalse('Foo'.isupper())\n",
    "\n",
    "\n",
    "## Normal usage (in a .py script)\n",
    "#if __name__ == '__main__':\n",
    "#    unittest.main()\n",
    "\n",
    "## For usage in jupyter and colaboratory (due to the kernel)\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['ignored', '-v'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario 2**: the unit test runs, but some <font color='red'>errors</font> occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstUniTTests(unittest.TestCase):\n",
    "\n",
    "    def test_fail(self):\n",
    "       self.assertEqual(hello_world(), 'bye world')\n",
    "\n",
    "    def test_isEqual(self):\n",
    "        self.assertEqual(hello_world(), 'hello world')\n",
    "\n",
    "    def test_isLess(self):\n",
    "        self.assertLess(5, 10)\n",
    "\n",
    "    def test_isLessEqual(self):\n",
    "        self.assertLessEqual(10, 10)\n",
    "\n",
    "    def test_isUpperTrue(self):\n",
    "        self.assertTrue('FOO'.isupper())\n",
    "\n",
    "    def test_isUpperFalse(self):\n",
    "        self.assertFalse('Foo'.isupper())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['ignored', '-v'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTest (state-of-the-art)\n",
    "\n",
    "1. A command-line (e.g., using a bash shell) driven testing approach\n",
    "2. Simplifies and helps organize unit tests\n",
    "    - done by creating **user-defined functions** for **each test** that you want to do\n",
    "\n",
    "https://docs.pytest.org/en/7.1.x/contents.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_sum.py\n",
    "''' The following will be created:\n",
    "        1. Four unit test functions\n",
    "            a. First 3 will pass\n",
    "            b. Last 1 will fail\n",
    "'''\n",
    "\n",
    "def test_pass_add_list_1():\n",
    "    ''' 1st unit test\n",
    "    '''\n",
    "    test_list = [1, 2, 3, 4]\n",
    "    assert sum(test_list) == 10\n",
    "\n",
    "\n",
    "def test_pass_add_list_2():\n",
    "    ''' 2nd unit test\n",
    "    '''\n",
    "    test_list = [1, 2, 3, 4, 5]\n",
    "    assert sum(test_list) == 15\n",
    "\n",
    "\n",
    "def test_pass_add_list_3():\n",
    "    ''' 3rd unit test\n",
    "    '''\n",
    "    test_list = [1, 2, 3, 4, 5, 6]\n",
    "    assert sum(test_list) == 21\n",
    "\n",
    "\n",
    "def test_fail_add_list_4():\n",
    "    ''' 4th unit test\n",
    "        Should Fail\n",
    "    '''\n",
    "    print('PRINT STATEMENT FOR FAILING TEST FUNCTION.')\n",
    "    \n",
    "    test_list = [1, 2, 3, 4, 5, 6]\n",
    "    assert sum(test_list) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTest will give the following output:\n",
    "- `.` (dot) = test <font color='DodgerBlue'>passed</font>\n",
    "- `F` =  test has <font color='DodgerBlue'>failed</font>\n",
    "- `E` =  test raised an <font color='DodgerBlue'>unexpected exception</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pytest test_sum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**\n",
    "1. The first three test passed\n",
    "2. The fourth test fails\n",
    "3. A traceback is given concerning the error\n",
    "4. None of the print statements are seen\n",
    "\n",
    "<font color='DodgerBlue'>To see print commands</font> within the user-defined functions, <font color='DodgerBlue'>use `-s` option</font>:\n",
    "\n",
    "(`-s` is a shortcut for `--capture=no` - see `pytest --help`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pytest test_sum.py -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm test_sum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Notebook function and PyTesting\n",
    "\n",
    "Create a User-defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_area(length, width):\n",
    "    \"\"\" Calculate the area of a rectangle.\n",
    "    \"\"\"\n",
    "    if length <= 0 or width <= 0:\n",
    "        raise ValueError(\"Dimensions must be positive.\")\n",
    "    return length * width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PyTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_calculate_area.py\n",
    "import pytest\n",
    "\n",
    "def calculate_area(length, width):\n",
    "    \"\"\" Calculate the area of a rectangle.\n",
    "    \"\"\"\n",
    "    if length <= 0 or width <= 0:\n",
    "        raise ValueError(\"Dimensions must be positive.\")\n",
    "    return length * width\n",
    "\n",
    "\n",
    "def test_positive_dimensions():\n",
    "    # Test case with standard positive inputs\n",
    "    assert calculate_area(5, 4) == 20\n",
    "\n",
    "\n",
    "def test_square_dimensions():\n",
    "    # Test case for a square (equal sides)\n",
    "    assert calculate_area(10, 10) == 100\n",
    "\n",
    "\n",
    "def test_zero_or_negative_dimensions():\n",
    "    # Test case for invalid input using pytest.raises\n",
    "    with pytest.raises(ValueError):\n",
    "        calculate_area(-1, 5)\n",
    "    with pytest.raises(ValueError):\n",
    "        calculate_area(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pytest test_calculate_area.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm test_calculate_area.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing `pytest.mark.parametrize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_even.py\n",
    "import pytest\n",
    "\n",
    "\n",
    "def add(number_1: int|float, number_2: int|float) -> float:\n",
    "    \"\"\"\n",
    "    Returns the sum of two numbers.\n",
    "\n",
    "    Parameters:\n",
    "        number_1: The first number.\n",
    "        number_2: The second number.\n",
    "\n",
    "    Returns:\n",
    "        int or float: The sum of a and b.\n",
    "    \"\"\"\n",
    "    return number_1 + number_2\n",
    "\n",
    "\n",
    "def test_add_multiple_inputs():\n",
    "    ''' An initial attempt that uses a for loop within the test (avoid this).\n",
    "    '''\n",
    "    test_data = [(1, 2, 3),\n",
    "                 (-1, -1, -2),\n",
    "                 (5, 0, 5)]\n",
    "    for num_1, num_2, expected in test_data:\n",
    "        assert add(number_1=num_1, number_2=num_2) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -vs test_even.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_add_numbers.py\n",
    "import pytest\n",
    "\n",
    "\n",
    "def add(number_1: int|float, number_2: int|float) -> float:\n",
    "    \"\"\"\n",
    "    Returns the sum of two numbers.\n",
    "\n",
    "    Parameters:\n",
    "        number_1: The first number.\n",
    "        number_2: The second number.\n",
    "\n",
    "    Returns:\n",
    "        int or float: The sum of a and b.\n",
    "    \"\"\"\n",
    "    return number_1 + number_2\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"num_1, num_2, expected\", [(1, 2, 3),        # Test Case 1: Positive numbers\n",
    "                                                    (-1, -1, -2),     # Test Case 2: Negative numbers\n",
    "                                                    (5, 0, 5),        # Test Case 3: Testing with zero\n",
    "                                                    (100, -50, 50)])  # Test Case 4: Positive and negative\n",
    "\n",
    "def test_add(num_1, num_2, expected):\n",
    "    assert add(number_1=num_1, number_2=num_2) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -vs test_add_numbers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm test_add_numbers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests in the Age of AI\n",
    "\n",
    "- LLMs are capable of generating entire functions, which means that\n",
    "    - Code is written faster, but\n",
    "    - Not necessarily safer or more correct.\n",
    "    \n",
    "- Unit tests serve as the essential\n",
    "    - safety net\n",
    "    - verification step\n",
    "\n",
    "<b>Top Points</b>\n",
    "\n",
    "1. Source of <b>Truth</b>\n",
    "\n",
    "    - Verifying AI-Generated Code (via <b>LLMs</b>):\n",
    "        - excellent at writing <b>syntactically correct</b> code,\n",
    "        - can <b>miss edge cases</b>, or\n",
    "        - <b>misinterpret</b> complex <b>domain logic</b>.\n",
    "    - Unit tests\n",
    "        - Deterministic <b>source of truth</b> that verifies an AI's output.\n",
    "        - Guards <b>against \"hallucinations\"</b>.\n",
    "\n",
    "2. Maintaining <b>Stability</b> During Rapid <b>Iterations</b>\n",
    "\n",
    "    - Safety Net for Refactoring\n",
    "        - AI tools <b>enable</b> rapid <b>refactoring / restructuring</b>\n",
    "        - A <b>unit test suite</b> - allows <b>developers</b> to <b>confirm</b> AI-driven refactoring (preventing bugs).\n",
    "\n",
    "    - Faster Debugging (Pinpointing the Error) of complex AI-generated code\n",
    "\n",
    "3. The Core of Effective Prompt Engineering\n",
    "\n",
    "    - Test-Driven Prompting\n",
    "        - The <b>unit test</b> file itself <b>becomes a powerful prompt</b> you can give an <b>LLM</b>.\n",
    "            - \"Write Python code that makes this unit test pass\" (vs. less precise/effective \"Write a function to do X.\")\n",
    "\n",
    "    - Defining Edge Cases\n",
    "        - Explicitly define expected behavior for all inputs (e.g., negative numbers, empty arrays, null values).\n",
    "        - Results in more <b>robust AI code</b>.\n",
    "\n",
    "4. Knowledge Transfer\n",
    "    - Onboarding and Documentation - Unit tests are a form of living documentation\n",
    "        - <b>New</b> developers running unit tests and <b>reading their inputs/assertions</b> is an efficient way to understand the intended <b>code's functionality</b> and <b>requirements</b>\n",
    "\n",
    "---\n",
    "# Take Home Messages\n",
    "\n",
    "In <b>scientific research</b>, it's <b>very important</b> that code produces <b>correct</b> and <b>reproducible</b> results.\n",
    "\n",
    "1. Importance of Testing and Untested Code\n",
    "    - Untested code is problematic because it:\n",
    "        - <b>Reduced confidence</b>\n",
    "        - Causes <b>fear</b> of making <b>changes</b> (refactoring), and\n",
    "        - Is <b>costly</b> debugging\n",
    "    \n",
    "    - Testing <b>flips</b> this dynamic.\n",
    "\n",
    "    - \"I Know It Works\" illusion is dangerous\n",
    "        - <b>Manual checks work temporarily</b> (e.g., print statement), but can change tomorrow (e.g., refactoring)\n",
    "        - Can easily break functionality without detection\n",
    "\n",
    "- <b>Unit Tests - Raising Errors</b>\n",
    "\n",
    "    - A unit test: an 'automated' procedure to verify a small, <b>isolated code piece</b> (e.g., a function) works as intended under different conditions.\n",
    "\n",
    "    - Unit Test's Key Characteristics:\n",
    "        - <b>Isolation</b>: Focuses on the <b>smallest testable part of the code</b>, and must run without external dependencies (e.g., databases).\n",
    "        - <b>Assertion / Raise Statements</b>: Every test contains these checks that <b>stop</b> and <b>report</b> an <b>error</b> if they fail.\n",
    "        - <b>Automation</b>: The test runs without interventions, and can do so through a framework (e.g., pytest, unittest).\n",
    "        - <b>Speed</b>: Executes very quickly since they are constantly rerun.\n",
    "\n",
    "- <b>`try-except` - Error Handling</b>\n",
    "\n",
    "    - `try-except` statement tells your code to\n",
    "        - try a block of code, and\n",
    "        - then specifies what to do if an exception (error) of a certain type occurs.\n",
    "\n",
    "    - Strengths:\n",
    "        - The <b>code continues to run</b> even after <b>encountering a problem</b>.\n",
    "        - Faster than using if statements when the majority of tasks are expected to be successful.\n",
    "\n",
    "    - Weakness:\n",
    "         - They can <b>mask or ignore potential bugs</b>\n",
    "             - Leading to corrupt or incomplete data while the program suggests otherwise (i.e., Error Hiding).\n",
    "         - Better practice: <b>allow logical errors to stop the algorithm</b>, thus notifying the programmer.\n",
    "\n",
    "- <b>Test-Driven Development</b> (TDD)\n",
    "\n",
    "    - TDD involves writing tests before you write your production code.\n",
    "\n",
    "    - Benefits:\n",
    "\n",
    "        - Ensures proper and directed functionality - <b>leads to concise code</b> that does a single thing.\n",
    "\n",
    "        - Helps <b>plan your code</b> and <b>encourages critical thinking</b> about what you actually want to do.\n",
    "\n",
    "        - <b>Reduces errors, ensures reproducibility</b>, and helps ensure a code's long life (<b>maintainability, sustainability</b>).\n",
    "\n",
    "    - TDD Workflow Concept\n",
    " \n",
    "        - Write a failing test,\n",
    "        - Run and ensure failure,\n",
    "        - Write code to pass,\n",
    "        - Run and ensure passing,\n",
    "        - Refactor (restructure/clean up code without changing its final result), and\n",
    "        - Redo\n",
    "\n",
    "- <b>PyTest</b> Tool\n",
    "\n",
    "    - PyTest is a command-line driven testing framework that <b>simplifies and organizes unit tests</b>\n",
    "        - via <b>creating separate user-defined functions for each test</b>.\n",
    "\n",
    "    - The output symbols help quickly assess test results\n",
    " \n",
    "        - `.` for passed,\n",
    "        - `F` for failed, and\n",
    "        - `E` for an unexpected exception\n",
    "\n",
    "- <b>Unit tests</b> are becoming <b>more important</b> in the age of <b>AI</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
