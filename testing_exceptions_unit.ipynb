{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert block alert-info alert\">\n",
    "\n",
    "# <center> Scientific Programming in Python\n",
    "\n",
    "## <center>Karl N. Kirschner<br>Bonn-Rhein-Sieg University of Applied Sciences<br>Sankt Augustin, Germany\n",
    "\n",
    "# <center> Testing Code (and GenAI)\n",
    "\n",
    "<!-- <br><br> -->\n",
    "\n",
    "\n",
    "#### References\n",
    "1. Fucci, D., Erdogmus, H., Turhan, B., Oivo, M. and Juristo, N., 2016. A dissection of the test-driven development process: Does it really matter to test-first or to test-last?. IEEE Transactions on Software Engineering, 43(7), pp.597-614. (https://ieeexplore.ieee.org/abstract/document/7592412)\n",
    "2. Mock, M., Melegati, J. and Russo, B., 2024, June. Generative AI for test driven development: preliminary results. In International Conference on Agile Software Development (pp. 24-32). Cham: Springer Nature Switzerland. (https://link.springer.com/chapter/10.1007/978-3-031-72781-8_3)\n",
    "3. Piya, S. and Sullivan, A., 2024, April. LLM4TDD: best practices for test driven development using large language models. In Proceedings of the 1st International Workshop on Large Language Models for Code (pp. 14-21). (https://dl.acm.org/doi/abs/10.1145/3643795.3648382)\n",
    "4. Ardic, B., Dilavrec, Q.L. and Zaidman, A., 2025. How Students Use Generative AI for Software Testing: An Observational Study. arXiv preprint arXiv:2510.10551. (https://arxiv.org/abs/2510.10551)\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "**Note**: All user-defined functions in the notebook do not include document strings (i.e. block comments) or internal checks. This is purposely done to focus on the teaching aspects of the lecture. **A full and proper user-defined function would include these.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<div> <img src=\"00_images/ai/unit_testing_gemini_generated.png\" width=\"400\"/> </div>\n",
    "Generated with Gemini AI âˆ™ October 27, 2025 at 16:20\n",
    "</center>\n",
    "\n",
    "# Context: The Hidden Cost of Untested Code\n",
    "\n",
    "\n",
    "**The \"I Know It Works\" Illusion**\n",
    "- Testing code mentally or with temporary `print()` statements:\n",
    "    - Works **today** and for this **one case**,\n",
    "    - But tomorrow may be different: a colleague changes a dependency, or the code is refactored.\n",
    "        - (That small, manual `print()` statement check is lost or bypassed.)\n",
    "\n",
    "**Take-Home Message**\n",
    "- Untested code is problematic:\n",
    "    - Reduced confidence in it\n",
    "    - Results in fear of making changes\n",
    "    - Costly debugging\n",
    "\n",
    "<br>\n",
    "\n",
    "- Unit testing <b>flips</b> this dynamic\n",
    "    - Gives you <b>confidence</b> to <b>refactor</b>, upgrade, and scale your code\n",
    "        - If you break something, the tests will catch it\n",
    "    - <b>Debugging</b> is <b>faster</b> and <b>easier</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "# Unit Tests\n",
    "\n",
    "- An <b>'automated'</b> procedure to verify a **small**, **isolated** code piece (i.e., a **unit**).\n",
    "    - Make sure that the unit works as intended under different conditions.\n",
    " \n",
    "## Unit Tests Characteristics\n",
    "\n",
    "- <font color='DodgerBlue'>**Isolation**</font>\n",
    "    - Focuses on the <b>smallest</b> testable <b>part</b> of the <b>code</b> (e.g., a single function).\n",
    "    - It must be <b>run</b> in <b>isolation</b> (i.e., it doesn't rely on external factors like databases).\n",
    "\n",
    "- <font color='DodgerBlue'>**Assertion / Raise Statements**</font>\n",
    "    - Every unit test contains one or more <b>assertions/raise statements</b>.\n",
    "        - An assertion is a conditional check (e.g., `assert calculated_value == expected_value`).\n",
    "        - If it fails, the <b>test stops</b> and <b>reports an error</b>.\n",
    "\n",
    "- <font color='DodgerBlue'>Automation</font>\n",
    "    - The test itself is code.\n",
    "    - It can be run instantly and repeatedly by a testing framework (e.g., <b>pytest</b>, unittest).\n",
    "    - This is crucial for speed and consistency.\n",
    "\n",
    "- <font color='DodgerBlue'>Speed</font>\n",
    "    - Unit tests must execute quickly (milliseconds).\n",
    "        - They will be run constantly by developers (or automated build systems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assert vs. Raise\n",
    "\n",
    "- Comparing <font color='DodgerBlue'>two floats</font> to be within a <font color='DodgerBlue'>tolerance</font> range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 0.001\n",
    "\n",
    "calculated_number = 0.115\n",
    "expected_number = 0.117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard `assert`: raises an AssertionError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition\n",
    "- `abs(calculated_number - expected_number)` must be **less than** the `tolerance` value\n",
    "- If `True`, then continue.\n",
    "- If `False`, then **raise customized error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(calculated_number - expected_number) <= tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Values are too far apart.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(calculated_number - expected_number) <= tolerance, \u001b[33m\"\u001b[39m\u001b[33mValues are too far apart.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: Values are too far apart."
     ]
    }
   ],
   "source": [
    "assert abs(calculated_number - expected_number) <= tolerance, \"Values are too far apart.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom `raise`: Checks the condition, explicitly raises an exception on failure\n",
    "\n",
    "- `abs(calculated_number - expected_number)` is **greater than** the `tolerance` value\n",
    "- If `True`, then **raise error**:\n",
    "- If `False`, then continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(calculated_number - expected_number) >= tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Error: Expected value was 0.117, but obtained 0.115.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(calculated_number - expected_number) >= tolerance:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError: Expected value was \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_number\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but obtained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcalculated_number\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: Error: Expected value was 0.117, but obtained 0.115."
     ]
    }
   ],
   "source": [
    "if abs(calculated_number - expected_number) >= tolerance:\n",
    "    raise AssertionError(f\"Error: Expected value was {expected_number:.3f}, but obtained {calculated_number:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grade(score, max_score=100):\n",
    "    \"\"\" Calculates the percentage grade from a score. \"\"\"\n",
    "\n",
    "    percentage = (score / max_score) * 100\n",
    "\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.0\n"
     ]
    }
   ],
   "source": [
    "print(calculate_grade(85))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look correct - great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Test - Raising Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_calculate_grade():\n",
    "    \"\"\" Verifies the function's output for known, good data. \"\"\"\n",
    "\n",
    "    # 1. Standard case check (80/100 should be 80.0)\n",
    "    assert calculate_grade(80) == 80.0, \"The calculate_grade function does not work for when max_score is 100.\"\n",
    "\n",
    "    # 2. Edge case check (max_score is not 100)\n",
    "    assert calculate_grade(25, max_score=50) == 50.0, \"The calculate_grade function does not work for when max_score is not 100.\"\n",
    "\n",
    "    print(\"Test Passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed.\n"
     ]
    }
   ],
   "source": [
    "test_calculate_grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add a `try`-`except` statement (for comparison to a later case where the test fails):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_calculate_grade()\n",
    "except AssertionError as error:\n",
    "    print(\"TEST FAILED! The test caught a bug that the simple manual check missed.\")\n",
    "    print(f\"Assert Failure Details: {error}\")\n",
    "    print(\"This is the power of a Unit Test: immediate feedback and regression prevention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario\n",
    "Now, let's say in 10 months a colleague moves the 100 to a global position (i.e. outside the user-defined function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grade(score, max_score=100):\n",
    "    \"\"\" Buggy - calculates the percentage grade from a score. \"\"\"\n",
    "\n",
    "    # BUG: Forgot to multiply by 100\n",
    "    percentage = (score / max_score) \n",
    "\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will still use our `test_calculate_grade()` function, but now it will test the <b>new buggy</b> `calculate_grade()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The calculate_grade function does not work for when max_score is 100.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtest_calculate_grade\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mtest_calculate_grade\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\" Verifies the function's output for known, good data. \"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. Standard case check (80/100 should be 80.0)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m calculate_grade(\u001b[32m80\u001b[39m) == \u001b[32m80.0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mThe calculate_grade function does not work for when max_score is 100.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 2. Edge case check (max_score is not 100)\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m calculate_grade(\u001b[32m25\u001b[39m, max_score=\u001b[32m50\u001b[39m) == \u001b[32m50.0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mThe calculate_grade function does not work for when max_score is not 100.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: The calculate_grade function does not work for when max_score is 100."
     ]
    }
   ],
   "source": [
    "test_calculate_grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will introduce the `try-except` statement for code <b>error handling</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST FAILED! The test caught a bug that the simple manual check missed.\n",
      "Assert Failure Details: The calculate_grade function does not work for when max_score is 100.\n",
      "This is the power of a Unit Test: immediate feedback and regression prevention.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_calculate_grade()\n",
    "except AssertionError as error:\n",
    "    print(\"TEST FAILED! The test caught a bug that the simple manual check missed.\")\n",
    "    print(f\"Assert Failure Details: {error}\")\n",
    "    print(\"This is the power of a Unit Test: immediate feedback and regression prevention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "## The `try`-`except` Statement - to Handle Errors\n",
    "- Tells your code to try something, and\n",
    "- then tells it what to do if it fails based on an <b>exception type</b> (i.e., an error).\n",
    "\n",
    " \n",
    "#### Strengths:\n",
    "1. The code will <font color='DodgerBlue'>**continue**</font> to run, even when it encounters a problem.\n",
    "\n",
    "   - This prevents the program from crashing (e.g., if a test fails).<br><br>\n",
    "\n",
    "2. <font color='DodgerBlue'>**Faster** than if statements</font> for when <font color='DodgerBlue'>majority of the planned tasks are **expected** to be **successful**</font> (i.e., they don't encounter an exception)\n",
    "\n",
    "3. Built-in exceptions: https://docs.python.org/3/library/exceptions.html#bltin-exceptions\n",
    "\n",
    "<br><br>\n",
    "**Simple Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: You can't have a zero in the denominator.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(5/0)\n",
    "except ZeroDivisionError:\n",
    "    print(f\"Error: You can't have a zero in the denominator.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A More Sophisticated Example**\n",
    "\n",
    "Let's set up a <font color='DodgerBlue'>division calculator</font> that allows users to input numbers and quit at any time using while and if loops (to demonstrate via a comparison of code).\n",
    "\n",
    "1. First, set something up without `try`-`except` in order to see its advantage later.\n",
    "2. Second, do the same thing with `try`-`except`\n",
    "\n",
    "<font color='DodgerBlue'>Demonstrate the following</font>:\n",
    "1. normal operation\n",
    "2. exiting by typing 'q'\n",
    "3. <font color='Red'>O</font> (i.e., a capital alphabet letter \"O\", as in \"O\"ktoberfest (also demonstrates traceback error))\n",
    "\n",
    "Without `try`-`except` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'q' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Numerator =  O\n",
      "Denominator =  4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'O'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mYou can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have a zero in the denominator.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m answer = \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnumerator\u001b[49m\u001b[43m)\u001b[49m/\u001b[38;5;28mfloat\u001b[39m(denominator)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAnswer for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumerator\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdenominator\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'O'"
     ]
    }
   ],
   "source": [
    "## print('Type two numbers that you want to be divided.')\n",
    "print(\"Type 'q' to quit.\")\n",
    "print()\n",
    "\n",
    "while True:\n",
    "    numerator = input('Numerator = ')\n",
    "    if numerator == 'q':\n",
    "        break\n",
    "\n",
    "    denominator = input('Denominator = ')\n",
    "    if denominator == 'q':\n",
    "        break\n",
    "\n",
    "    if denominator == '0':\n",
    "        print(\"You can't have a zero in the denominator.\")\n",
    "        break\n",
    "\n",
    "    answer = float(numerator)/float(denominator)\n",
    "    print(f'Answer for {numerator}/{denominator} = {answer}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the above code to use a **`try`-`except` statement**, and try it with <font color='Red'>O</font>.\n",
    "\n",
    "**Note**: <font color='DodgerBlue'>Multiple `except` conditions via a **tuple**</font>:<br>\n",
    "`except (ZeroDivisionError, ValueError):`\n",
    "- `ZeroDivisionError` when the denominator is zero\n",
    "- `ValueError` for when a string is given as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type two numbers that you want to be divided.\n",
      "Type 'q' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Numerator =  8\n",
      "Denominator =  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer for 8/4 = 2.0\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Numerator =  6\n",
      "Denominator =  O\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your input was either not a number, or you are dividing by a zero.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Numerator =  q\n"
     ]
    }
   ],
   "source": [
    "print('Type two numbers that you want to be divided.')\n",
    "print(\"Type 'q' to quit.\")\n",
    "print()\n",
    "\n",
    "while True:\n",
    "    numerator = input('Numerator = ')\n",
    "    if numerator == 'q':\n",
    "        break\n",
    "\n",
    "    denominator = input('Denominator = ')\n",
    "    if denominator == 'q':\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        answer = float(numerator)/float(denominator)\n",
    "        print(f'Answer for {numerator}/{denominator} = {answer}\\n')\n",
    "\n",
    "    except (ZeroDivisionError, ValueError):\n",
    "        print('Your input was either not a number, or you are dividing by a zero.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>Now the code continues to run, even though an error was raised!</font>\n",
    "\n",
    "### Warning - Be Careful when Using `try-except` Statements\n",
    "- They will mask or ignore potential bugs\n",
    "\n",
    "<b>Scenario</b>: Creating a dungeon crawler game\n",
    "\n",
    "#### Bad Practice: Demonstrates Error Hiding and False Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program finished execution.\n",
      "Success: Program finished execution with all players assigned.\n"
     ]
    }
   ],
   "source": [
    "players = {1: None, 2: None, 3: None, 4: None, 5: None}\n",
    "equipment_list = ['ax', 'potion', 'wand', 'dust', 'sword']\n",
    "\n",
    "index = 0\n",
    "\n",
    "while index <= 5:\n",
    "    index += 1\n",
    "    try:\n",
    "        players[index + 1] = equipment_list[index] \n",
    "    except IndexError:\n",
    "        break  # Stop the loop, hiding the fact that not all players were assigned.\n",
    "\n",
    "# The data is corrupt/incomplete, but the program claims success.\n",
    "print(\"Program finished execution.\")\n",
    "print(\"Success: Program finished execution with all players assigned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player data (corrupted):\n",
      "{1: None, 2: 'potion', 3: 'wand', 4: 'dust', 5: 'sword'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Player data (corrupted):\\n{players}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Notice</b>: <b>Player 1</b> has <b>no</b> equipment assigned.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Better Practice: Allows logical errors to stop the algorithm.\n",
    "\n",
    "- Allow the program to error, thus notifying the programmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m index <= \u001b[32m5\u001b[39m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Logical bug: index will reach 5, which is beyond the equipment_list range.\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# The assignment below will fail when index = 5.\u001b[39;00m\n\u001b[32m      9\u001b[39m     index += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     players[index + \u001b[32m1\u001b[39m] = \u001b[43mequipment_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProgram finished execution.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSuccess: Program finished execution with all players assigned.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "players = {1: None, 2: None, 3: None, 4: None, 5: None}\n",
    "equipment_list = ['ax', 'potion', 'wand', 'dust', 'sword']\n",
    "\n",
    "index = 0\n",
    "\n",
    "while index <= 5:\n",
    "    # Logical bug: index will reach 5, which is beyond the equipment_list range.\n",
    "    # The assignment below will fail when index = 5.\n",
    "    index += 1\n",
    "    players[index + 1] = equipment_list[index] \n",
    "    \n",
    "print(\"Program finished execution.\")\n",
    "print(\"Success: Program finished execution with all players assigned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Best Solution: correct, clear, and robust\n",
    "\n",
    "To show how the problem could be coded well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program finished execution.\n",
      "Success: Program finished execution with all players assigned.\n"
     ]
    }
   ],
   "source": [
    "players = {1: None, 2: None, 3: None, 4: None, 5: None}\n",
    "equipment_list = ['ax', 'potion', 'wand', 'dust', 'sword']\n",
    "\n",
    "for index in range(len(equipment_list)): # Correctly iterates from 0 to 4\n",
    "    players[index + 1] = equipment_list[index] \n",
    "\n",
    "# Verification\n",
    "if None in players.values():\n",
    "    print(\"Problem: Program finished execution with UNASSIGNED players.\")\n",
    "else:\n",
    "    print(\"Program finished execution.\")\n",
    "    print(\"Success: Program finished execution with all players assigned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player data (uncorrupted):\n",
      "{1: 'ax', 2: 'potion', 3: 'wand', 4: 'dust', 5: 'sword'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Player data (uncorrupted):\\n{players}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "<h1 align='center'>Test Driven Development (<font color='DodgerBlue'>TDD</font>)\n",
    "    \n",
    "<h2 align='center'> using Unit Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/library/unittest.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Driven Development - writing tests before you write your production code\n",
    "1. Ensures proper and directed functionality of your code\n",
    " - creating **concise** code that does a **single** thing (e.g., <font color='DodgerBlue'>**user-defined functions**</font>)\n",
    "2. Helps you plan your code - what do you **actually want** to do (critical thinking)\n",
    "3. Reduces **errors**\n",
    "4. Ensures **reproducibility**\n",
    "5. Helps to ensure a code's **long life**\n",
    "\n",
    "## The Traditional Workflow Concept (write test -> write production code)\n",
    "1. Write a failing test\n",
    "2. Run and ensure failure\n",
    "3. Write code to pass\n",
    "4. Run and ensure passing\n",
    "5. Refactor (i.e., restructure/clean up code without changing it final result)\n",
    "6. Redo steps 1-5\n",
    "\n",
    "However, the **sequence** of test and production code development **might not matter** [1]:\n",
    "1. Write **test** -> write **production** code, or\n",
    "2. Write **production** code -> write **test** -> **refactor** production code\n",
    "\n",
    "<font color='DodgerBlue'>**What is important is that unit tests are included!**</font>\n",
    "\n",
    "## Scientific and Data Research\n",
    "It is **CRITICAL** that:\n",
    "1. The correct results are generated\n",
    "2. The results are reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assert statements that can be used in unittest library\n",
    "\n",
    "https://docs.python.org/3/library/unittest.html#module-unittest\n",
    "\n",
    "\n",
    "| Method | Checks that | New in |\n",
    "|:------|:-:|:-:|\n",
    "| assertEqual(a, b) | a == b | |\n",
    "| assertNotEqual(a, b)| a != b | |\n",
    "| assertTrue(x) | bool(x) is True | |\n",
    "| assertFalse(x) | bool(x) is False | |\n",
    "| assertIs(a, b) | a is b | 3.1 |\n",
    "| assertIsNot(a, b) | a is not b | 3.1 |\n",
    "| assertIsNone(x) | x is None | 3.1 |\n",
    "| assertIsNotNone(x) | x is not None | 3.1 |\n",
    "| assertIn(a, b) | a in b | 3.1 |\n",
    "| assertNotIn(a, b) | a not in b | 3.1 |\n",
    "| assertIsInstance(a, b) | isinstance(a, b) | 3.2 |\n",
    "| assertNotIsInstance(a, b) | not isinstance(a, b) | 3.2 |\n",
    "| | | |\n",
    "| | | |\n",
    "| assertAlmostEqual(a, b) | round(a-b, 7) == 0 | |\n",
    "| assertNotAlmostEqual(a, b) | round(a-b, 7) != 0 | |\n",
    "| assertGreater(a, b) | a > b | 3.1 |\n",
    "| assertGreaterEqual(a, b) | a >= b | 3.1 |\n",
    "| assertLess(a, b) | a < b | 3.1 |\n",
    "| assertLessEqual(a, b) | a <= b | 3.1 |\n",
    "| assertRegex(s, r) | r.search(s) | 3.1 |\n",
    "| assertNotRegex(s, r) | not r.search(s) | 3.2 |\n",
    "| assertCountEqual(a, b) | a and b have the same elements in the same number, regardless of their order. | 3.2 |\n",
    "\n",
    "\n",
    "| Method | Used to compare | New in|\n",
    "|:------|:-:|:-:|\n",
    "| assertMultiLineEqual(a, b) | strings | 3.1 |\n",
    "| assertSequenceEqual(a, b) | sequences | 3.1 |\n",
    "| assertListEqual(a, b) | lists | 3.1 |\n",
    "| assertTupleEqual(a, b) | tuples | 3.1 |\n",
    "| assertSetEqual(a, b) | sets or frozensets | 3.1 |\n",
    "| assertDictEqual(a, b) | dicts | 3.1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate the following two scenarios:\n",
    "1. Scenario 1: the unit test runs with everything correct\n",
    "2. Scenario 2: the unit test runs, but with errors\n",
    "     - A new `assertEqual` is added\n",
    "\n",
    "**Note**: We will include <font color='DodgerBlue'>additional assert statements just to demonstrate how the output of a unit test looks like</font>, even though it is not relevant to our user-defined function.\n",
    "\n",
    "**Scenario 1**: the unit test runs with everything <font color='DodgerBlue'>correctly</font>\n",
    "\n",
    "Define a user-defined function to demo how that is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello_world():\n",
    "    return 'hello world'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: `isupper()`\n",
    "\n",
    "- `str.isupper()`: Return `True` if all characters in a given string are uppercase, otherwise it is `False`.\n",
    "    \n",
    "    - https://docs.python.org/3/library/stdtypes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_isEqual (__main__.MyFirstUniTTests.test_isEqual) ... ok\n",
      "test_isLess (__main__.MyFirstUniTTests.test_isLess) ... ok\n",
      "test_isLessEqual (__main__.MyFirstUniTTests.test_isLessEqual) ... ok\n",
      "test_isUpperFalse (__main__.MyFirstUniTTests.test_isUpperFalse) ... ok\n",
      "test_isUpperTrue (__main__.MyFirstUniTTests.test_isUpperTrue) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.006s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class MyFirstUniTTests(unittest.TestCase):\n",
    "\n",
    "    def test_isEqual(self):\n",
    "        self.assertEqual(hello_world(), 'hello world')\n",
    "\n",
    "    def test_isLess(self):\n",
    "        self.assertLess(5, 10)\n",
    "\n",
    "    def test_isLessEqual(self):\n",
    "        self.assertLessEqual(10, 10)\n",
    "\n",
    "    def test_isUpperTrue(self):\n",
    "        self.assertTrue('FOO'.isupper())\n",
    "\n",
    "    def test_isUpperFalse(self):\n",
    "        self.assertFalse('Foo'.isupper())\n",
    "\n",
    "\n",
    "## Normal usage (in a .py script)\n",
    "#if __name__ == '__main__':\n",
    "#    unittest.main()\n",
    "\n",
    "## For usage in jupyter and colaboratory (due to the kernel)\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['ignored', '-v'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario 2**: the unit test runs, but some <font color='red'>errors</font> occur (i.e., in the **first test**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_fail (__main__.MyFirstUniTTests.test_fail) ... FAIL\n",
      "test_isEqual (__main__.MyFirstUniTTests.test_isEqual) ... ok\n",
      "test_isLess (__main__.MyFirstUniTTests.test_isLess) ... ok\n",
      "test_isLessEqual (__main__.MyFirstUniTTests.test_isLessEqual) ... ok\n",
      "test_isUpperFalse (__main__.MyFirstUniTTests.test_isUpperFalse) ... ok\n",
      "test_isUpperTrue (__main__.MyFirstUniTTests.test_isUpperTrue) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_fail (__main__.MyFirstUniTTests.test_fail)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_64572/3283447063.py\", line 4, in test_fail\n",
      "    self.assertEqual(hello_world(), 'bye world')\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: 'hello world' != 'bye world'\n",
      "- hello world\n",
      "+ bye world\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.012s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "class MyFirstUniTTests(unittest.TestCase):\n",
    "\n",
    "    def test_fail(self):\n",
    "       self.assertEqual(hello_world(), 'bye world')\n",
    "\n",
    "    def test_isEqual(self):\n",
    "        self.assertEqual(hello_world(), 'hello world')\n",
    "\n",
    "    def test_isLess(self):\n",
    "        self.assertLess(5, 10)\n",
    "\n",
    "    def test_isLessEqual(self):\n",
    "        self.assertLessEqual(10, 10)\n",
    "\n",
    "    def test_isUpperTrue(self):\n",
    "        self.assertTrue('FOO'.isupper())\n",
    "\n",
    "    def test_isUpperFalse(self):\n",
    "        self.assertFalse('Foo'.isupper())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['ignored', '-v'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTest (state-of-the-art)\n",
    "\n",
    "1. A command-line driven testing approach (e.g., via bash shell)\n",
    "2. Simplifies and helps organize unit tests\n",
    "    - done by creating **user-defined functions** for **each test** that you want to do\n",
    "\n",
    "https://docs.pytest.org/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_sum.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_sum.py\n",
    "''' The following will be created:\n",
    "        1. Four unit test functions, where\n",
    "            a. First 3 will pass, and\n",
    "            b. Last 1 will fail.\n",
    "'''\n",
    "\n",
    "def test_pass_add_list_1():\n",
    "    ''' 1st unit test.\n",
    "    '''\n",
    "    test_list = [1, 2, 3, 4]\n",
    "    assert sum(test_list) == 10\n",
    "\n",
    "\n",
    "def test_pass_add_list_2():\n",
    "    ''' 2nd unit test.\n",
    "    '''\n",
    "    test_list = [1, 2, 3, 4, 5]\n",
    "    assert sum(test_list) == 15\n",
    "\n",
    "\n",
    "def test_pass_add_list_3():\n",
    "    ''' 3rd unit test with print statements.\n",
    "    '''\n",
    "    print('PRINT STATEMENT FOR FAILING TEST FUNCTION.')\n",
    "    print('Another statement')\n",
    "\n",
    "    test_list = [1, 2, 3, 4, 5, 6]\n",
    "    assert sum(test_list) == 21\n",
    "\n",
    "\n",
    "def test_fail_add_list_4():\n",
    "    ''' 4th unit test\n",
    "        Should Fail\n",
    "    '''    \n",
    "    test_list = [1, 2, 3, 4, 5, 6]\n",
    "    assert sum(test_list) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTest will give the following output:\n",
    "- `.` (dot) = test <font color='DodgerBlue'>passed</font>\n",
    "- `F` =  test has <font color='DodgerBlue'>failed</font>\n",
    "- `E` =  test raised an <font color='DodgerBlue'>unexpected exception</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=========================================================== test session starts ============================================================\u001b[0m\n",
      "platform linux -- Python 3.13.2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: /home/karl/Scientific_Programming_Course\n",
      "plugins: anyio-4.8.0\n",
      "collected 4 items                                                                                                                          \u001b[0m\n",
      "\n",
      "test_sum.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                                                     [100%]\u001b[0m\n",
      "\n",
      "================================================================= FAILURES =================================================================\n",
      "\u001b[31m\u001b[1m___________________________________________________________ test_fail_add_list_4 ___________________________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_fail_add_list_4\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m''' 4th unit test\u001b[39;49;00m\n",
      "    \u001b[33m        Should Fail\u001b[39;49;00m\n",
      "    \u001b[33m    '''\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        test_list = [\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96msum\u001b[39;49;00m(test_list) == \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 21 == 0\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 21 = sum([1, 2, 3, 4, 5, 6])\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_sum.py\u001b[0m:36: AssertionError\n",
      "\u001b[36m\u001b[1m========================================================= short test summary info ==========================================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_sum.py::\u001b[1mtest_fail_add_list_4\u001b[0m - assert 21 == 0\n",
      "\u001b[31m======================================================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m3 passed\u001b[0m\u001b[31m in 0.07s\u001b[0m\u001b[31m ========================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pytest test_sum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**\n",
    "1. The first three test passed\n",
    "2. The fourth test fails\n",
    "3. A traceback is given concerning the error\n",
    "4. None of the print statements are seen\n",
    "\n",
    "<font color='DodgerBlue'>To see print commands</font> within the user-defined functions, <font color='DodgerBlue'>use `-s` option</font>:\n",
    "\n",
    "(`-s` is a shortcut for `--capture=no` - see `pytest --help`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=========================================================== test session starts ============================================================\u001b[0m\n",
      "platform linux -- Python 3.13.2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: /home/karl/Scientific_Programming_Course\n",
      "plugins: anyio-4.8.0\n",
      "collected 4 items                                                                                                                          \u001b[0m\n",
      "\n",
      "test_sum.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0mPRINT STATEMENT FOR FAILING TEST FUNCTION.\n",
      "Another statement\n",
      "\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\n",
      "\n",
      "================================================================= FAILURES =================================================================\n",
      "\u001b[31m\u001b[1m___________________________________________________________ test_fail_add_list_4 ___________________________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_fail_add_list_4\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m''' 4th unit test\u001b[39;49;00m\n",
      "    \u001b[33m        Should Fail\u001b[39;49;00m\n",
      "    \u001b[33m    '''\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        test_list = [\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96msum\u001b[39;49;00m(test_list) == \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 21 == 0\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 21 = sum([1, 2, 3, 4, 5, 6])\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_sum.py\u001b[0m:36: AssertionError\n",
      "\u001b[36m\u001b[1m========================================================= short test summary info ==========================================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_sum.py::\u001b[1mtest_fail_add_list_4\u001b[0m - assert 21 == 0\n",
      "\u001b[31m======================================================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m3 passed\u001b[0m\u001b[31m in 0.06s\u001b[0m\u001b[31m ========================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pytest test_sum.py -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm test_sum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Notebook Function and PyTesting\n",
    "\n",
    "Create a User-defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_area(length, width):\n",
    "    \"\"\" Calculate the area of a rectangle.\n",
    "    \"\"\"\n",
    "    if length <= 0 or width <= 0:\n",
    "        raise ValueError(\"Dimensions must be positive.\")\n",
    "    return length * width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PyTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_calculate_area.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_calculate_area.py\n",
    "import pytest\n",
    "\n",
    "def calculate_area(length, width):\n",
    "    \"\"\" Calculate the area of a rectangle.\n",
    "    \"\"\"\n",
    "    if length <= 0 or width <= 0:\n",
    "        raise ValueError(\"Dimensions must be positive.\")\n",
    "    else:\n",
    "        return length * width\n",
    "\n",
    "\n",
    "def test_positive_dimensions():\n",
    "    # Test case with standard positive inputs\n",
    "    assert calculate_area(5, 4) == 20\n",
    "\n",
    "\n",
    "def test_square_dimensions():\n",
    "    # Test case for a square (equal sides)\n",
    "    assert calculate_area(10, 10) == 100\n",
    "\n",
    "\n",
    "def test_zero_or_negative_dimensions():\n",
    "    # Test case for invalid input using pytest.raises\n",
    "    with pytest.raises(ValueError):\n",
    "        calculate_area(-1, 5)\n",
    "    with pytest.raises(ValueError):\n",
    "        calculate_area(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=========================================================== test session starts ============================================================\u001b[0m\n",
      "platform linux -- Python 3.13.2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: /home/karl/Scientific_Programming_Course\n",
      "plugins: anyio-4.8.0\n",
      "collected 3 items                                                                                                                          \u001b[0m\n",
      "\n",
      "test_calculate_area.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                                           [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================================================ \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m =============================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pytest test_calculate_area.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm test_calculate_area.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "## Unit Tests in the Age of Generative AI (GenAI)\n",
    "\n",
    "Topics to Consider:\n",
    "1. **Testing GenAI-generated code** to ensure correctness\n",
    "2. Using **GenAI** to help **generate unit tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "display.Image(\"https://imgs.xkcd.com/comics/machine_learning.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GenAI and Unit Tests [2, 3]\n",
    "\n",
    "\"Generative AI (GenAI) may reduce the extra effort imposed by TDD.\" [2]\n",
    "\n",
    "\"**GenAI** can be efficiently used in TDD, but it **requires supervision** on the **quality** of the **code produced**. In some cases, it can **mislead** non-expert **developers** and **propose solutions that change tests rather than the production code**, which may remain buggy, to **make tests pass**.\"\n",
    "\n",
    "**GenAI** to **create Unit Tests**: <font color='DodgerBlue'>**Cutting Edge Idea**</font>, but\n",
    "- <font color='DodgerBlue'>There is very little experience</font> on how to do this **properly**\n",
    "- Example, the **name** of a **user-defined** function can iinfluence the <font color='DodgerBlue'>GenAI to create **wrong code**</font> [3]\n",
    "\n",
    "### Conceptual Approaches [4]\n",
    "1. <font color='magenta'>**GenAI**<sub>IDEA</sub></font> \\& <font color='magenta'>**GenAI**<sub>IMPL</sub></font>\n",
    "    - GenAI generates the testing **ideas** (i.e., test cases themselves)\n",
    "    - GenAI generates the **test code** (i.e., implementation of test cases)\n",
    "\n",
    "2. <font color='magenta'>**GenAI**<sub>IDEA</sub></font> \\& <font color='DodgerBlue'>**Human**<sub>IMPL</sub></font>\n",
    "    - GenAI generates the testing **ideas**\n",
    "    - Human writes the actual test code\n",
    "\n",
    "3. <font color='DodgerBlue'>**Human**<sub>IDEA</sub></font> \\& <font color='magenta'>**GenAI**<sub>IMPL</sub></font> (**Recommended**)\n",
    "    - Human generates the testing **ideas**\n",
    "    - GenAI generates the **test code**\n",
    "\n",
    "\n",
    "### Potential Benefits from using GenAI [4]\n",
    "1. **Time-saving**\n",
    "2. Reduced cognitive load (a double-edged sword)\n",
    "\n",
    "### Potential Drawbacks from using GenAI [4]\n",
    "1. **Lack of Ownership**\n",
    "   - feeling less accountable for code's correctness\n",
    "   - more inclined to accept the output without critical evaluation\n",
    "\n",
    "2. **Lack of Quality**\n",
    "   - Misses edge cases or complex logic\n",
    "   - Partially aligned test with the user-defined functional requirements\n",
    "   - Unpolished/duplicated code/tests\n",
    "\n",
    "3. **Lack of Trust**\n",
    "   - Inconsistencies in GenAI outputs across similar prompts\n",
    "   - Trivial test cases\n",
    "   - Redundant or irrelevant tests that added noise rather than insight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Testing GenAI-Generated Code\n",
    "\n",
    "<b>LLMs</b> are capable of generating <b>entire functions</b>, which means that\n",
    "- Code is written <b>faster</b>, but\n",
    "- <b>Not necessarily safer</b> or more <b>correct</b>.\n",
    "    \n",
    "<b>Unit tests</b> serve as the essential\n",
    "- <b>Safety net</b>\n",
    "- <b>Verification step</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts about Unit Tests\n",
    "1. Source of <b>Truth</b>\n",
    "\n",
    "    - Verifying AI-Generated Code (via <b>LLMs</b>):\n",
    "        - excellent at writing <b>syntactically correct</b> code,\n",
    "        - can <b>miss edge cases</b>, or\n",
    "        - <b>misinterpret</b> complex <b>domain logic</b>.\n",
    "    - Unit tests\n",
    "        - Deterministic <font color='DodgerBlue'><b>source of truth</b></font> that **verifies** an AI's output.\n",
    "        - Guards <b>against \"hallucinations\"</b>.\n",
    "\n",
    "2. Maintaining <b>Stability</b> During Rapid Code <b>Iterations</b>\n",
    "\n",
    "    - Safety Net for Refactoring\n",
    "        - GenAI tools <b>enable</b> rapid <b>refactoring / restructuring</b>\n",
    "        - <b>Unit test suite</b> - allows <b>developers</b> to <font color='DodgerBlue'><b>confirm</b> AI-driven refactoring</font> (preventing bugs).\n",
    "\n",
    "    - Faster Debugging (Pinpointing the Error) of complex AI-generated code\n",
    "\n",
    "3. The Core of Effective Prompt Engineering for AI\n",
    "\n",
    "    - Test-Driven Prompting\n",
    "        - The <b>unit test</b> file itself <font color='DodgerBlue'><b>becomes a powerful prompt</b></font> you can give an <b>LLM</b>.\n",
    "            - \"Write Python code that makes this unit test pass\" (vs. less precise/effective \"Write a function to do X.\")\n",
    "\n",
    "    - Defining Edge Cases\n",
    "        - Explicitly define expected behavior for all inputs (e.g., negative numbers, empty arrays, null values).\n",
    "        - Might results in more <font color='DodgerBlue'><b>robust AI code</b></font>.\n",
    "\n",
    "4. Knowledge Transfer\n",
    "    - Onboarding and Documentation - <font color='DodgerBlue'>Unit tests are a form of living documentation</font>\n",
    "        - <b>New</b> developers running unit tests and <b>reading their inputs/assertions</b> is an efficient way to understand the intended <b>code's functionality</b> and <b>requirements</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Take Home Messages\n",
    "\n",
    "In <b>scientific research</b>, it's <b>very important</b> that code produces <b>correct</b> and <b>reproducible</b> results.\n",
    "\n",
    "1. Importance of Testing and Untested Code\n",
    "    - Untested code is problematic because it:\n",
    "        - <b>Reduced confidence</b>\n",
    "        - Causes <b>fear</b> of making <b>changes</b> (refactoring), and\n",
    "        - Is <b>costly</b> debugging\n",
    "    \n",
    "    - Testing <b>flips</b> this dynamic.\n",
    "\n",
    "    - \"I Know It Works\" illusion is dangerous\n",
    "        - <b>Manual checks work temporarily</b> (e.g., print statement), but can change tomorrow (e.g., refactoring)\n",
    "        - Can easily break functionality without detection\n",
    "\n",
    "- <b>Unit Tests - Raising Errors</b>\n",
    "\n",
    "    - A unit test: an 'automated' procedure to verify a small, <b>isolated code piece</b> (e.g., a function) works as intended under different conditions.\n",
    "\n",
    "    - Unit Test's Key Characteristics:\n",
    "        - <b>Isolation</b>: Focuses on the <b>smallest testable part of the code</b>, and must run without external dependencies (e.g., databases).\n",
    "        - <b>Assertion / Raise Statements</b>: Every test contains these checks that <b>stop</b> and <b>report</b> an <b>error</b> if they fail.\n",
    "        - <b>Automation</b>: The test runs without interventions, and can do so through a framework (e.g., pytest, unittest).\n",
    "        - <b>Speed</b>: Executes very quickly since they are constantly rerun.\n",
    "\n",
    "- <b>`try-except` - Error Handling</b>\n",
    "\n",
    "    - `try-except` statement tells your code to\n",
    "        - try a block of code, and\n",
    "        - then specifies what to do if an exception (error) of a certain type occurs.\n",
    "\n",
    "    - Strengths:\n",
    "        - The <b>code continues to run</b> even after <b>encountering a problem</b>.\n",
    "        - Faster than using if statements when the majority of tasks are expected to be successful.\n",
    "\n",
    "    - Weakness:\n",
    "         - They can <b>mask or ignore potential bugs</b>\n",
    "             - Leading to corrupt or incomplete data while the program suggests otherwise (i.e., Error Hiding).\n",
    "         - Better practice: <b>allow logical errors to stop the algorithm</b>, thus notifying the programmer.\n",
    "\n",
    "- <b>Test-Driven Development</b> (TDD)\n",
    "\n",
    "    - TDD involves writing tests before you write your production code.\n",
    "\n",
    "    - Benefits:\n",
    "\n",
    "        - Ensures proper and directed functionality - <b>leads to concise code</b> that does a single thing.\n",
    "\n",
    "        - Helps <b>plan your code</b> and <b>encourages critical thinking</b> about what you actually want to do.\n",
    "\n",
    "        - <b>Reduces errors, ensures reproducibility</b>, and helps ensure a code's long life (<b>maintainability, sustainability</b>).\n",
    "\n",
    "    - TDD Workflow Concept\n",
    " \n",
    "        - Write a failing test,\n",
    "        - Run and ensure failure,\n",
    "        - Write code to pass,\n",
    "        - Run and ensure passing,\n",
    "        - Refactor (restructure/clean up code without changing its final result), and\n",
    "        - Redo\n",
    "\n",
    "- <b>PyTest</b> Tool\n",
    "\n",
    "    - PyTest is a command-line driven testing framework that <b>simplifies and organizes unit tests</b>\n",
    "        - via <b>creating separate user-defined functions for each test</b>.\n",
    "\n",
    "    - The output symbols help quickly assess test results\n",
    " \n",
    "        - `.` for passed,\n",
    "        - `F` for failed, and\n",
    "        - `E` for an unexpected exception\n",
    "\n",
    "- <b>Unit tests</b> are becoming <b>more important</b> in the age of <b>GenAI</b>\n",
    "\n",
    "    - <font color='DodgerBlue'>Recommened Interaction: **Human**<sub>IDEA</sub></font> \\& <font color='magenta'>**GenAI**<sub>IMPL</sub></font>\n",
    "    - GenAI usage is still an uncharted territory\n",
    "        - no **\"absolutes\"** yet\n",
    "        - output is **prone to error**\n",
    "    - Critical thinking is still needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
