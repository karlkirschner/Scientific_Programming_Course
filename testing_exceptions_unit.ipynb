{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert block alert-info alert\">\n",
    "\n",
    "# <center> Scientific Programming in Python\n",
    "\n",
    "## <center>Karl N. Kirschner<br>Bonn-Rhein-Sieg University of Applied Sciences<br>Sankt Augustin, Germany\n",
    "\n",
    "# <center> Testing Inside your Code<br><br>and<br><br>Testing the Code Itself\n",
    "\n",
    "<!-- <br><br> -->\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "**Note**: All user-defined functions in the notebook do not include document strings (i.e. block comments) or internal checks. This is purposely done to focus on the teaching aspects of the lecture. **A full and proper user-defined function would include these.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context: The Hidden Cost of Untested Code\n",
    "\n",
    "\n",
    "**The \"I Know It Works\" Illusion**\n",
    "- Testing code mentally or with a few temporary `print()` statements.\n",
    "    - Works **today** and for this **one case**.\n",
    "    - But tomorrow may be different: a colleague changes a dependency, or the code is refactored\n",
    "        - That small, manual `print()` check is lost.\n",
    "\n",
    "**Take-Home Message**\n",
    "- Untested code is problematic.\n",
    "    - Reduced confidence in it\n",
    "    - Results in fear of making changes\n",
    "    - Costly debugging\n",
    "\n",
    "<br>\n",
    "\n",
    "- Testing flips this dynamic\n",
    "    - Gives you confidence to refactor, upgrade, and scale your code\n",
    "        - If you break something, the tests will catch it\n",
    "    - Debugging is faster and easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Tests\n",
    "\n",
    "- An 'automated' procedure to verify a **small**, **isolated** code piece (i.e., a **unit**).\n",
    "    - Make sure that the unit works as intended under different conditions.\n",
    " \n",
    "## Unit Tests Characteristics\n",
    "\n",
    "- <font color='DodgerBlue'>**Isolation**</font>\n",
    "    - Focuses on the smallest testable part of the code (e.g., single function).\n",
    "    - It must be run in isolation (i.e., doesn't rely on external factors like databases).\n",
    "\n",
    "- <font color='DodgerBlue'>**Assertion / Raise Statements**</font>\n",
    "    - Every unit test contains one or more assertions/raise statements.\n",
    "        - An assertion is a conditional check (e.g., `assert calculated_value == expected_value`).\n",
    "        - If it fails, the test stops and reports an error.\n",
    "\n",
    "- <font color='DodgerBlue'>Automation</font>\n",
    "    - The test is code itself.\n",
    "    - It can be run instantly and repeatedly by a testing framework (e.g., pytest, unittest).\n",
    "    - This is crucial for speed and consistency.\n",
    "\n",
    "- <font color='DodgerBlue'>Speed</font>\n",
    "    - Unit tests must execute very quickly (milliseconds).\n",
    "        - They will be run constantly by developers (or automated build systems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assert vs. Raise\n",
    "\n",
    "- Comparing <font color='DodgerBlue'>two floats</font> to be within a <font color='DodgerBlue'>tolerance</font> range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 0.001\n",
    "\n",
    "calculated_number = 0.115\n",
    "expected_number = 0.117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard `assert`: raises an AssertionError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition\n",
    "- `abs(calculated_number - expected_number)` must be **less than** the `tolerance` value\n",
    "- If `True`, then continue.\n",
    "- If `False`, then **raise error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(calculated_number - expected_number) <= tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Values are too far apart.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(calculated_number \u001b[38;5;241m-\u001b[39m expected_number) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValues are too far apart.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Values are too far apart."
     ]
    }
   ],
   "source": [
    "assert abs(calculated_number - expected_number) <= tolerance, \"Values are too far apart.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Raise: Checks the condition, explicitly raises an exception on failure\n",
    "\n",
    "- `abs(calculated_number - expected_number)` is **greater than** the `tolerance` value\n",
    "- If `True`, then **raise error**:\n",
    "- If `False`, then continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(calculated_number - expected_number) >= tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Error: Expected value was 0.117, but obtained 0.115.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(calculated_number \u001b[38;5;241m-\u001b[39m expected_number) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Expected value was \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_number\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but obtained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcalculated_number\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Error: Expected value was 0.117, but obtained 0.115."
     ]
    }
   ],
   "source": [
    "if abs(calculated_number - expected_number) >= tolerance:\n",
    "    raise AssertionError(f\"Error: Expected value was {expected_number:.3f}, but obtained {calculated_number:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grade(score, max_score=100):\n",
    "    \"\"\" Calculates the percentage grade from a score. \"\"\"\n",
    "    \n",
    "    percentage = (score / max_score) * 100\n",
    "\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n"
     ]
    }
   ],
   "source": [
    "print(calculate_grade(80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look correct - great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_calculate_grade():\n",
    "    \"\"\" Verifies the function's output for known good data. \"\"\"\n",
    "\n",
    "    # 1. Standard case check (80/100 should be 80.0)\n",
    "    assert calculate_grade(80) == 80.0\n",
    "\n",
    "    # 2. Edge case check (max_score is not 100)\n",
    "    assert calculate_grade(25, max_score=50) == 50.0\n",
    "\n",
    "    print(\"Test Passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed.\n"
     ]
    }
   ],
   "source": [
    "test_calculate_grade()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2891776488.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    We will add a `try`-`except` statement\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "We will add a `try`-`except` statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_calculate_grade()\n",
    "except AssertionError:\n",
    "    print(\"TEST FAILED! The test caught a bug that the simple manual check missed.\")\n",
    "    print(\"This is the power of a Unit Test: immediate feedback and regression prevention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's say in 10 months a colleague moves the 100 to an integer 100 global position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grade(score, max_score=100):\n",
    "    \"\"\" Buggy - calculates the percentage grade from a score. \"\"\"\n",
    "    \n",
    "    # BUG: Forgot to multiply by 100\n",
    "    percentage = (score / max_score) \n",
    "    \n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST FAILED! The test caught a bug that the original/earlier simple manual check missed.\n",
      "This is the power of a Unit Test: immediate feedback and regression prevention.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_calculate_grade()\n",
    "except AssertionError:\n",
    "    print(\"TEST FAILED! The test caught a bug that the original/earlier simple manual check missed.\")\n",
    "    print(\"This is the power of a Unit Test: immediate feedback and regression prevention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exceptions: testing for expected conditions\n",
    "<!-- EAFP (\"<font color='DodgerBlue'>**E**asier to **A**sk for **F**orgiveness than **P**ermission</font>\")\n",
    "- often adopted by programmers,\n",
    "- but is that good practice?\n",
    "\n",
    "EAFP can be implemented via the: -->\n",
    "### `try`-`except` statement\n",
    "- tells your code to try something\n",
    "- then tell it what to do if it fails based on an exception type\n",
    "\n",
    " \n",
    "#### Strengths:\n",
    "1. your code will <font color='DodgerBlue'>**continue**</font> when it encounters a problem<br><br>\n",
    "\n",
    "2. <font color='DodgerBlue'>**faster** than if statements</font> for when <font color='DodgerBlue'>majority of the planned tasks are **expected** to be **successful**</font> (i.e., they don't encounter an exception)\n",
    "\n",
    "<br><br>\n",
    "**Simple Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(5/0)\n",
    "except ZeroDivisionError:\n",
    "    print(\"Error: You can't have a zero in the denominator.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A More Sophisticated Example**\n",
    "\n",
    "Let's set up a <font color='DodgerBlue'>division calculator</font> that allows users to input numbers and quit at any time using while and if loops (to demonstrate via a comparison of code).\n",
    "\n",
    "1. First, set something up without `try`-`except` in order to see its advantage later.\n",
    "2. Second, do the same thing with `try`-`except`\n",
    "\n",
    "<font color='DodgerBlue'>Demonstrate the following</font>:\n",
    "1. normal operation\n",
    "2. exiting by typing 'q'\n",
    "3. <font color='Red'>O</font> (i.e., a capital alphabet letter \"O\", as in \"O\"ktoberfest (also demonstrates traceback error))\n",
    "\n",
    "Without `try`-`except` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print('Type two numbers that you want to be divided.')\n",
    "print(\"Type 'q' to quit.\")\n",
    "print()\n",
    "\n",
    "while True:\n",
    "    numerator = input('Numerator = ')\n",
    "    if numerator == 'q':\n",
    "        break\n",
    "\n",
    "    denominator = input('Denominator = ')\n",
    "    if denominator == 'q':\n",
    "        break\n",
    "\n",
    "    if denominator == '0':\n",
    "        print(\"You can't have a zero in the denominator.\")\n",
    "        break\n",
    "\n",
    "    answer = float(numerator)/float(denominator)\n",
    "    print(f'Answer for {numerator}/{denominator} = {answer}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the above code to use a **`try`-`except` statement**, and try it with <font color='Red'>O</font>.\n",
    "\n",
    "**Note**: <font color='DodgerBlue'>Multiple `except` conditions via a **tuple**</font>:<br>\n",
    "`except (ZeroDivisionError, ValueError):`\n",
    "- `ZeroDivisionError` when the denominator is zero\n",
    "- `ValueError` for when a string is given as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Type two numbers that you want to be divided.')\n",
    "print(\"Type 'q' to quit.\")\n",
    "print()\n",
    "\n",
    "while True:\n",
    "    numerator = input('Numerator = ')\n",
    "    if numerator == 'q':\n",
    "        break\n",
    "\n",
    "    denominator = input('Denominator = ')\n",
    "    if denominator == 'q':\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        answer = float(numerator)/float(denominator)\n",
    "        print(f'Answer for {numerator}/{denominator} = {answer}\\n')\n",
    "\n",
    "    except (ZeroDivisionError, ValueError):\n",
    "        print('Your input was either not a number, or you are dividing by a zero.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>Now the code continues to run, even though an error was raised!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "<h1 align='center'>Test Driven Development\n",
    "    \n",
    "<h2 align='center'> a.k.a. Unit Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/library/unittest.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Driven Development - writing tests before you write your production code\n",
    "1. Ensures proper and directed functionality of your code\n",
    " - creating **concise** code that does a **single** thing (e.g., user-defined functions)\n",
    "2. Helps you plan your code - what do you **actually want** to do (critical thinking)\n",
    "3. Reduces **errors**\n",
    "4. Ensures **reproducibility**\n",
    "5. Helps to ensure a code's **long life**\n",
    "\n",
    "## The Workflow Concept\n",
    "1. Write a failing test\n",
    "2. Run and ensure failure\n",
    "3. Write code to pass\n",
    "4. Run and ensure passing\n",
    "5. Refactor (i.e., restructure/clean up code without changing it final result)\n",
    "6. Redo steps 1-5\n",
    "\n",
    "## Scientific and Data Research\n",
    "It is **CRITICAL** that:\n",
    "1. you get the correct results\n",
    "2. you make it generate reproducible results, especially as the code becomes bigger (and changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assert statements that can be used in unittest library\n",
    "\n",
    "https://docs.python.org/3/library/unittest.html#module-unittest\n",
    "\n",
    "\n",
    "| Method | Checks that | New in |\n",
    "|:------|:-:|:-:|\n",
    "| assertEqual(a, b) | a == b | |\n",
    "| assertNotEqual(a, b)| a != b | |\n",
    "| assertTrue(x) | bool(x) is True | |\n",
    "| assertFalse(x) | bool(x) is False | |\n",
    "| assertIs(a, b) | a is b | 3.1 |\n",
    "| assertIsNot(a, b) | a is not b | 3.1 |\n",
    "| assertIsNone(x) | x is None | 3.1 |\n",
    "| assertIsNotNone(x) | x is not None | 3.1 |\n",
    "| assertIn(a, b) | a in b | 3.1 |\n",
    "| assertNotIn(a, b) | a not in b | 3.1 |\n",
    "| assertIsInstance(a, b) | isinstance(a, b) | 3.2 |\n",
    "| assertNotIsInstance(a, b) | not isinstance(a, b) | 3.2 |\n",
    "| | | |\n",
    "| | | |\n",
    "| assertAlmostEqual(a, b) | round(a-b, 7) == 0 | |\n",
    "| assertNotAlmostEqual(a, b) | round(a-b, 7) != 0 | |\n",
    "| assertGreater(a, b) | a > b | 3.1 |\n",
    "| assertGreaterEqual(a, b) | a >= b | 3.1 |\n",
    "| assertLess(a, b) | a < b | 3.1 |\n",
    "| assertLessEqual(a, b) | a <= b | 3.1 |\n",
    "| assertRegex(s, r) | r.search(s) | 3.1 |\n",
    "| assertNotRegex(s, r) | not r.search(s) | 3.2 |\n",
    "| assertCountEqual(a, b) | a and b have the same elements in the same number, regardless of their order. | 3.2 |\n",
    "\n",
    "\n",
    "| Method | Used to compare | New in|\n",
    "|:------|:-:|:-:|\n",
    "| assertMultiLineEqual(a, b) | strings | 3.1 |\n",
    "| assertSequenceEqual(a, b) | sequences | 3.1 |\n",
    "| assertListEqual(a, b) | lists | 3.1 |\n",
    "| assertTupleEqual(a, b) | tuples | 3.1 |\n",
    "| assertSetEqual(a, b) | sets or frozensets | 3.1 |\n",
    "| assertDictEqual(a, b) | dicts | 3.1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate the following two scenarios:\n",
    "1. Scenario 1: the unit test runs with everything correct\n",
    "2. Scenario 2: the unit test runs, but with errors\n",
    "     - A new `assertEqual` is added\n",
    "\n",
    "**Note**: We will include <font color='DodgerBlue'>additional assert statements just to demonstrate how the output of a unit test looks like</font>, even though it is not relevant to our user-defined function.\n",
    "\n",
    "**Scenario 1**: the unit test runs with everything <font color='DodgerBlue'>correctly</font>\n",
    "\n",
    "Define a user-defined function to demo how that is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello_world():\n",
    "    return 'hello world'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: `isupper()`\n",
    "\n",
    "- `str.isupper()`: Return `True` if all characters in a given string are uppercase, otherwise it is `False`.\n",
    "    \n",
    "    - https://docs.python.org/3/library/stdtypes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class MyFirstUniTTests(unittest.TestCase):\n",
    "\n",
    "    def test_isEqual(self):\n",
    "        self.assertEqual(hello_world(), 'hello world')\n",
    "\n",
    "    def test_isLess(self):\n",
    "        self.assertLess(5, 10)\n",
    "\n",
    "    def test_isLessEqual(self):\n",
    "        self.assertLessEqual(10, 10)\n",
    "\n",
    "    def test_isUpperTrue(self):\n",
    "        self.assertTrue('FOO'.isupper())\n",
    "\n",
    "    def test_isUpperFalse(self):\n",
    "        self.assertFalse('Foo'.isupper())\n",
    "\n",
    "\n",
    "## Normal usage (in a .py script)\n",
    "#if __name__ == '__main__':\n",
    "#    unittest.main()\n",
    "\n",
    "## For usage in jupyter and colaboratory (due to the kernel)\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['ignored', '-v'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario 2**: the unit test runs, but some <font color='red'>errors</font> occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstUniTTests(unittest.TestCase):\n",
    "\n",
    "    def test_fail(self):\n",
    "       self.assertEqual(hello_world(), 'bye world')\n",
    "\n",
    "    def test_isEqual(self):\n",
    "        self.assertEqual(hello_world(), 'hello world')\n",
    "\n",
    "    def test_isLess(self):\n",
    "        self.assertLess(5, 10)\n",
    "\n",
    "    def test_isLessEqual(self):\n",
    "        self.assertLessEqual(10, 10)\n",
    "\n",
    "    def test_isUpperTrue(self):\n",
    "        self.assertTrue('FOO'.isupper())\n",
    "\n",
    "    def test_isUpperFalse(self):\n",
    "        self.assertFalse('Foo'.isupper())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['ignored', '-v'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTest\n",
    "\n",
    "1. A command-line (e.g., using a bash shell) driven testing approach\n",
    "2. Simplifies and helps organize unit tests\n",
    "    - done by creating **user-defined functions** for **each test** that you want to do\n",
    "\n",
    "https://docs.pytest.org/en/7.1.x/contents.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_sum.py\n",
    "''' The following will be created:\n",
    "        1. Four unit test functions\n",
    "            a. First 3 will pass\n",
    "            b. Last 1 will fail\n",
    "'''\n",
    "\n",
    "def test_pass_add_list_1():\n",
    "    ''' 1st unit test\n",
    "    '''\n",
    "    test_list = [1, 2, 3, 4]\n",
    "    assert sum(test_list) == 10\n",
    "\n",
    "\n",
    "def test_pass_add_list_2():\n",
    "    ''' 2nd unit test\n",
    "    '''\n",
    "    test_list = [1, 2, 3, 4, 5]\n",
    "    assert sum(test_list) == 15\n",
    "\n",
    "\n",
    "def test_pass_add_list_3():\n",
    "    ''' 3rd unit test\n",
    "    '''\n",
    "    test_list = [1, 2, 3, 4, 5, 6]\n",
    "    assert sum(test_list) == 21\n",
    "\n",
    "\n",
    "def test_fail_add_list_4():\n",
    "    ''' 4th unit test\n",
    "        Should Fail\n",
    "    '''\n",
    "    print('PRINT STATEMENT FOR FAILING TEST FUNCTION.')\n",
    "    \n",
    "    test_list = [1, 2, 3, 4, 5, 6]\n",
    "    assert sum(test_list) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTest will give the following output:\n",
    "- `.` (dot) = test <font color='DodgerBlue'>passed</font>\n",
    "- `F` =  test has <font color='DodgerBlue'>failed</font>\n",
    "- `E` =  test raised an <font color='DodgerBlue'>unexpected exception</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pytest test_sum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**\n",
    "1. The first three test passed\n",
    "2. The fourth test fails\n",
    "3. A traceback is given concerning the error\n",
    "4. None of the print statements are seen\n",
    "\n",
    "<font color='DodgerBlue'>To see print commands</font> within the user-defined functions, <font color='DodgerBlue'>use `-s` option</font>:\n",
    "\n",
    "(`-s` is a shortcut for `--capture=no` - see `pytest --help`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pytest test_sum.py -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm test_sum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Notebook function and PyTesting\n",
    "\n",
    "Create a User-defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_area(length, width):\n",
    "    \"\"\" Calculate the area of a rectangle.\n",
    "    \"\"\"\n",
    "    if length <= 0 or width <= 0:\n",
    "        raise ValueError(\"Dimensions must be positive.\")\n",
    "    return length * width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PyTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_calculate_area.py\n",
    "import pytest\n",
    "\n",
    "def calculate_area(length, width):\n",
    "    \"\"\" Calculate the area of a rectangle.\n",
    "    \"\"\"\n",
    "    if length <= 0 or width <= 0:\n",
    "        raise ValueError(\"Dimensions must be positive.\")\n",
    "    return length * width\n",
    "\n",
    "\n",
    "def test_positive_dimensions():\n",
    "    # Test case with standard positive inputs\n",
    "    assert calculate_area(5, 4) == 20\n",
    "\n",
    "\n",
    "def test_square_dimensions():\n",
    "    # Test case for a square (equal sides)\n",
    "    assert calculate_area(10, 10) == 100\n",
    "\n",
    "\n",
    "def test_zero_or_negative_dimensions():\n",
    "    # Test case for invalid input using pytest.raises\n",
    "    with pytest.raises(ValueError):\n",
    "        calculate_area(-1, 5)\n",
    "    with pytest.raises(ValueError):\n",
    "        calculate_area(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pytest test_calculate_area.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm test_calculate_area.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing `pytest.mark.parametrize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_even.py\n",
    "import pytest\n",
    "\n",
    "\n",
    "def add(number_1: int|float, number_2: int|float) -> float:\n",
    "    \"\"\"\n",
    "    Returns the sum of two numbers.\n",
    "\n",
    "    Parameters:\n",
    "        number_1: The first number.\n",
    "        number_2: The second number.\n",
    "\n",
    "    Returns:\n",
    "        int or float: The sum of a and b.\n",
    "    \"\"\"\n",
    "    return number_1 + number_2\n",
    "\n",
    "\n",
    "def test_add_multiple_inputs():\n",
    "    ''' An initial attempt that uses a for loop within the test (avoid this).\n",
    "    '''\n",
    "    test_data = [(1, 2, 3),\n",
    "                 (-1, -1, -2),\n",
    "                 (5, 0, 5)]\n",
    "    for num_1, num_2, expected in test_data:\n",
    "        assert add(number_1=num_1, number_2=num_2) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -vs test_even.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_add_numbers.py\n",
    "import pytest\n",
    "\n",
    "\n",
    "def add(number_1: int|float, number_2: int|float) -> float:\n",
    "    \"\"\"\n",
    "    Returns the sum of two numbers.\n",
    "\n",
    "    Parameters:\n",
    "        number_1: The first number.\n",
    "        number_2: The second number.\n",
    "\n",
    "    Returns:\n",
    "        int or float: The sum of a and b.\n",
    "    \"\"\"\n",
    "    return number_1 + number_2\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"num_1, num_2, expected\", [(1, 2, 3),        # Test Case 1: Positive numbers\n",
    "                                                    (-1, -1, -2),     # Test Case 2: Negative numbers\n",
    "                                                    (5, 0, 5),        # Test Case 3: Testing with zero\n",
    "                                                    (100, -50, 50)])  # Test Case 4: Positive and negative\n",
    "\n",
    "def test_add(num_1, num_2, expected):\n",
    "    assert add(number_1=num_1, number_2=num_2) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -vs test_add_numbers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm test_add_numbers.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
