{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57df2cba",
   "metadata": {},
   "source": [
    "<div class=\"alert block alert-info alert\">\n",
    "\n",
    "# <center> Scientific Programming in Python\n",
    "\n",
    "## <center>Karl N. Kirschner<br>Bonn-Rhein-Sieg University of Applied Sciences<br>Sankt Augustin, Germany\n",
    "\n",
    "# <center> Neural Network using NumPy\n",
    "\n",
    "## <center> A Perceptron\n",
    "**Original concept**: Justin Johnson at https://sebarnold.net/tutorials/beginner/pytorch_with_examples.html\n",
    "\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf9c2f-9c7d-4742-bce8-f1da7bd8a465",
   "metadata": {},
   "source": [
    "<center><img src=\"00_images/31_machine_learning/nn_perceptron_example_nodes.png\" alt=\"nn_percepton\" style=\"width: 500px;\"/><br>Figure 1: The architecture of a NN perceptron.</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"00_images/31_machine_learning/nn_perceptron_example.png\" alt=\"nn_percepton\" style=\"width: 1000px;\"/><br>Figure 2: The details a NN perception.</center>\n",
    "\n",
    "#### Architecture\n",
    "- \"Fully connected\" network\n",
    "- <font color='DodgerBlue'>**1 Input** layer</font>\n",
    "    - **`X1`**: **2 input samples** (e.g., <font color='DodgerBlue'>2 houses</font>) with features\n",
    "        - 10 features (e.g., number of bedrooms, number of bathrooms, size, etc.)\n",
    "            - Initial values: toy data (see below)\n",
    "    - **`W1`**:**10 sets** of **3 weighting factors**\n",
    "        - Needed due to matrix dot product\n",
    "        - $W1 \\cdot X1 = a \\cdot b = ({\\color{blue}m}, n) \\cdot (n, {\\color{red}k}) = ({\\color{blue}m}, {\\color{red}k})$\n",
    "            - (<font color='DodgerBlue'>2</font>, 10)(10, <font color='red'>3</font>) = (<font color='DodgerBlue'>2</font>, <font color='red'>3</font>))\n",
    "        - initial values: randomly set\n",
    "    - **`B1`**: 2 sets of 3 biases (due to matrix dot product)\n",
    "        - initial values: 0.0\n",
    "\n",
    "<br>\n",
    "\n",
    "- <font color='DodgerBlue'>**1 Hidden** layer</font> (actually <font color='DodgerBlue'>2 layers</font>: **`X2`** and **`Y1`**)\n",
    "    - transformer function (data combination and reduction: <font color='DodgerBlue'>10 $\\rightarrow$ 2 features</font>)\n",
    "    - activation function (adding non-linearity)\n",
    "\n",
    "<br>\n",
    "\n",
    "- <font color='DodgerBlue'>**1 Output** layer</font> (**`Y2`**)\n",
    "    - 1 target feature (e.g., house price)\n",
    "    - 2 predictions - 1 for each of the input houses\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Purpose of Weights and Biases\n",
    "\n",
    "- **Weights**: adjust the <font color='DodgerBlue'>connection strengths</font> between neurons\n",
    "\n",
    "- **Biases**: adjust the <font color='DodgerBlue'>**neurons**</font> themselves (**increasing** or **decreasing** their <font color='DodgerBlue'>**outputs**</font>).\n",
    "    - In effect, they change how the activation function manipulates each neuron.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Needed Mathematics\n",
    "- Linear algebra (matrices)\n",
    "- Calculus (derivatives)\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14fef40e-4304-445d-bc4a-06dbc6236164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46126ee0",
   "metadata": {},
   "source": [
    "## Gradient Descent/Optimization\n",
    "\n",
    "Optimization Problem:\n",
    "- follow the negative of the gradient (i.e., slope; the **first derivative**)\n",
    "    - thus, <font color='DodgerBlue'>move in the direction of the steepest descent</font>\n",
    "\n",
    "**Partial derivative** with respect to the **$\\mathbf{x}$ variable**:\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\nabla_x = \\frac{\\partial}{\\partial x}}\n",
    "\\end{equation}\n",
    "\n",
    "The gradient descent equation is then defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{x_{n+1} = x_n - \\alpha \\nabla_x{y(x)}}\n",
    "\\end{equation}\n",
    "where <font color='DodgerBlue'>$\\mathbf{x_n}$</font> is **current x value** (e.g., initial) and <font color='DodgerBlue'>$\\mathbf{\\alpha}$</font> is the **step size** (i.e., **learning rate**), and <font color='DodgerBlue'>$\\mathbf{x_{n+1}}$</font> is the **new x value**.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### <font color='DodgerBlue'>Example</font>: Gradient descent for a simple one-dimensional function\n",
    "$$\\mathbf{y(x) = x^{2}}$$\n",
    "$$\\mathbf{\\frac{\\partial \\ y(x)}{\\partial x} = 2x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576564e-6923-47bc-a8ba-15d946760d85",
   "metadata": {},
   "source": [
    "Create 2 callable functions:\n",
    "1. $\\mathbf{y(x) = x^2}$ and its gradient value (i.e., $\\mathbf{y'(x) = 2x}$)\n",
    "2. Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d224122a-375a-4d27-bb11-0fa17baf9197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_x_sqrd(x_value: float) -> (float, float):\n",
    "    ''' Evaluate y(x) = x^2 and its derivative.\n",
    "\n",
    "        Args\n",
    "            x_value: input value\n",
    "\n",
    "        Returns\n",
    "            y_value: y(x) value evaluated at x_value\n",
    "            gradient: derivative value evaluated at x_value\n",
    "    '''\n",
    "    y_value = x_value**2\n",
    "    gradient = 2*x_value\n",
    "\n",
    "    return y_value, gradient\n",
    "\n",
    "\n",
    "def gradient_descent(function: callable, x_n: float, learning_rate: float) -> (float, float):\n",
    "    ''' Gradient descent algorithm.\n",
    "\n",
    "        x_n+1 = x_n - learning_rate * gradient\n",
    "\n",
    "        The learning rate is also known as the step size.\n",
    "\n",
    "        Args\n",
    "            function: a 1-dimensional mathematical function\n",
    "            x_n: input value\n",
    "            step_size: how big of a move to take (i.e., learning rate)\n",
    "\n",
    "        Return\n",
    "            x_n1: new x value\n",
    "            gradient: gradient value determined at the x_n input\n",
    "    '''\n",
    "    _, gradient = function(x_n)\n",
    "    x_n1 = x_n - learning_rate * gradient\n",
    "\n",
    "    return x_n1, gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74004f5-7041-4725-b10b-68faaabc04e9",
   "metadata": {},
   "source": [
    "- **Initialize objects**:\n",
    "    - the **initial guess** (and the initial gradient - needed for entering the `while` loop below)\n",
    "    - the stepsize (**learning rate**) for the gradient descent\n",
    "    - a **convergence criteria** for stopping the optimization\n",
    "    - a dictionary to collect results\n",
    "\n",
    "- <font color='dodgerblue'>**Run** the first gradient descent</font>, and collect results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00423716-48e3-49c2-9840-33d6e8dc0a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [2.0, 4.0, 4.0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_guess = 2.0\n",
    "learning_rate = 0.1\n",
    "convergence_criteria = 0.01\n",
    "\n",
    "y_value, gradient = function_x_sqrd(x_value=x_guess) ## the initial gradient\n",
    "\n",
    "iteration_dict = {0: [x_guess, y_value, gradient]}\n",
    "\n",
    "iteration_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07054462-fd5b-48c2-b523-34ec4a20c23e",
   "metadata": {},
   "source": [
    "- Perform <font color='dodgerblue'>**gradient optimization**</font> until the **convergence criteria** is **acheived**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef56917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y(x)</th>\n",
       "      <th>gradient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.560000</td>\n",
       "      <td>3.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.280000</td>\n",
       "      <td>1.638400</td>\n",
       "      <td>2.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.024000</td>\n",
       "      <td>1.048576</td>\n",
       "      <td>2.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.819200</td>\n",
       "      <td>0.671089</td>\n",
       "      <td>1.638400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.655360</td>\n",
       "      <td>0.429497</td>\n",
       "      <td>1.310720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.524288</td>\n",
       "      <td>0.274878</td>\n",
       "      <td>1.048576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.419430</td>\n",
       "      <td>0.175922</td>\n",
       "      <td>0.838861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.335544</td>\n",
       "      <td>0.112590</td>\n",
       "      <td>0.671089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.268435</td>\n",
       "      <td>0.072058</td>\n",
       "      <td>0.536871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.214748</td>\n",
       "      <td>0.046117</td>\n",
       "      <td>0.429497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.171799</td>\n",
       "      <td>0.029515</td>\n",
       "      <td>0.343597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.137439</td>\n",
       "      <td>0.018889</td>\n",
       "      <td>0.274878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.109951</td>\n",
       "      <td>0.012089</td>\n",
       "      <td>0.219902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.087961</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>0.175922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.070369</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>0.140737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.056295</td>\n",
       "      <td>0.003169</td>\n",
       "      <td>0.112590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.045036</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>0.090072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.036029</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.072058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.028823</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.057646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.023058</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.046117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.018447</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.036893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.014757</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.029515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.011806</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.023612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.009445</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.018889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.007556</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.015112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.006045</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.012089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.009671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.003869</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.007737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x      y(x)  gradient\n",
       "0   2.000000  4.000000  4.000000\n",
       "1   1.600000  2.560000  3.200000\n",
       "2   1.280000  1.638400  2.560000\n",
       "3   1.024000  1.048576  2.048000\n",
       "4   0.819200  0.671089  1.638400\n",
       "5   0.655360  0.429497  1.310720\n",
       "6   0.524288  0.274878  1.048576\n",
       "7   0.419430  0.175922  0.838861\n",
       "8   0.335544  0.112590  0.671089\n",
       "9   0.268435  0.072058  0.536871\n",
       "10  0.214748  0.046117  0.429497\n",
       "11  0.171799  0.029515  0.343597\n",
       "12  0.137439  0.018889  0.274878\n",
       "13  0.109951  0.012089  0.219902\n",
       "14  0.087961  0.007737  0.175922\n",
       "15  0.070369  0.004952  0.140737\n",
       "16  0.056295  0.003169  0.112590\n",
       "17  0.045036  0.002028  0.090072\n",
       "18  0.036029  0.001298  0.072058\n",
       "19  0.028823  0.000831  0.057646\n",
       "20  0.023058  0.000532  0.046117\n",
       "21  0.018447  0.000340  0.036893\n",
       "22  0.014757  0.000218  0.029515\n",
       "23  0.011806  0.000139  0.023612\n",
       "24  0.009445  0.000089  0.018889\n",
       "25  0.007556  0.000057  0.015112\n",
       "26  0.006045  0.000037  0.012089\n",
       "27  0.004836  0.000023  0.009671\n",
       "28  0.003869  0.000015  0.007737"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iteration = 0\n",
    "\n",
    "while gradient > convergence_criteria:\n",
    "    iteration += 1\n",
    "\n",
    "    new_x, gradient = gradient_descent(function=function_x_sqrd, x_n=x_guess, learning_rate=learning_rate)\n",
    "    \n",
    "    new_y, new_grad = function_x_sqrd(new_x)\n",
    "\n",
    "    iteration_dict[iteration] = [new_x, new_y, new_grad]\n",
    "\n",
    "    x_guess = new_x # updated guess\n",
    "\n",
    "iteration_pd = pd.DataFrame.from_dict(iteration_dict, orient='index', columns=['x', 'y(x)', 'gradient'])\n",
    "iteration_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e58eac8-48ea-43e9-b545-1dd9edffb486",
   "metadata": {},
   "source": [
    "- Visualize the optimization iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2823a0dc-2f28-4c82-9e44-cde673de79ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'y(x)')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(iteration_pd) - 1):\n",
    "    x_curr, y_curr = iteration_pd.iloc[i]['x'], iteration_pd.iloc[i]['y(x)']\n",
    "    x_next, y_next = iteration_pd.iloc[i+1]['x'], iteration_pd.iloc[i+1]['y(x)']\n",
    "\n",
    "    triangle_x = [x_curr, x_next, x_next]\n",
    "    triangle_y = [y_curr, y_curr, y_next]\n",
    "\n",
    "    plt.fill(triangle_x, triangle_y, color='DodgerBlue', alpha=0.2)\n",
    "\n",
    "plt.plot(iteration_pd['x'], iteration_pd['y(x)'], color='black', alpha=0.3)\n",
    "plt.scatter(iteration_pd['x'], iteration_pd['y(x)'], color='red', label='iteration')\n",
    "\n",
    "## Annotation\n",
    "for index, row in iteration_pd.iterrows():\n",
    "    plt.text(row['x'] + 0.0, row['y(x)'] + 0.03, str(index),\n",
    "             fontsize=10, color='DodgerBlue', ha='right', va='bottom')  \n",
    "\n",
    "plt.xlabel(xlabel='x')\n",
    "plt.ylabel(ylabel='y(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f120f-18dd-4c69-bf6e-f1ce67dc4bee",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "## Fully-Connected Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219cfac7-3b0f-4b6d-a577-2fad509831d0",
   "metadata": {},
   "source": [
    "#### <font color='dodgerblue'>Activation Function</font>\n",
    "\n",
    "**Purpose**\n",
    "- introduce **non-linearity** into the system\n",
    "\n",
    "Without activation functions, the **nodes are linearly connected** (i.e., <font color='dodgerblue'>see equation below$</font>) due to the **matrix multiplication**.\n",
    "\n",
    "<font color='dodgerblue'>$y(x) = w\\cdot x + b$</font>\n",
    "- where `w` is a weight\n",
    "- `b` is the bias\n",
    "\n",
    "If all nodes are computed using the **summation of linear functions**, then the end **prediction** will be **linearly connected to the input**.\n",
    "\n",
    " \n",
    "**Source**:\n",
    "- https://towardsdatascience.com/the-importance-and-reasoning-behind-activation-functions-4dc00e74db41\n",
    "\n",
    "<br>\n",
    "\n",
    "Define the <font color='dodgerblue'>**activation function**</font>: a **rectified linear unit** (<font color='dodgerblue'>ReLU</font>)\n",
    "- https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "\n",
    "\\begin{equation}\n",
    "    ReLU(x) = max(0, x) = \\frac{x + |x|}{2} =\n",
    "    \\begin{cases}\n",
    "        x,& \\text{if } x>1\\\\\n",
    "        0,              & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Visualize**: What does the ReLU function look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ef803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.array, bias: float=0.0) -> np.array:\n",
    "    ''' A rectified linear unit function.\n",
    "\n",
    "        Args:\n",
    "            x:    The input array,\n",
    "            bias: A scalar bias value to be added to each element of `x`\n",
    "                       (ie., shifts the plot on x axis)\n",
    "\n",
    "        Returns:\n",
    "            Values of `x` where all negative values (after adding bias)\n",
    "                have been replaced by 0, while positive values remain unchanged.\n",
    "    '''\n",
    "    return np.maximum(0, x + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39c1fc-ff22-4ab5-b6b7-59abc0a5616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relu(bias:float):\n",
    "    '''Plot the ReLU activation function.'''\n",
    "    x = np.linspace(-10, 20, 100)\n",
    "\n",
    "    relu_x = relu(x=x, bias=bias)  \n",
    "\n",
    "    plt.plot(x, relu_x)\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Output')\n",
    "\n",
    "plot_relu(bias=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554899d-704d-483b-abf6-3f1990bb6805",
   "metadata": {},
   "source": [
    "#### <font color='dodgerblue'>Loss Function</font>\n",
    "\n",
    "**Definition of Loss Function**: a function that <font color='dodgerblue'>**quantifies the error**</font> between the **predicted** value and the **actual** value.\n",
    "\n",
    "Define the loss function. This is needed for \n",
    "\n",
    "1. evaluate the predictions, and\n",
    "2. to take its **gradient**, then we can <font color='dodgerblue'>**optimize**</font> the <font color='dodgerblue'>**weights**</font> and <font color='dodgerblue'>**biases**</font>.\n",
    "\n",
    "A common **loss function** is the **mean squared error**, which is also known as **L2 loss**.\n",
    "\n",
    "\\begin{equation}\n",
    "    MSE = \\frac{1}{n} \\sum_{i=1}^n (y_{\\text{prediction}_i} - y_{\\text{target}_i})^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49139a-8473-4345-b051-3c94a0c87a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(predicted: np.array, target: np.array) -> np.array:\n",
    "    ''' Mean squared error loss function.\n",
    "\n",
    "        Provides a single metric for the loss values.\n",
    "    '''\n",
    "    return np.mean(np.square(predicted - target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b098c5-6e72-4822-bf80-51822e2b7ed4",
   "metadata": {},
   "source": [
    "Consequently, the **gradient** of the **loss function** would be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc24c2-dc3b-43d9-bd0d-057456c6a972",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\nabla_y = \\frac{\\partial}{\\partial y} = \\frac{2}{n}*(y_{\\text{prediction}_i} - y_{\\text{target}_i})\n",
    "\\end{equation}\n",
    "\n",
    "**Note**: We will drop the $n$, which basically makes the learning rate bigger, and the optimization faster (also simplifies the function/equation some)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c6f5d7-4e88-4a7c-9d4b-1f2c56fdf5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_gradient(predicted: np.array, target: np.array) -> np.array:\n",
    "    ''' Gradient of the mean squared error loss function.\n",
    "            i.e., 2.0 * (output_Y2 - target_Y2)\n",
    "    '''\n",
    "    gradient = np.multiply(2, np.subtract(predicted, target))\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30177e79-7ac5-4879-bf5a-33d9ba54366e",
   "metadata": {},
   "source": [
    "#### <font color='dodgerblue'>Toy Data</font>\n",
    "For <u>teaching purposes</u>, a random **seed** will be **explicitly set**, allowing for **reproducible results**.\n",
    "<!-- The first **epoch** data generated below should correspond to the numeric values given in the figure above. -->\n",
    "\n",
    "The object naming will also be done to parallel the figure above. `rng` (random number generator) follows the example given by NumPy.\n",
    "\n",
    "<font color='dodgerblue'>Random Number Generator in NumPy</font>:\n",
    "- `np.random.default_rng`: https://numpy.org/doc/stable/reference/random/generator.html\n",
    "- via a normal (Gaussian) distribution using `numpy.random.Generator.normal`: https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Define some toy data:\n",
    "- **input x** values\n",
    "- **target y** values (for computing the loss function)\n",
    "- two **initial weight** matrices\n",
    "- two **initial bias** matrices (set initially to zero)\n",
    "\n",
    "**Important Note**: Normally with <font color='dodgerblue'>real-world data</font>, one often <font color='dodgerblue'>normalizes</font> (e.g., data **transpose** to a range [0, 1]) the <font color='dodgerblue'>input data</font>. This helps the mathematics when different input features have **large magnitude differences** (e.g., feature 1: 1.5 and feature 2 : 2.5e6).\n",
    "- https://en.wikipedia.org/wiki/Normalization_(statistics)\n",
    "- `sklearn.preprocessing.normalize`: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html\n",
    "\n",
    "In this example, the generated data will have the same magnitude, so *no* normalization is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed4eb9-fd95-4450-9688-8601e36e61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=12345)\n",
    "\n",
    "input_X1 = rng.normal(size=(2, 10))\n",
    "target_Y2 = rng.normal(size=(2, 1))\n",
    "\n",
    "weight_W1 = rng.normal(size=(10, 3))\n",
    "weight_W2 = rng.normal(size=(3, 1))\n",
    "\n",
    "bias_B1 = np.full((2, 3), 0.0)  # shape of middle layer\n",
    "bias_B2 = np.full((2, 1), 0.0)  # shape of output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d4bcf-4470-4fa1-a397-aa1d27f60b3a",
   "metadata": {},
   "source": [
    "Let's look at the resulting arrays:\n",
    "- shape\n",
    "- values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282da9d0-b544-4ddb-8601-4d20ccd1cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_array_specs(in_arrays: dict):\n",
    "    ''' Helper function for nicely printing NumPy arrays' info.\n",
    "\n",
    "        Print: shape, data type and values\n",
    "    '''\n",
    "    for key, value in in_arrays.items():\n",
    "        print(f'{key}:\\n{value.shape}, {value.dtype}')\n",
    "        print(f'{value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb52ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_ini = {'input_X1': input_X1, 'target_Y2': target_Y2,\n",
    "               'weight_W1': weight_W1, 'weight_W2': weight_W2,\n",
    "               'bias_B1': bias_B1, 'bias_B2': bias_B2}\n",
    "\n",
    "print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca933a48-849f-491a-8a98-f4fa623d1057",
   "metadata": {},
   "source": [
    "### <font color='dodgerblue'>NN Steps in Detail</font>\n",
    "\n",
    "<center><img src=\"00_images/31_machine_learning/nn_perceptron_example.png\" alt=\"nn_percepton\" style=\"width: 1000px;\"/></center>\n",
    "\n",
    "Here is a detailed explanation of what will happen in the following code cell.\n",
    "\n",
    "Two major data propagation steps:\n",
    "1. Forward\n",
    "2. Backward\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Forward Propagation\n",
    "\n",
    "Perform the math, starting at the neural network's beginning and working progressively towards the end.\n",
    "\n",
    "1. <font color='dodgerblue'>$X2 = X1\\cdot W1 + B1$</font>\n",
    "    - In words: **transform X1 input** data into fewer dimensions\n",
    "    - `transformer function`: **linear combination** (a.k.a. weighted sum)\n",
    "    - matrix multiplication results in a **fully-connected** neural network\n",
    "    - `X1` and `X2` are **linearly connected**\n",
    "\n",
    "<br>\n",
    "\n",
    "2. <font color='dodgerblue'>$Y1 = \\text{ReLu}(X2)$</font>\n",
    "    - In words: activation that results in a <font color='dodgerblue'>non-linear output data</font> (a **node-wise operation**)\n",
    "   - `X1` and `Y1` are now **non-linearly connected**\n",
    "\n",
    "<br>\n",
    "\n",
    "3. <font color='dodgerblue'>$Y2 = Y1\\cdot W2 + B2$</font>\n",
    "    - In words: **transform Y1 output** data into fewer dimensions\n",
    "    - matrix multiplication results in a **fully-connected** neural network\n",
    "\n",
    "<br>\n",
    "\n",
    "4. <font color='dodgerblue'>$\\text{L (i.e., Loss)} = (Y2- \\text{y target})^2$</font>\n",
    "    - In words: **MSE loss** is computed\n",
    "\n",
    "<br>\n",
    "\n",
    "5. <font color='dodgerblue'>$\\text{grad\\_loss} = \\frac{\\partial L}{\\partial x}$</font>\n",
    "    - In words: <font color='dodgerblue'>gradient</font> of the **loss** (see `mse_loss_gradient` above)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. Backward Propagation\n",
    "\n",
    "Now perform the math, starting at the neural network's end and working progressively towards the beginning.\n",
    "\n",
    "##### Update W2 Weights\n",
    "\n",
    "6. <font color='dodgerblue'>$\\text{grad\\_W2} = Y1^T \\cdot \\text{grad\\_loss}$</font>\n",
    "\n",
    "    or rephrased\n",
    "   \n",
    "   <font color='dodgerblue'>$\\text{grad\\_W2} = Y1^T \\cdot \\frac{\\partial L}{\\partial x}$</font>\n",
    "    - In words: <font color='dodgerblue'>gradient</font> of the <font color='dodgerblue'>weights</font> **connecting** the **hidden layer** (i.e., after applying the ReLU activation) to the output layer\n",
    "    - the **gradient of the loss** concerning the **weights W2**\n",
    "    - each **element** in `grad_w2` reflects **how much** a **specific weight** should be **adjusted** based on the **hidden layer's output** and the corresponding **output error**\n",
    " \n",
    "<br>\n",
    "\n",
    "7. <font color='dodgerblue'>$W2_{new} = W2_{old} - \\text{learning rate} * \\text{grad\\_W2}$</font>\n",
    "    - In words: **gradient descent** that **updates** the **weights W2**\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Update W1 Weights\n",
    "\n",
    "8. <font color='dodgerblue'>$\\text{grad\\_Y1} = \\frac{\\partial L}{\\partial x} \\cdot W2^T$</font>\n",
    "    - In words: **gradients** concerning the **hidden layer output** (after applying the ReLU activation)\n",
    "    -  the **gradient of the loss** concerning the **hidden layer's output**\n",
    "    -  quantifies how **changes** in the **hidden layer's output** (quantified by the weight values) affect the **overall loss**\n",
    "\n",
    "<br>\n",
    "\n",
    "9. <font color='dodgerblue'>rev_X2[X2 < 0] = 0</font>\n",
    "    - In words: **Zero out the negative gradients**, which ensures that any **hidden unit** that was **inactive** (i.e., had a **negative** input to **ReLU**) **does not contribute** to the **gradient** (i.e., its value is set to zero).\n",
    "    - consequently, the inactive units <font color='dodgerblue'>do not influence</font> the curent caluculation\n",
    "\n",
    "<br>\n",
    "\n",
    "10. <font color='dodgerblue'>$\\text{grad\\_W1} = X1^T \\cdot \\text{rev\\_X2}$</font>\n",
    "    - In words: **gradients** concerning the **weights W1** that connect the **input** layer to the **hidden** layer.\n",
    "    - represents **how the loss changes** with respect to the **weights** connecting the **input layer** to the **hidden layer**\n",
    " \n",
    "<br>\n",
    "\n",
    "11. <font color='dodgerblue'>$W1_{new} = W1_{old} - \\text{learning rate} * \\text{grad\\_W1}$</font>\n",
    "    - In words: **gradient descent** that **updates** the **weights W1**\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Update B2 Bias\n",
    "12. <font color='dodgerblue'>grad_B2 = grad_loss.sum(axis=0, keepdims=True)</font>\n",
    "    - In words: the **gradients** concerning `bias_B2` is calculated as the **sum of `grad_loss`** along each node (the rows).\n",
    "    - Relatively straightforward since they mainly serve as simple offsets \n",
    "\n",
    "<br>\n",
    "\n",
    "13. <font color='dodgerblue'>$B2_{new} = B2_{old} - \\text{learning rate} * \\text{grad\\_B2}$</font>\n",
    "    - In words: **gradient descent** that **updates** the **bias B2**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Update B1 Bias\n",
    "14. <font color='dodgerblue'>grad_B1 = rev_X2.sum(axis=0, keepdims=True)</font>\n",
    "    - In words: the **gradients** concerning `bias_B1` is calculated as the **sum of `grad_loss`** along each node (the rows), **after reversing** the ReLU **activation**.\n",
    "    - Relatively straightforward since they mainly serve as simple offsets \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "15. <font color='dodgerblue'>$B1_{new} = B1_{old} - \\text{learning rate} * \\text{grad\\_B1}$</font>\n",
    "    - In words: **gradient descent** that **updates** the **bias B1**\n",
    "\n",
    "<br>\n",
    "\n",
    "####  Neural Network Training\n",
    "**Repeat** the **forward** and **backward propagation** for many **iterations** (a.k.a. <font color='dodgerblue'>epochs</font>)\n",
    "- done for a finite number of times\n",
    "- done until specified loss convergence is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c517334-b742-4b63-bc44-acfa7b695452",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "#### New Term:\n",
    "**An Epoch**: one complete pass of the entire neural network (forward and backward propagation).\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "#### <font color='dodgerblue'>Final objects that are needed</font>\n",
    "- a **learning rate** (e.g., 1.0e-3)\n",
    "- a **convergence criteria** (e.g., 1.5)\n",
    "\n",
    "- **results container**: empty dictionary to store the iterations\n",
    "- **initial large loss value** (to start the `while` loop)\n",
    "- an iteration **counter**\n",
    "\n",
    "\n",
    "Note: I will heavily comment on the code below for teaching purposes since it is the first time encountering forward and backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e48302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "loss = 2\n",
    "convergence_criteria = 1.5\n",
    "iteration = 0\n",
    "learning_rate = 1.0e-3\n",
    "\n",
    "while loss > convergence_criteria:\n",
    "    if iteration > 50: # set maximum iterations to prevent possible infinite runs\n",
    "        break\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "    ## forward propagation\n",
    "    # transform\n",
    "    X2 = input_X1.dot(weight_W1) + bias_B1\n",
    "\n",
    "    # activate\n",
    "    Y1 = relu(x=X2)\n",
    "\n",
    "    # transform\n",
    "    output_Y2 = Y1.dot(weight_W2) + bias_B2\n",
    "\n",
    "    loss = mse_loss(predicted=output_Y2, target=target_Y2)\n",
    "\n",
    "    ## backward propagation\n",
    "    ## gradient of loss function\n",
    "    grad_loss = mse_loss_gradient(predicted=output_Y2, target=target_Y2)\n",
    "\n",
    "    ### update weight_W2\n",
    "    grad_W2 = Y1.T.dot(grad_loss)\n",
    "\n",
    "    ## gradient decent: w2_1 = w2_0 - learning_rate.(Y1.T).(dL/dx)\n",
    "    # weight_W2 = weight_W2 - learning_rate*grad_Y1\n",
    "    weight_W2 = np.subtract(weight_W2, learning_rate * grad_W2)\n",
    "\n",
    "    ## update bias_B2\n",
    "    grad_B2 = grad_loss.sum(axis=0, keepdims=True)\n",
    "    bias_B2 = np.subtract(bias_B2, learning_rate * grad_B2)\n",
    "\n",
    "    ### update weight_W1\n",
    "    grad_Y1 = grad_loss.dot(weight_W2.T)\n",
    "\n",
    "    ## reversing ReLu\n",
    "    rev_X2 = grad_Y1.copy()\n",
    "    rev_X2[X2 < 0] = 0\n",
    "\n",
    "    grad_W1 = input_X1.T.dot(rev_X2)\n",
    "\n",
    "    ## gradient decent: w1_1 = w1_0 - learning_rate.(X1.T).(dL/dx.grad_Y1)\n",
    "    # weight_W1 = weight_W1 - learning_rate*grad_W1\n",
    "    weight_W1 = np.subtract(weight_W1, learning_rate * grad_W1)\n",
    "\n",
    "    ### update bias_B1\n",
    "    grad_B1 = rev_X2.sum(axis=0, keepdims=True)  # the propagated gradient of the loss\n",
    "    bias_B1 = np.subtract(bias_B1, learning_rate * grad_B1)\n",
    "\n",
    "    results[f'{iteration}'] = [output_Y2, loss, weight_W1, weight_W2, bias_B1, bias_B2]\n",
    "\n",
    "    print(f'Iteration {iteration}: Loss = {loss.item():.3f}')\n",
    "\n",
    "    ## Uncomment the following to see iteration details\n",
    "    # objects_ini = {'weight_W1': weight_W1, 'bias_B1': bias_B1,\n",
    "    #                'X2': X2, 'Y1': Y1,\n",
    "    #                'weight_W2': weight_W2, 'bias_B2': bias_B2, \n",
    "    #                'output_Y2': output_Y2,\n",
    "    #                'loss': loss, 'grad_loss': grad_loss,\n",
    "    #                'grad_W2': grad_W2, 'grad_B2': grad_B2, 'grad_Y1': grad_Y1,\n",
    "    #                'rev_X2': rev_X2,\n",
    "    #                'grad_W1': grad_W1, 'weight_W1': weight_W1, 'grad_B1': grad_B1}\n",
    "\n",
    "    # print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8473cd-b4f2-4042-a17d-85f80f75db8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2a757b-1b5c-480d-a9d8-e1ec10784e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_pd = pd.DataFrame.from_dict(results, orient='index', columns=['y_predicted', 'loss',\n",
    "                                                                        'weight_W1', 'weight_W2',\n",
    "                                                                        'bias_b1', 'bias_b2'])\n",
    "iteration_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bda840-d706-49e7-be9d-536c69dfe6de",
   "metadata": {},
   "source": [
    "Let's **visualize** the results for **weights_w2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40962283-429c-4977-bd09-ebd88aa7986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w2 = pd.DataFrame()\n",
    "\n",
    "for iteration, values in results.items():\n",
    "    weights_w2_list = [x.tolist() for x in results[iteration][3]]\n",
    "    weights_w2_list = [item for sublist in weights_w2_list for item in sublist]\n",
    "    df_w2 = pd.concat([df_w2, pd.DataFrame([weights_w2_list])], ignore_index=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for iteration in range(0, 3, 1):\n",
    "    ax.plot(df_w2.index, df_w2[iteration], label=f'weight: {iteration}')\n",
    "    ax.scatter(df_w2.index, df_w2[iteration]) \n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff48fc2-547b-4a9c-86b4-2f7cc2013425",
   "metadata": {},
   "source": [
    "Visualize the loss value and the **predicted y value** of the **second observable**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02adfc1-ae9c-46e5-8dcb-489242172fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "observable_2 = []\n",
    "\n",
    "for values in iteration_pd['y_predicted'].values:\n",
    "    observable_2.append(values[1])\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(iteration_pd.index.values, iteration_pd['loss'], label='loss value')\n",
    "ax2.scatter(iteration_pd.index.values, observable_2, color='red', label='current value') \n",
    "ax2.hlines(target_Y2[1],\n",
    "           min(iteration_pd.index.astype(int)-1),\n",
    "           max(iteration_pd.index.astype(int)-1), colors='red',\n",
    "           linestyles='dashed', label='target value')\n",
    "\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.tick_params(\"x\", labelrotation=90)\n",
    "\n",
    "ax1.set_ylabel('Loss', color='DodgerBlue')\n",
    "ax2.set_ylabel('Observable 2', color='red')\n",
    "\n",
    "ax1.legend(loc='upper center')\n",
    "ax2.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61810754-7996-4cdf-8657-025dd6ee3ee9",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "### NumPy Neural Network Summary:\n",
    "1. created a simple **perceptron** neural network using **NumPy only**\n",
    "2. toy data creation\n",
    "3. **weights**: adjust the **relationship between nodes**\n",
    "4. **biases**: adjust the **nodes themselves** (for their activation)\n",
    "5. **activation functions**\n",
    "    - adds some **nonlinearity** to the mathematics\n",
    "    - ReLU\n",
    "6. **gradient optimization**\n",
    "7. **loss function**\n",
    "8. detailed **explanation** of each **mathematical** step in a neural network\n",
    "9. **forward propagation**\n",
    "10. **backward propagation**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
