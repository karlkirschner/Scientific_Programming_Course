{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57df2cba",
   "metadata": {},
   "source": [
    "<div class=\"alert block alert-info alert\">\n",
    "\n",
    "# <center> Scientific Programming in Python\n",
    "\n",
    "## <center>Karl N. Kirschner<br>Bonn-Rhein-Sieg University of Applied Sciences<br>Sankt Augustin, Germany\n",
    "\n",
    "# <center> Neural Network using NumPy\n",
    "\n",
    "## <center> A Perceptron\n",
    "**Original concept**: Justin Johnson at https://sebarnold.net/tutorials/beginner/pytorch_with_examples.html\n",
    "\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf9c2f-9c7d-4742-bce8-f1da7bd8a465",
   "metadata": {},
   "source": [
    "<center><img src=\"00_images/31_machine_learning/nn_perceptron_example_nodes.png\" alt=\"nn_percepton\" style=\"width: 500px;\"/></center>\n",
    "\n",
    "<center><img src=\"00_images/31_machine_learning/nn_perceptron_example.png\" alt=\"nn_percepton\" style=\"width: 1000px;\"/></center>\n",
    "\n",
    "#### Architecture\n",
    "- \"Fully connected\" network\n",
    "- 1 `X1` input layer\n",
    "    - 2 input samples (e.g., 2 houses) with certain features\n",
    "    - 10 features (e.g., number of bedrooms, number of bathrooms, size, etc.)\n",
    "        - initial values: toy data (see below)\n",
    "    - 10 sets of 3 weighting factors (needed due to matrix dot product: (<font color='dodgerblue'>2</font>,10)(10,<font color='red'>3</font>) = (<font color='dodgerblue'>2</font>,10)(?,<font color='red'>?</font>) = (<font color='dodgerblue'>10</font>,<font color='red'>3</font>))\n",
    "        - initial values: randomly set\n",
    "    - 2 sets of 3 biases (due to matrix dot product)\n",
    "        - initial values: 0.0\n",
    "\n",
    "<br>\n",
    "\n",
    "- 1 hidden layer (actually 2 layers: `X2` and `Y1`)\n",
    "    - transformer function (data combination and reduction)\n",
    "    - activation function (adding non-linearity)\n",
    "\n",
    "<br>\n",
    "\n",
    "- 1 `Y2` output layer\n",
    "    - 1 target feature (e.g., house price)\n",
    "    - 2 predictions - 1 for each of the input houses\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Purpose of Weights and Biases\n",
    "\n",
    "**Weights**: adjust the connection strengths between neurons\n",
    "\n",
    "**Biases**: adjust the **neurons** themselves, **increasing** or **decreasing** their **outputs**. In effect, they change how the activation function manipulates each neuron.\n",
    "\n",
    "\n",
    "#### Needed Mathematics\n",
    "- linear algebra (matrices)\n",
    "- calculus (derivatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fef40e-4304-445d-bc4a-06dbc6236164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46126ee0",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "## Gradient Descent/Optimization\n",
    "\n",
    "Optimization Problem:\n",
    "- follow the negative of the gradient (i.e., slope; the **first derivative**)\n",
    "    - thus, <font color='dodgerblue'>move in the direction of the steepest descent</font>\n",
    "\n",
    "**Partial derivative** with respect to the **x variable**:\n",
    "\\begin{equation}\n",
    "    \\nabla_x = \\frac{\\partial}{\\partial x}\n",
    "\\end{equation}\n",
    "\n",
    "The gradient descent equation is then defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "    x_{n+1} = x_n - \\alpha \\nabla_x{y(x)}\n",
    "\\end{equation}\n",
    "where <font color='dodgerblue'>$x_n$</font> is **current x value** (e.g., initial) and <font color='dodgerblue'>$\\alpha$</font> is the **step size** (i.e., **learning rate**), and <font color='dodgerblue'>$x_{n+1}$</font> is the **new x value**.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### <font color='dodgerblue'>Example</font>: Gradient descent for a simple one-dimensional function\n",
    "$$y(x) = x^{2}$$\n",
    "$$\\frac{\\partial \\ y(x)}{\\partial x} = 2x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d224122a-375a-4d27-bb11-0fa17baf9197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_x_sqrd(x_value: float) -> (float, float):\n",
    "    ''' Evaluate y(x) = x^2 and its derivative.\n",
    "\n",
    "        Args\n",
    "            x_value: input value\n",
    "\n",
    "        Returns\n",
    "            y_value: y(x) value evaluated at x_value\n",
    "            gradient: derivative value evaluated at x_value\n",
    "    '''\n",
    "    y_value = x_value**2\n",
    "    gradient = 2*x_value\n",
    "\n",
    "    return y_value, gradient\n",
    "\n",
    "\n",
    "def gradient_descent(function: callable, x_n: float, learning_rate: float) -> (float, float):\n",
    "    ''' Gradient descent algorithm.\n",
    "\n",
    "        x_n+1 = x_n - learning_rate * gradient\n",
    "\n",
    "        The learning rate is also known as the step size.\n",
    "\n",
    "        Args\n",
    "            function: a 1-dimensional mathematical function\n",
    "            x_n: input value\n",
    "            step_size: how big of a move to take (i.e., learning rate)\n",
    "\n",
    "        Return\n",
    "            x_n1: new x value\n",
    "            gradient: gradient value determined at the x_n input\n",
    "    '''\n",
    "    _, gradient = function(x_n)\n",
    "    x_n1 = x_n - learning_rate * gradient\n",
    "\n",
    "    return x_n1, gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74004f5-7041-4725-b10b-68faaabc04e9",
   "metadata": {},
   "source": [
    "- Initialize objects:\n",
    "    - the **initial guess** (and the initial gradient - needed for entering the `while` loop below)\n",
    "    - the stepsize/**learning rate** (for the gradient descent)\n",
    "    - a **convergence criteria** for stopping the optimization\n",
    "    - a dictionary to collect results\n",
    "\n",
    "- <font color='dodgerblue'>Run the first gradient descent</font>, and collect results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00423716-48e3-49c2-9840-33d6e8dc0a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_guess = 2.0\n",
    "learning_rate = 0.1\n",
    "convergence_criteria = 0.01\n",
    "\n",
    "y_value, gradient = function_x_sqrd(x_value=x_guess) ## the initial gradient\n",
    "\n",
    "iteration_dict = {0: [x_guess, y_value, gradient]}\n",
    "\n",
    "iteration_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07054462-fd5b-48c2-b523-34ec4a20c23e",
   "metadata": {},
   "source": [
    "- Perform <font color='dodgerblue'>**gradient optimization**</font> until the **convergence criteria** is **acheived**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef56917",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "\n",
    "while gradient > convergence_criteria:\n",
    "    iteration += 1\n",
    "\n",
    "    new_x, gradient = gradient_descent(function=function_x_sqrd, x_n=x_guess, learning_rate=learning_rate)\n",
    "    \n",
    "    new_y, new_grad = function_x_sqrd(new_x)\n",
    "\n",
    "    iteration_dict[iteration] = [new_x, new_y, new_grad]\n",
    "\n",
    "    x_guess = new_x # updated guess\n",
    "\n",
    "iteration_pd = pd.DataFrame.from_dict(iteration_dict, orient='index', columns=['x', 'y(x)', 'gradient'])\n",
    "iteration_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e58eac8-48ea-43e9-b545-1dd9edffb486",
   "metadata": {},
   "source": [
    "- Visualize the optimization iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe6e8f-ee18-4bbf-aecf-f10bfa80df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iteration_pd['x'], iteration_pd['y(x)'])\n",
    "plt.scatter(iteration_pd['x'], iteration_pd['y(x)'], color='red', label='iteration')\n",
    "\n",
    "for index, row in iteration_pd.iterrows():\n",
    "    plt.text(row['x'] + 0.0, row['y(x)'] + 0.05, str(index),\n",
    "             fontsize=10, color='DodgerBlue', ha='right', va='bottom')    \n",
    "\n",
    "plt.xlabel(xlabel='x')\n",
    "plt.ylabel(ylabel='y(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f120f-18dd-4c69-bf6e-f1ce67dc4bee",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "## Fully-Connected Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219cfac7-3b0f-4b6d-a577-2fad509831d0",
   "metadata": {},
   "source": [
    "#### <font color='dodgerblue'>Activation Function</font>\n",
    "\n",
    "**Purpose**\n",
    "- introduce **non-linearity** into the system\n",
    "\n",
    "Without activation functions, the **nodes are linearly connected** (i.e., <font color='dodgerblue'>see equation below$</font>) due to the **matrix multiplication**.\n",
    "\n",
    "<font color='dodgerblue'>$y(x) = w\\cdot x + b$</font>\n",
    "- where `w` is a weight\n",
    "- `b` is the bias\n",
    "\n",
    "If all nodes are computed using the **summation of linear functions**, then the end **prediction** will be **linearly connected to the input**.\n",
    "\n",
    " \n",
    "**Source**:\n",
    "- https://towardsdatascience.com/the-importance-and-reasoning-behind-activation-functions-4dc00e74db41\n",
    "\n",
    "<br>\n",
    "\n",
    "Define the <font color='dodgerblue'>**activation function**</font>: a **rectified linear unit** (<font color='dodgerblue'>ReLU</font>)\n",
    "- https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "\n",
    "\\begin{equation}\n",
    "    ReLU(x) = max(0, x) = \\frac{x + |x|}{2} =\n",
    "    \\begin{cases}\n",
    "        x,& \\text{if } x>1\\\\\n",
    "        0,              & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Visualize**: What does the ReLU function look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ef803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.array, bias: float=0.0) -> np.array:\n",
    "    ''' A rectified linear unit function.\n",
    "\n",
    "        Args:\n",
    "            x:    The input array,\n",
    "            bias: A scalar bias value to be added to each element of `x`\n",
    "                       (ie., shifts the plot on x axis)\n",
    "\n",
    "        Returns:\n",
    "            Values of `x` where all negative values (after adding bias)\n",
    "                have been replaced by 0, while positive values remain unchanged.\n",
    "    '''\n",
    "    return np.maximum(0, x + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39c1fc-ff22-4ab5-b6b7-59abc0a5616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relu(bias:float):\n",
    "    '''Plot the ReLU activation function.'''\n",
    "    x = np.linspace(-10, 20, 100)\n",
    "\n",
    "    relu_x = relu(x=x, bias=bias)  \n",
    "\n",
    "    plt.plot(x, relu_x)\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Output')\n",
    "\n",
    "plot_relu(bias=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554899d-704d-483b-abf6-3f1990bb6805",
   "metadata": {},
   "source": [
    "#### <font color='dodgerblue'>Loss Function</font>\n",
    "\n",
    "**Definition of Loss Function**: a function that <font color='dodgerblue'>**quantifies the error**</font> between the **predicted** value and the **actual** value.\n",
    "\n",
    "Define the loss function. This is needed for \n",
    "\n",
    "1. evaluate the predictions, and\n",
    "2. to take its **gradient**, then we can <font color='dodgerblue'>**optimize**</font> the <font color='dodgerblue'>**weights**</font> and <font color='dodgerblue'>**biases**</font>.\n",
    "\n",
    "A common **loss function** is the **mean squared error**, which is also known as **L2 loss**.\n",
    "\n",
    "\\begin{equation}\n",
    "    MSE = \\frac{1}{n} \\sum_{i=1}^n (y_{\\text{prediction}_i} - y_{\\text{target}_i})^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49139a-8473-4345-b051-3c94a0c87a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(predicted: np.array, target: np.array) -> np.array:\n",
    "    ''' Mean squared error loss function.\n",
    "\n",
    "        Provides a single metric for the loss values.\n",
    "    '''\n",
    "    return np.mean(np.square(predicted - target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b098c5-6e72-4822-bf80-51822e2b7ed4",
   "metadata": {},
   "source": [
    "Consequently, the **gradient** of the **loss function** would be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc24c2-dc3b-43d9-bd0d-057456c6a972",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\nabla_y = \\frac{\\partial}{\\partial y} = \\frac{2}{n}*(y_{\\text{prediction}_i} - y_{\\text{target}_i})\n",
    "\\end{equation}\n",
    "\n",
    "**Note**: We will drop the $n$, which basically makes the learning rate bigger, and the optimization faster (also simplifies the function/equation some)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c6f5d7-4e88-4a7c-9d4b-1f2c56fdf5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_gradient(predicted: np.array, target: np.array) -> np.array:\n",
    "    ''' Gradient of the mean squared error loss function.\n",
    "            i.e., 2.0 * (output_Y2 - target_Y2)\n",
    "    '''\n",
    "    gradient = np.multiply(2, np.subtract(predicted, target))\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30177e79-7ac5-4879-bf5a-33d9ba54366e",
   "metadata": {},
   "source": [
    "#### <font color='dodgerblue'>Toy Data</font>\n",
    "For <u>teaching purposes</u>, a random **seed** will be **explicitly set**, allowing for **reproducible results**.\n",
    "<!-- The first **epoch** data generated below should correspond to the numeric values given in the figure above. -->\n",
    "\n",
    "The object naming will also be done to parallel the figure above. `rng` (random number generator) follows the example given by NumPy.\n",
    "\n",
    "<font color='dodgerblue'>Random Number Generator in NumPy</font>:\n",
    "- `np.random.default_rng`: https://numpy.org/doc/stable/reference/random/generator.html\n",
    "- via a normal (Gaussian) distribution using `numpy.random.Generator.normal`: https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Define some toy data:\n",
    "- **input x** values\n",
    "- **target y** values (for computing the loss function)\n",
    "- two **initial weight** matrices\n",
    "- two **initial bias** matrices (set initially to zero)\n",
    "\n",
    "**Important Note**: Normally with <font color='dodgerblue'>real-world data</font>, one often <font color='dodgerblue'>normalizes</font> (e.g., data **transpose** to a range [0, 1]) the <font color='dodgerblue'>input data</font>. This helps the mathematics when different input features have **large magnitude differences** (e.g., feature 1: 1.5 and feature 2 : 2.5e6).\n",
    "- https://en.wikipedia.org/wiki/Normalization_(statistics)\n",
    "- `sklearn.preprocessing.normalize`: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html\n",
    "\n",
    "In this example, the generated data will have the same magnitude, so *no* normalization is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed4eb9-fd95-4450-9688-8601e36e61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=12345)\n",
    "\n",
    "input_X1 = rng.normal(size=(2, 10))\n",
    "target_Y2 = rng.normal(size=(2, 1))\n",
    "\n",
    "weight_W1 = rng.normal(size=(10, 3))\n",
    "weight_W2 = rng.normal(size=(3, 1))\n",
    "\n",
    "bias_B1 = np.full((2, 3), 0.0)  # shape of middle layer\n",
    "bias_B2 = np.full((2, 1), 0.0)  # shape of output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d4bcf-4470-4fa1-a397-aa1d27f60b3a",
   "metadata": {},
   "source": [
    "Let's look at the resulting arrays:\n",
    "- shape\n",
    "- values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282da9d0-b544-4ddb-8601-4d20ccd1cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_array_specs(in_arrays: dict):\n",
    "    ''' Helper function for nicely printing NumPy arrays' info.\n",
    "\n",
    "        Print: shape, data type and values\n",
    "    '''\n",
    "    for key, value in in_arrays.items():\n",
    "        print(f'{key}:\\n{value.shape}, {value.dtype}')\n",
    "        print(f'{value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb52ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_ini = {'input_X1': input_X1, 'target_Y2': target_Y2,\n",
    "               'weight_W1': weight_W1, 'weight_W2': weight_W2,\n",
    "               'bias_B1': bias_B1, 'bias_B2': bias_B2}\n",
    "\n",
    "print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca933a48-849f-491a-8a98-f4fa623d1057",
   "metadata": {},
   "source": [
    "### <font color='dodgerblue'>NN Steps in Detail</font>\n",
    "\n",
    "<center><img src=\"00_images/31_machine_learning/nn_perceptron_example.png\" alt=\"nn_percepton\" style=\"width: 1000px;\"/></center>\n",
    "\n",
    "Here is a detailed explanation of what will happen in the following code cell.\n",
    "\n",
    "Two major data propagation steps:\n",
    "1. Forward\n",
    "2. Backward\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Forward Propagation\n",
    "\n",
    "Perform the math, starting at the neural network's beginning and working progressively towards the end.\n",
    "\n",
    "1. <font color='dodgerblue'>$X2 = X1\\cdot W1 + B1$</font>\n",
    "    - In words: **transform X1 input** data into fewer dimensions\n",
    "    - `transformer function`: **linear combination** (a.k.a. weighted sum)\n",
    "    - matrix multiplication results in a **fully-connected** neural network\n",
    "    - `X1` and `X2` are **linearly connected**\n",
    "\n",
    "<br>\n",
    "\n",
    "2. <font color='dodgerblue'>$Y1 = \\text{ReLu}(X2)$</font>\n",
    "    - In words: activation that results in a <font color='dodgerblue'>non-linear output data</font> (a **node-wise operation**)\n",
    "   - `X1` and `Y1` are now **non-linearly connected**\n",
    "\n",
    "<br>\n",
    "\n",
    "3. <font color='dodgerblue'>$Y2 = Y1\\cdot W2 + B2$</font>\n",
    "    - In words: **transform Y1 output** data into fewer dimensions\n",
    "    - matrix multiplication results in a **fully-connected** neural network\n",
    "\n",
    "<br>\n",
    "\n",
    "4. <font color='dodgerblue'>$\\text{L (i.e., Loss)} = (Y2- \\text{y target})^2$</font>\n",
    "    - In words: **MSE loss** is computed\n",
    "\n",
    "<br>\n",
    "\n",
    "5. <font color='dodgerblue'>$\\text{grad\\_loss} = \\frac{\\partial L}{\\partial x}$</font>\n",
    "    - In words: <font color='dodgerblue'>gradient</font> of the **loss** (see `mse_loss_gradient` above)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. Backward Propagation\n",
    "\n",
    "Now perform the math, starting at the neural network's end and working progressively towards the beginning.\n",
    "\n",
    "##### Update W2 Weights\n",
    "\n",
    "6. <font color='dodgerblue'>$\\text{grad\\_W2} = Y1^T \\cdot \\text{grad\\_loss}$</font>\n",
    "\n",
    "    or rephrased\n",
    "   \n",
    "   <font color='dodgerblue'>$\\text{grad\\_W2} = Y1^T \\cdot \\frac{\\partial L}{\\partial x}$</font>\n",
    "    - In words: <font color='dodgerblue'>gradient</font> of the <font color='dodgerblue'>weights</font> **connecting** the **hidden layer** (i.e., after applying the ReLU activation) to the output layer\n",
    "    - the **gradient of the loss** concerning the **weights W2**\n",
    "    - each **element** in `grad_w2` reflects **how much** a **specific weight** should be **adjusted** based on the **hidden layer's output** and the corresponding **output error**\n",
    " \n",
    "<br>\n",
    "\n",
    "7. <font color='dodgerblue'>$W2_{new} = W2_{old} - \\text{learning rate} * \\text{grad\\_W2}$</font>\n",
    "    - In words: **gradient descent** that **updates** the **weights W2**\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Update W1 Weights\n",
    "\n",
    "8. <font color='dodgerblue'>$\\text{grad\\_Y1} = \\frac{\\partial L}{\\partial x} \\cdot W2^T$</font>\n",
    "    - In words: **gradients** concerning the **hidden layer output** (after applying the ReLU activation)\n",
    "    -  the **gradient of the loss** concerning the **hidden layer's output**\n",
    "    -  quantifies how **changes** in the **hidden layer's output** (quantified by the weight values) affect the **overall loss**\n",
    "\n",
    "<br>\n",
    "\n",
    "9. <font color='dodgerblue'>rev_X2[X2 < 0] = 0</font>\n",
    "    - In words: **Zero out the negative gradients**, which ensures that any **hidden unit** that was **inactive** (i.e., had a **negative** input to **ReLU**) **does not contribute** to the **gradient** (i.e., its value is set to zero).\n",
    "    - consequently, the inactive units <font color='dodgerblue'>do not influence</font> the curent caluculation\n",
    "\n",
    "<br>\n",
    "\n",
    "10. <font color='dodgerblue'>$\\text{grad\\_W1} = X1^T \\cdot \\text{rev\\_X2}$</font>\n",
    "    - In words: **gradients** concerning the **weights W1** that connect the **input** layer to the **hidden** layer.\n",
    "    - represents **how the loss changes** with respect to the **weights** connecting the **input layer** to the **hidden layer**\n",
    " \n",
    "<br>\n",
    "\n",
    "11. <font color='dodgerblue'>$W1_{new} = W1_{old} - \\text{learning rate} * \\text{grad\\_W1}$</font>\n",
    "    - In words: **gradient descent** that **updates** the **weights W1**\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Update B2 Bias\n",
    "12. <font color='dodgerblue'>grad_B2 = grad_loss.sum(axis=0, keepdims=True)</font>\n",
    "    - In words: the **gradients** concerning `bias_B2` is calculated as the **sum of `grad_loss`** along each node (the rows).\n",
    "    - Relatively straightforward since they mainly serve as simple offsets \n",
    "\n",
    "<br>\n",
    "\n",
    "13. <font color='dodgerblue'>$B2_{new} = B2_{old} - \\text{learning rate} * \\text{grad\\_B2}$</font>\n",
    "    - In words: **gradient descent** that **updates** the **bias B2**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Update B1 Bias\n",
    "14. <font color='dodgerblue'>grad_B1 = rev_X2.sum(axis=0, keepdims=True)</font>\n",
    "    - In words: the **gradients** concerning `bias_B1` is calculated as the **sum of `grad_loss`** along each node (the rows), **after reversing** the ReLU **activation**.\n",
    "    - Relatively straightforward since they mainly serve as simple offsets \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "15. <font color='dodgerblue'>$B1_{new} = B1_{old} - \\text{learning rate} * \\text{grad\\_B1}$</font>\n",
    "    - In words: **gradient descent** that **updates** the **bias B1**\n",
    "\n",
    "<br>\n",
    "\n",
    "####  Neural Network Training\n",
    "**Repeat** the **forward** and **backward propagation** for many **iterations** (a.k.a. <font color='dodgerblue'>epochs</font>)\n",
    "- done for a finite number of times\n",
    "- done until specified loss convergence is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c517334-b742-4b63-bc44-acfa7b695452",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "#### New Term:\n",
    "**An Epoch**: one complete pass of the entire neural network (forward and backward propagation).\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "#### <font color='dodgerblue'>Final objects that are needed</font>\n",
    "- a **learning rate** (e.g., 1.0e-3)\n",
    "- a **convergence criteria** (e.g., 1.5)\n",
    "\n",
    "- **results container**: empty dictionary to store the iterations\n",
    "- **initial large loss value** (to start the `while` loop)\n",
    "- an iteration **counter**\n",
    "\n",
    "\n",
    "Note: I will heavily comment on the code below for teaching purposes since it is the first time encountering forward and backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e48302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "loss = 2\n",
    "convergence_criteria = 1.5\n",
    "iteration = 0\n",
    "learning_rate = 1.0e-3\n",
    "\n",
    "while loss > convergence_criteria:\n",
    "    if iteration > 50: # set maximum iterations to prevent possible infinite runs\n",
    "        break\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "    ## forward propagation\n",
    "    # transform\n",
    "    X2 = input_X1.dot(weight_W1) + bias_B1\n",
    "\n",
    "    # activate\n",
    "    Y1 = relu(x=X2)\n",
    "\n",
    "    # transform\n",
    "    output_Y2 = Y1.dot(weight_W2) + bias_B2\n",
    "\n",
    "    loss = mse_loss(predicted=output_Y2, target=target_Y2)\n",
    "\n",
    "    ## backward propagation\n",
    "    ## gradient of loss function\n",
    "    grad_loss = mse_loss_gradient(predicted=output_Y2, target=target_Y2)\n",
    "\n",
    "    ### update weight_W2\n",
    "    grad_W2 = Y1.T.dot(grad_loss)\n",
    "\n",
    "    ## gradient decent: w2_1 = w2_0 - learning_rate.(Y1.T).(dL/dx)\n",
    "    # weight_W2 = weight_W2 - learning_rate*grad_Y1\n",
    "    weight_W2 = np.subtract(weight_W2, learning_rate * grad_W2)\n",
    "\n",
    "    ## update bias_B2\n",
    "    grad_B2 = grad_loss.sum(axis=0, keepdims=True)\n",
    "    bias_B2 = np.subtract(bias_B2, learning_rate * grad_B2)\n",
    "\n",
    "    ### update weight_W1\n",
    "    grad_Y1 = grad_loss.dot(weight_W2.T)\n",
    "\n",
    "    ## reversing ReLu\n",
    "    rev_X2 = grad_Y1.copy()\n",
    "    rev_X2[X2 < 0] = 0\n",
    "\n",
    "    grad_W1 = input_X1.T.dot(rev_X2)\n",
    "\n",
    "    ## gradient decent: w1_1 = w1_0 - learning_rate.(X1.T).(dL/dx.grad_Y1)\n",
    "    # weight_W1 = weight_W1 - learning_rate*grad_W1\n",
    "    weight_W1 = np.subtract(weight_W1, learning_rate * grad_W1)\n",
    "\n",
    "    ### update bias_B1\n",
    "    grad_B1 = rev_X2.sum(axis=0, keepdims=True)  # the propagated gradient of the loss\n",
    "    bias_B1 = np.subtract(bias_B1, learning_rate * grad_B1)\n",
    "\n",
    "    results[f'{iteration}'] = [output_Y2, loss, weight_W1, weight_W2, bias_B1, bias_B2]\n",
    "\n",
    "    print(f'Iteration {iteration}: Loss = {loss.item():.3f}')\n",
    "\n",
    "    ## Uncomment the following to see iteration details\n",
    "    # objects_ini = {'weight_W1': weight_W1, 'bias_B1': bias_B1,\n",
    "    #                'X2': X2, 'Y1': Y1,\n",
    "    #                'weight_W2': weight_W2, 'bias_B2': bias_B2, \n",
    "    #                'output_Y2': output_Y2,\n",
    "    #                'loss': loss, 'grad_loss': grad_loss,\n",
    "    #                'grad_W2': grad_W2, 'grad_B2': grad_B2, 'grad_Y1': grad_Y1,\n",
    "    #                'rev_X2': rev_X2,\n",
    "    #                'grad_W1': grad_W1, 'weight_W1': weight_W1, 'grad_B1': grad_B1}\n",
    "\n",
    "    # print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8473cd-b4f2-4042-a17d-85f80f75db8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2a757b-1b5c-480d-a9d8-e1ec10784e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_pd = pd.DataFrame.from_dict(results, orient='index', columns=['y_predicted', 'loss',\n",
    "                                                                        'weight_W1', 'weight_W2',\n",
    "                                                                        'bias_b1', 'bias_b2'])\n",
    "iteration_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bda840-d706-49e7-be9d-536c69dfe6de",
   "metadata": {},
   "source": [
    "Let's **visualize** the results for **weights_w2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40962283-429c-4977-bd09-ebd88aa7986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w2 = pd.DataFrame()\n",
    "\n",
    "for iteration, values in results.items():\n",
    "    weights_w2_list = [x.tolist() for x in results[iteration][3]]\n",
    "    weights_w2_list = [item for sublist in weights_w2_list for item in sublist]\n",
    "    df_w2 = pd.concat([df_w2, pd.DataFrame([weights_w2_list])], ignore_index=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for iteration in range(0, 3, 1):\n",
    "    ax.plot(df_w2.index, df_w2[iteration], label=f'weight: {iteration}')\n",
    "    ax.scatter(df_w2.index, df_w2[iteration]) \n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff48fc2-547b-4a9c-86b4-2f7cc2013425",
   "metadata": {},
   "source": [
    "Visualize the loss value and the **predicted y value** of the **second observable**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02adfc1-ae9c-46e5-8dcb-489242172fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "observable_2 = []\n",
    "\n",
    "for values in iteration_pd['y_predicted'].values:\n",
    "    observable_2.append(values[1])\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(iteration_pd.index.values, iteration_pd['loss'], label='loss value')\n",
    "ax2.scatter(iteration_pd.index.values, observable_2, color='red', label='current value') \n",
    "ax2.hlines(target_Y2[1],\n",
    "           min(iteration_pd.index.astype(int)-1),\n",
    "           max(iteration_pd.index.astype(int)-1), colors='red',\n",
    "           linestyles='dashed', label='target value')\n",
    "\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.tick_params(\"x\", labelrotation=90)\n",
    "\n",
    "ax1.set_ylabel('Loss', color='DodgerBlue')\n",
    "ax2.set_ylabel('Observable 2', color='red')\n",
    "\n",
    "ax1.legend(loc='upper center')\n",
    "ax2.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61810754-7996-4cdf-8657-025dd6ee3ee9",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "### NumPy Neural Network Summary:\n",
    "1. created a simple **perceptron** neural network using **NumPy only**\n",
    "2. toy data creation\n",
    "3. **weights**: adjust the **relationship between nodes**\n",
    "4. **biases**: adjust the **nodes themselves** (for their activation)\n",
    "5. **activation functions**\n",
    "    - adds some **nonlinearity** to the mathematics\n",
    "    - ReLU\n",
    "6. **gradient optimization**\n",
    "7. **loss function**\n",
    "8. detailed **explanation** of each **mathematical** step in a neural network\n",
    "9. **forward propagation**\n",
    "10. **backward propagation**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
