{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b906ebb7-9ff7-427c-bdd9-7a072c7de9db",
   "metadata": {},
   "source": [
    "<div class=\"alert block alert-info alert\">\n",
    "\n",
    "# <center>PyTorch</center>\n",
    "\n",
    "A popular machine learning library.\n",
    "\n",
    "- Major focus: a class called Tensor (torch.Tensor)\n",
    "    - stores and operates on homogeneous multidimensional rectangular arrays of numbers\n",
    "    - similar to NumPy Arrays, but can also be operated on via a CUDA-capable NVIDIA GPU\n",
    "\n",
    "**Sources**:\n",
    "- https://pytorch.org\n",
    "- https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html\n",
    "\n",
    "#### Citation:\n",
    "\n",
    "Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen et al. \"Pytorch: An imperative style, high-performance deep learning library.\" Advances in neural information processing systems 32 (2019).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "@inproceedings{pytorch,\n",
    "\n",
    " author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},\n",
    " \n",
    " booktitle = {Advances in Neural Information Processing Systems},\n",
    " \n",
    " editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\\textquotesingle Alch\\'{e}-Buc and E. Fox and R. Garnett},\n",
    " pages = {},\n",
    " \n",
    " publisher = {Curran Associates, Inc.},\n",
    " \n",
    " title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},\n",
    " \n",
    " url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},\n",
    " \n",
    " volume = {32},\n",
    " \n",
    " year = {2019}\n",
    " \n",
    "}\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ba193-953e-4536-b772-951efc05b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b56eb3-673e-49c3-a0a5-988ddf61475f",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "## Initializing a PyTorch Tensor - different approaches\n",
    "#### 1. From a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f4535-9424-4fcd-b1cc-74ce9d07d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array_np = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "display(data_array_np)\n",
    "\n",
    "data_array_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee4f5e-e2fe-4072-869d-ac222e44a84c",
   "metadata": {},
   "source": [
    "PyTorch tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb1bb7-b75e-4bcc-b9c9-6bdebfb06017",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_tensor_torch = torch.from_numpy(data_array_np)\n",
    "\n",
    "generic_tensor_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d7ed7a-388a-4b7d-a773-21b896ae688c",
   "metadata": {},
   "source": [
    "#### 2. Based on an existing tensor\n",
    "- retains shape (e.g., the `generic_tensor_torch` given above)\n",
    "- replace existing values with others, for example\n",
    "    - 1's via `ones_like`\n",
    "    - random values via `rand_like`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa1ca0-d394-47a1-bb3b-7dce8e9c9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones_like(generic_tensor_torch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f24b792-fc6b-4b3b-b204-e82f4b5004c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand_like(generic_tensor_torch, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d5808-29de-4b63-ac1e-075a713644f1",
   "metadata": {},
   "source": [
    "#### 3. Fill in a specific shape\n",
    "    - Note: you can have multiple dimensions (e.g., (2, 3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122327a7-3620-4b2d-bfe7-94083b9f94aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_shape = (2, 3)\n",
    "\n",
    "rand_tensor = torch.rand(my_shape)\n",
    "\n",
    "rand_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a038fbf-5119-4f31-a02d-18b7f76e217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_shape = (2, 3, 5)\n",
    "\n",
    "three_dim_tensor = torch.rand(my_shape)\n",
    "\n",
    "three_dim_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d08919-cb78-40c4-b8fe-a66998a364b4",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"></hr>\n",
    "\n",
    "## Book keeping\n",
    "- `tensor.shape`\n",
    "- `tensor.dtype`\n",
    "- `tensor.device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc7c9a-53e9-4cb3-8f7b-8fab42924958",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of tensor: {three_dim_tensor.shape}')\n",
    "print(f'Datatype of tensor: {three_dim_tensor.dtype}')\n",
    "print(f'Device tensor is stored on: {three_dim_tensor.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770596c1-9298-4fc4-96d4-4a5caa081956",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"></hr>\n",
    "\n",
    "## Tensor operations\n",
    "- https://pytorch.org/docs/stable/torch.html\n",
    "\n",
    "#### Joining tensors `torch.cat`\n",
    "- https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat\n",
    "\n",
    "Add together three times\n",
    "- `dim=0`: conceptually like adding more rows (same as NumPy)\n",
    "- `dim=1`: conceptually like adding more columns (same as NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a474c-3b08-4c51-a56c-d2e49a6bff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([rand_tensor, rand_tensor, rand_tensor], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6e0bb-8a26-4dfd-8ed9-47c4d5003cef",
   "metadata": {},
   "source": [
    "View as a Pandas' `DataFrame`:\n",
    "- same idea as we did with NumPy before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218425d7-b080-4327-8c7c-1bb69e91febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(torch.cat([rand_tensor, rand_tensor, rand_tensor], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a08d4a-311e-41b2-b93b-f9ca02480a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([rand_tensor, rand_tensor, rand_tensor], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b01d14e-7e5c-4402-afca-be08791285c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(torch.cat([rand_tensor, rand_tensor, rand_tensor], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68d47d-8fde-418a-ada8-90a22f917dfe",
   "metadata": {},
   "source": [
    "#### Pairwise multiplication\n",
    "- `mul`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fe014f-9a6a-4f48-8dd0-ef9011dd778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a2f1e0-744b-4c41-97a0-b92737d1bcfa",
   "metadata": {},
   "source": [
    "Multiple `rand_tensor` * `rand_tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b32ed-718b-4c6e-b693-0dc4f4cdce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_tensor.mul(rand_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32744c-ca04-4ed9-abc6-2c499242fbc4",
   "metadata": {},
   "source": [
    "Accessing specific values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b220c621-4675-42a9-9d67-448551651f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_tensor[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63f46c2-a8a2-4b0e-86df-c3324b137dd5",
   "metadata": {},
   "source": [
    "Multiply individual elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc3df1-c658-4a7e-b88d-e9a437f0d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_tensor[0][0]*rand_tensor[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d860c-4e35-4d4f-823d-52bb4895d1c7",
   "metadata": {},
   "source": [
    "## PyTorch and NumPy Interactions\n",
    "\n",
    "Torch to Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc4532-d6ed-4a71-aff6-29fa3e728e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_array = rand_tensor.numpy()\n",
    "rand_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627146f0-e19e-44fc-8bcc-771322470ea4",
   "metadata": {},
   "source": [
    "## Using GPU\n",
    "\n",
    "To use a GPU device instead of a CPU device, one needs to <font color='dodgerblue'>move a **specified** PyTorch tensor from its current device (i.e., CPU) to the GPU</font>.\n",
    " \n",
    "(If youâ€™re using Colab, allocate a GPU by going to Edit > Notebook Settings.)\n",
    "\n",
    "Let's move our `rand_tensor` to GPU if possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121038eb-e028-48f4-bba4-521ee5a06ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDA is available - congrats!')\n",
    "    tensor = rand_tensor.to('cuda')\n",
    "    display(tensor.device)\n",
    "\n",
    "    print('\\nExample GPU calculation - adding the tensor to itself:')\n",
    "    result_tensor = tensor + tensor\n",
    "    display(result_tensor)\n",
    "else:\n",
    "    print('CUDA is not available - bummer.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230fba6a-6caa-4263-b60a-bf33a15672b7",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "1. `Torch` has a lot of similarity to `NumPy`\n",
    "2. However, it can be used on a **GPU**\n",
    "3. Some special \"things\" that make it unique\n",
    "    - <font color='dodgerblue'>history accumulation</font> (coming below)\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747e2d54-163d-4b6d-8468-1b1aee60421e",
   "metadata": {},
   "source": [
    "<div class=\"alert block alert-info alert\">\n",
    "\n",
    "# <center>Neural Networks (NNs)</center>\n",
    "\n",
    "## Background:\n",
    "- https://en.wikipedia.org/wiki/Neural_network_(machine_learning)\n",
    "\n",
    "<font color='dodgerblue'>\"A machine learning model is a **function**, with **inputs** and **outputs**.\" [1]</font>\n",
    "\n",
    "- a **collection of <font color='dodgerblue'>nested functions</font>**\n",
    "    - these functions are **executed on input data**\n",
    "    - these functions are defined by parameters (stored in PyTorch tensors)\n",
    "        - **weights**, and \n",
    "        - **biases**\n",
    "\n",
    "<br>\n",
    "\n",
    "What is the difference between **weights** and **bias** values?\n",
    "\n",
    "If we have a **linear <font color='dodgerblue'>\"activation function\"</font>**: $y = mx + b$\n",
    "- **weight** (i.e., the weight for x): <font color='dodgerblue'>$m$</font>\n",
    "    - for polynomial functions (e.g. linear equation), the weights are the coefficients (see \"Extra Information\" for nonlinear example)\n",
    "<br>\n",
    "- **bias** (i.e., the equation's bias): <font color='dodgerblue'>$b$</font>\n",
    "    - offset - a constant term (consider the phase shifts we covered in SciPy lecture)\n",
    "\n",
    "**What is an <font color='dodgerblue'>activation function</font>**\n",
    "- A function (surprise ðŸ™‚)\n",
    "- Calculates a **node's output** (i.e., information) that is **passed** to the **next node**, using *its* specific input parameters (i.e., weights and bias)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Additional Resources\n",
    "\n",
    "##### Great Tutorial Series: **3Blue1Brown**\n",
    "- https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&feature=shared\n",
    "    - DeepLearning Chapter 1 (DL1): \"What is a NN\"\n",
    "    - DL2: \"Gradient descent, how neural networks learn\"\n",
    "    - DL3: \"Backpropagation, step-by-step\" (meta-level)\n",
    "    - DL4: \"Backpropagation calculus\" (the mathematics/formulas)\n",
    "\n",
    "##### TensorFlow Playground\n",
    "- (For later)\n",
    "- https://playground.tensorflow.org\n",
    "\n",
    "**Sources**:\n",
    "1. https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0179057-8326-4f7d-aba8-180bedc8f4fe",
   "metadata": {},
   "source": [
    "<p><img alt=\"neural network\" width=\"800\" src=\"00_images/31_machine_learning/deep_neural_network.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
    "\n",
    "Image Source: https://www.studytonight.com/post/understanding-deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7d885-818a-4209-8ebc-f4f7de5be361",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<hr style=\"border:1.5px dashed gray\"></hr>\n",
    "\n",
    "## Extra Information\n",
    "\n",
    "### Sigmoid Activation Functions\n",
    "- What might a weighting factor (i.e., $w$) look like\n",
    "\n",
    "Sigmoid Equation:\n",
    "\\begin{equation}\n",
    "\\huge y(x) = \\frac{1}{1 + \\exp^{w*x}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea739d-4ebd-4702-814f-df3837582477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sigmoid(x_data: np.array, weight: float):\n",
    "    ''' Simple Sigmoid function with a weight.\n",
    "\n",
    "        Args:\n",
    "            x_data: independent data\n",
    "            weight: weighting factor that adjusts the equation's shape\n",
    "\n",
    "        Return\n",
    "            results: calculated results\n",
    "    '''\n",
    "\n",
    "    result = 1/(1 + np.exp(weight*-x_data))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411333d6-e011-4ce7-93a9-b7b663fb18e5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Plot a sigmoid equation using different weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e94bc89-f6fe-41e4-982e-b1989ff4ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_list = [-1, 1, 5]\n",
    "\n",
    "x_values = np.linspace(-5, 5)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for weight in weight_list:\n",
    "    y_values = weighted_sigmoid(x_data=x_values, weight=weight)\n",
    "\n",
    "    ax.plot(x_values, y_values, label=f'{weight}')\n",
    "\n",
    "plt.legend(loc='right', title='weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac0cab-e149-465c-b50f-e41decbb3789",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "So, we see that the **weighting factor** can **adjust** the **function's shape**, and consequently **adjust** the **information** being **passed** from one node to another.\n",
    "    \n",
    "<hr style=\"border:1.5px dashed gray\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49615985-3aa6-49c4-a663-ad68b0ee12f9",
   "metadata": {},
   "source": [
    "## NN Training\n",
    "\n",
    "Training is done in 2 Steps\n",
    "\n",
    "1. **Forward Propagation**\n",
    "    - <font color='dodgerblue'>pass data</font> (observables, weights and biases) <font color='dodgerblue'>forward</font> to **predict target observable**\n",
    "        - compute the **activation function** that connects each node\n",
    "    - (Basically, running the model as it will be eventually used.)\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Backward Propagation**\n",
    "    - adjusts parameters proportionally to the observable error (i.e., forward propagation's output/result vs. the target)\n",
    "    - done by\n",
    "        - traversing backwards from the output\n",
    "        - collect the **derivatives** (i.e., gradients) of the error with respect to the functions' parameters\n",
    "        - **optimizes** the parameters using <font color='dodgerblue'>**gradient descent**</font>\n",
    "\n",
    "**`torch.autograd`** is the workhorse for <font color='dodgerblue'>backward propagation</font>.\n",
    "\n",
    "**Source**: https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c10707f-818a-4e9a-ab01-6b2ebbc03f5c",
   "metadata": {},
   "source": [
    "### Tensor History\n",
    "\n",
    "A key concept in PyTorch is that it's <font color='dodgerblue'>tensor objects</font> can have a <font color='dodgerblue'>\"history\"</font> (i.e., **new data** that is **attached** to the object **after** it is used in a **calculation**).\n",
    "\n",
    "For example:\n",
    "- `torch.rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None,` **`requires_grad=False,`**` pin_memory=False) â†’ Tensor`\n",
    "\n",
    "- **`requires_grad=True`**: for every computation that follows, **`autograd`** will <font color='dodgerblue'>record the computation's history</font> in the output tensors\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Example: A <font color='dodgerblue'>Single Layer Perceptron</font> (SLP)\n",
    "- the simplest type of artificial NN\n",
    "    -  classify binary categories\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Perceptron\n",
    "\n",
    "<center>\n",
    "<img alt=\"neural network\" width=\"500\" src=\"00_images/31_machine_learning/perceptron_english.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></center>\n",
    "\n",
    "<center>Image Source: https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png</center>\n",
    "\n",
    "\n",
    "<br>\n",
    "Example Workflow:\n",
    "\n",
    "1. Create input data (e.g., independent x data)\n",
    "    - `requires_grad=True`\n",
    "2. Create a simple model based on an equation: $y = sin(x)$\n",
    "    - the activation function \n",
    "4. Forward Propagation\n",
    "5. Backwards Propagation (i.e., first derivatives)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. Create input data\n",
    "   - Everything is weighted equally (i.e., no weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be95448f-8131-465b-b9a7-ba82d0b1c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.linspace(0.0, 2.0*torch.pi, steps=25, requires_grad=True)\n",
    "\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fddefb7-c66f-4d63-9c86-8e2259c96f3d",
   "metadata": {},
   "source": [
    "Notice the `requires_grad=True` in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bbb2fd-54df-4b52-a90d-d09aa041b058",
   "metadata": {},
   "source": [
    "#### 2. Create a simple model based on an equation: $y = sin(x)$ and\n",
    "#### 3. Forward Propagation\n",
    "\n",
    "<font color='dodgerblue'>Activation Function: $y = a*sin(b*x)$</font>\n",
    "\n",
    "This is a <font color='dodgerblue'>\"**Single Layer Perceptron**\" NN:\n",
    "- **1 input layer** (n features $\\rightarrow$ n nodes),\n",
    "- **1 transfer function** (basically an activation function that gets the data ready)\n",
    "- **1 node (in 1 layer)** $\\rightarrow$ output\n",
    "- **activation function** (i.e., $a*sin(b*x)$)\n",
    "    - a = 1\n",
    "    - b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab273b-d3dd-4305-8892-df9df830bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine_activation_func(x: float, a: float, b: float) -> float:\n",
    "    ''' Sine function for use in PyTorch neural networks.\n",
    "    ''' \n",
    "    return a * torch.sin(b * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede742f1-b29f-4808-9490-7ea1c849a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_model = sine_activation_func(x=input_data, a=1, b=1)\n",
    "\n",
    "sin_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4194aec-97c2-4654-85c2-bb8ba9b86a71",
   "metadata": {},
   "source": [
    "Two things to note:\n",
    "- The `grad_fn=<SinBackward0>` tells us that this object <font color='dodgerblue'>**is accumulating history**</font>.\n",
    "- That is the **first** output of our <font color='dodgerblue'>**forward propogation**</font>.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Okay, let's compare this to what would have happened if `requires_grad=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846cb62d-1e60-4c08-a4d9-fe0775994a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_false = torch.linspace(0.0, 2.0*torch.pi, steps=25, requires_grad=False)\n",
    "\n",
    "print(input_data_false)\n",
    "\n",
    "torch.sin(input_data_false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df65483-4af6-4988-9d99-f9c79a5d28bc",
   "metadata": {},
   "source": [
    "Notice this changes the object - **no** `grad_fn=<SinBackward0>` - thus, **no history accumulation**.\n",
    "<br><br>\n",
    "\n",
    "#### History Accumulation Demonstration\n",
    "\n",
    "Okay, now back to our `sin_model`.\n",
    "\n",
    "Since <font color='dodgerblue'>history is accumulated</font> in `sin_model`, we can now use `detach()` to <font color='dodgerblue'>grab</font> only the tensor values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc7f1c7-bd85-4d43-98a9-ceff972607ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_model.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693bd170-c49c-478b-a005-b75537239753",
   "metadata": {},
   "source": [
    "**Visualize** the independent (`input_data`) and dependent (sin_model) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d1ece-2a53-4a66-a5b3-9cd1d262151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(input_data.detach(), sin_model.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e767cd7-b29f-47d9-a89e-b1d859dbbb2f",
   "metadata": {},
   "source": [
    "##### <font color='dodgerblue'>Adding to the History</font>\n",
    "\n",
    "Let's include **an additional operation** on the `sin_model` by summing values:\n",
    "\n",
    "- `torch.sum()`: https://pytorch.org/docs/stable/generated/torch.sum.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8daff1-5830-4e16-8081-4a0c2c01370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_sum = sin_model.sum()\n",
    "print(sin_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b03d4ef-6f8a-4058-a4be-90b26736e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sin_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a1b2f-ee64-42d3-9f21-0963e7b78418",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sin_sum.grad_fn)\n",
    "print(sin_sum.grad_fn.next_functions)\n",
    "print(sin_sum.grad_fn.next_functions[0][0].next_functions)\n",
    "print(sin_sum.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(sin_sum.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions) \n",
    "print(sin_sum.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a518bd2-323f-4876-96aa-ced3525dbbee",
   "metadata": {},
   "source": [
    "We see that if we access the <font color='dodgerblue'>\"history\"</font> portion of the object via `grad_fn`, we get a short explanation of what it is\n",
    "- from **most recent** operation $\\rightarrow$ **oldest** operation\n",
    "    - `SumBackward0` $\\rightarrow$ `sin_model.sum()`\n",
    "    - `MulBackward0` $\\rightarrow$ `b*x`\n",
    "    - `SinBackward0` $\\rightarrow$ `torch.sin(input_data)`\n",
    "    - `MulBackward0` $\\rightarrow$ `a*sin`\n",
    "        - Note: the above three operations come from the line: `sin_model = sine_activation_func(x=input_data, a=1, b=1)`\n",
    "    - `AccumulateGrad` $\\rightarrow$ `torch.linspace(0.0, 2.0*np.pi, steps=25, requires_grad=True)`\n",
    "    - `()` $\\rightarrow$ starting point\n",
    "\n",
    "With that basic understanding in place, we can now continue to the <font color='dodgerblue'>backwards propagation</font> idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28aa9a0-e69a-4d73-b763-8a3b40624f36",
   "metadata": {},
   "source": [
    "### Taking the first derivative of sin(x)\n",
    "- a.k.a \"gradient\"\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{d}{dx} sin(x) = cos(x)\n",
    "\\end{equation}\n",
    "\n",
    "##### `Autograd`: Computing Gradients\n",
    "\n",
    "- `autograd` is used when calling `backward()` function (i.e., `autograd.backwards`)\n",
    "- https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed3a155-6b86-4236-89ff-24373072cd78",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<hr style=\"border:1.5px dashed gray\"></hr>\n",
    "\n",
    "## Extra Information\n",
    "\n",
    "### `autograd.grad`\n",
    "\n",
    "Both `autograd.grad` and `autograd.backwards` take derivatives. However, they differ concerning if they affect the accumulation history.\n",
    "\n",
    "`autograd.grad`: compute gradients for specific tensors without affecting others (e.g., **does not** accumulate history)\n",
    "\n",
    "Source: https://www.geeksforgeeks.org/understanding-pytorchs-autogradgrad-and-autogradbackward/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2801a8-e2a6-4f4c-bb91-71ea805c373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new object\n",
    "test_sin_model = torch.sin(input_data)\n",
    "test_sin_sum = test_sin_model.sum()\n",
    "test_grad = torch.autograd.grad(outputs=test_sin_sum, inputs=input_data)\n",
    "\n",
    "display(test_grad)\n",
    "display(test_grad[0].sum())\n",
    "# print(test_grad.grad_fn) ## demonstrates the lack of history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4f7bc6-f604-4677-a5ee-34959b31b6d1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<hr style=\"border:1.5px dashed gray\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a338827-1fb5-40fd-8cdb-76c34b87860f",
   "metadata": {},
   "source": [
    "#### 1. `backward()`: compute gradients\n",
    "\n",
    "- will be added to the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f560d2f3-8c39-4a61-a878-c0aaf4ac18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_sum.backward()\n",
    "sin_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fafdaf2-31d3-4cfd-b300-597fd3a72903",
   "metadata": {},
   "source": [
    "- `1.4504e-07`: **sum** of all **elements** in the `sin_model` **tensor**\n",
    "    - Recall that `sin_sum` was created using `sin_sum = sin_model.sum()`\n",
    "    - Thus, we have to go back to `sin_model`\n",
    "- `<SumBackward0>`: name of the PyTorch function (i.e., sum during backward propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83794cc4-3d39-4d36-b466-1dbf567859d6",
   "metadata": {},
   "source": [
    "Let's verify that `1.4504e-07` is indeed the summ of the tensor elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e0e96-74cd-4f4b-b1ac-3b27b065c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d262a2-aa5e-404a-8dcc-8f686f8d75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sin_model.sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee692bf6-0c8a-456a-9bd6-81b97c402065",
   "metadata": {},
   "source": [
    "So, everything looks okay.\n",
    "\n",
    "<br>\n",
    "\n",
    "Now what did `backward()` do with the history?\n",
    "\n",
    "`sin_sum.backward()` **<font color='dodgerblue'>created a `grad` property</font>** within the <font color='dodgerblue'>**original input data</font>** as part of its **accumulated history**.\n",
    "\n",
    "(A property of an object is coming from a Python class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0b1c05-4218-4bb0-8ffc-64e1617304ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Input values:'\n",
    "      f'\\n{input_data}')\n",
    "print()\n",
    "print(f'Gradient values:'\n",
    "      f'\\n{input_data.grad}')\n",
    "print()\n",
    "print(f'Detached from the input_data:'\n",
    "      f'\\n{input_data.grad.detach()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff5c500-8685-4275-9efd-f8116c1f5c97",
   "metadata": {},
   "source": [
    "##### Side note: Verification of the derivative of sin\n",
    "\n",
    "We can now prove to ourselves that what is happening by `backwards()` is what is expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6520496-690b-4652-af1c-0387d79815d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_derivative_sin = np.cos(input_data.detach().numpy())\n",
    "first_derivative_sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb7b0a-4a81-4ae4-8c24-9f67d39cb965",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(input_data.detach(), input_data.grad.detach(), linestyle='-', linewidth=10)\n",
    "plt.plot(input_data.detach(), first_derivative_sin, linestyle='--', linewidth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c804ed-21eb-4611-8bdd-a3e0396a4afa",
   "metadata": {},
   "source": [
    "What is left that is missing is the **weights** and **bias optimization** portion of the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd02c0c-d189-4058-995e-7fc62f48b45f",
   "metadata": {},
   "source": [
    "# Take-home\n",
    "\n",
    "1. PyTorch is similar to NumPy\n",
    "    - similar functions\n",
    "    - `torch` instead of `array`\n",
    "    - able to use GPUs (not just CPUs)\n",
    "2.  Basics of neural networks\n",
    "    - forward propagation\n",
    "    - backwards propagation\n",
    "3. History accumulation\n",
    "4. Example: A Single Layer Perceptron (SLP)\n",
    "\n",
    "The above forms the foundation for understading how PyTorch is \"typically\" used in a ML project -- a future lecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
