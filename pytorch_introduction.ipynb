{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b906ebb7-9ff7-427c-bdd9-7a072c7de9db",
   "metadata": {},
   "source": [
    "<div class=\"alert block alert-info alert\">\n",
    "\n",
    "# <center>PyTorch</center>\n",
    "\n",
    "A popular machine learning library.\n",
    "\n",
    "- Major focus: a class called Tensor (torch.Tensor)\n",
    "    - stores and operates on homogeneous multidimensional rectangular arrays of numbers\n",
    "    - similar to NumPy Arrays, but can also be operated on using GPUs\n",
    "\n",
    "**Sources**:\n",
    "- https://pytorch.org\n",
    "- https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html\n",
    "\n",
    "#### Citation:\n",
    "\n",
    "Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen et al. \"Pytorch: An imperative style, high-performance deep learning library.\" Advances in neural information processing systems 32 (2019).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "@inproceedings{pytorch,\n",
    "\n",
    " author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},\n",
    " \n",
    " booktitle = {Advances in Neural Information Processing Systems},\n",
    " \n",
    " editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\\textquotesingle Alch\\'{e}-Buc and E. Fox and R. Garnett},\n",
    " pages = {},\n",
    " \n",
    " publisher = {Curran Associates, Inc.},\n",
    " \n",
    " title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},\n",
    " \n",
    " url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},\n",
    " \n",
    " volume = {32},\n",
    " \n",
    " year = {2019}\n",
    " \n",
    "}\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ba193-953e-4536-b772-951efc05b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b56eb3-673e-49c3-a0a5-988ddf61475f",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "## Initializing a PyTorch Tensor - different approaches\n",
    "#### 1. From a <font color='DodgerBlue'>NumPy array</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f4535-9424-4fcd-b1cc-74ce9d07d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array_np = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "display(data_array_np)\n",
    "\n",
    "data_array_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee4f5e-e2fe-4072-869d-ac222e44a84c",
   "metadata": {},
   "source": [
    "PyTorch tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb1bb7-b75e-4bcc-b9c9-6bdebfb06017",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_tensor_torch = torch.from_numpy(data_array_np)\n",
    "\n",
    "generic_tensor_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d7ed7a-388a-4b7d-a773-21b896ae688c",
   "metadata": {},
   "source": [
    "#### 2. Based on an <font color='DodgerBlue'>existing tensor</font>\n",
    "- <font color='DodgerBlue'>retains</font> the <font color='DodgerBlue'>shape</font> (e.g., the `generic_tensor_torch` given above)\n",
    "- replaces the existing values with others, for example\n",
    "    - 1's via `ones_like`\n",
    "    - random values via `rand_like`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa1ca0-d394-47a1-bb3b-7dce8e9c9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones_like(generic_tensor_torch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f24b792-fc6b-4b3b-b204-e82f4b5004c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand_like(generic_tensor_torch, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d5808-29de-4b63-ac1e-075a713644f1",
   "metadata": {},
   "source": [
    "#### 3. Based on a <font color='DodgerBlue'>specific shape</font>\n",
    "\n",
    "<font color='DodgerBlue'>2-dimensional</font> tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122327a7-3620-4b2d-bfe7-94083b9f94aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_shape = (2, 3)\n",
    "\n",
    "rand_tensor = torch.rand(my_shape)\n",
    "\n",
    "rand_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ba5b08-50bb-47ba-b477-aadf5bd461f5",
   "metadata": {},
   "source": [
    "<font color='DodgerBlue'>3-dimensional</font> tensor:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e69602-dbe9-4334-8532-dabf881aaf6d",
   "metadata": {},
   "source": [
    "Explanation of the tensor's shape\n",
    "\n",
    "- 2: The number of matrices (the \"stacks\" or \"batches\").\n",
    "- 3: The number of rows in each matrix.\n",
    "- 5: The number of columns in each matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a038fbf-5119-4f31-a02d-18b7f76e217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_shape = (2, 3, 5)\n",
    "\n",
    "three_dim_tensor = torch.rand(my_shape)\n",
    "\n",
    "three_dim_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d08919-cb78-40c4-b8fe-a66998a364b4",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"></hr>\n",
    "\n",
    "## Book keeping\n",
    "- `tensor.shape`\n",
    "- `tensor.dtype`\n",
    "- `tensor.device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc7c9a-53e9-4cb3-8f7b-8fab42924958",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of tensor: {three_dim_tensor.shape}')\n",
    "print(f'Datatype of tensor: {three_dim_tensor.dtype}')\n",
    "print(f'Device tensor is stored on: {three_dim_tensor.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770596c1-9298-4fc4-96d4-4a5caa081956",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"></hr>\n",
    "\n",
    "## Tensor operations\n",
    "- https://pytorch.org/docs/stable/torch.html\n",
    "\n",
    "#### 1. Joining tensors via **concatenation**: `torch.cat`\n",
    "- https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat\n",
    "\n",
    "Combine <font color='DodgerBlue'>1 tensor</font>, <font color='DodgerBlue'>3 times</font>\n",
    "- `dim=0`: conceptually like adding more **rows** (same as NumPy)\n",
    "- `dim=1`: conceptually like adding more **columns** (same as NumPy)\n",
    "\n",
    "**Example 1**: As <font color='DodgerBlue'>rows</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a474c-3b08-4c51-a56c-d2e49a6bff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([rand_tensor, rand_tensor, rand_tensor], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6e0bb-8a26-4dfd-8ed9-47c4d5003cef",
   "metadata": {},
   "source": [
    "View **tensor** as a **Pandas' `DataFrame`** (same idea as we did with NumPy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218425d7-b080-4327-8c7c-1bb69e91febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(torch.cat([rand_tensor, rand_tensor, rand_tensor], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9140e3-b807-40a8-bd8c-fdcba93d7405",
   "metadata": {},
   "source": [
    "**Example 2**: As <font color='DodgerBlue'>columns</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a08d4a-311e-41b2-b93b-f9ca02480a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([rand_tensor, rand_tensor, rand_tensor], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b01d14e-7e5c-4402-afca-be08791285c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(torch.cat([rand_tensor, rand_tensor, rand_tensor], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68d47d-8fde-418a-ada8-90a22f917dfe",
   "metadata": {},
   "source": [
    "#### 2. Pairwise multiplication\n",
    "- **`torch.mul`**\n",
    "    - https://docs.pytorch.org/docs/stable/generated/torch.mul.html\n",
    "\n",
    "Multiply `rand_tensor` * `rand_tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ebd64-6e53-43d7-af68-0b84a9954be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mul(rand_tensor, rand_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6488e5f-bc06-4ece-90f2-b49c61dce028",
   "metadata": {},
   "source": [
    "- **Chaining methods**\n",
    "\n",
    "However, you could also do the following\n",
    "- a statement that <font color='DodgerBlue'>chains</font> the math together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d73337-10bd-4075-a0a0-2aa8b543b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_tensor.mul(rand_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ef08f-fd7e-46df-85ba-8b31d3c32c13",
   "metadata": {},
   "source": [
    "This makes reading <font color='DodgerBlue'>multpile operations easier</font>, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b32ed-718b-4c6e-b693-0dc4f4cdce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_tensor.abs().mul(rand_tensor).sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964e4bc4-3cc6-4ef7-9026-2a0755fac79f",
   "metadata": {},
   "source": [
    "For comparison, the equivalent using `torch.?` type commands (<font color='DodgerBlue'>harder to read</font>) would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8be1e6-9613-4e65-a947-5983b3627c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sqrt(torch.mul(torch.abs(rand_tensor), rand_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32744c-ca04-4ed9-abc6-2c499242fbc4",
   "metadata": {},
   "source": [
    "- **Operator (`*`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8823bf4-bf1d-4679-b168-597737b25827",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_tensor*rand_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63f46c2-a8a2-4b0e-86df-c3324b137dd5",
   "metadata": {},
   "source": [
    "One can also multiply individual tensor elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b220c621-4675-42a9-9d67-448551651f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_tensor[0][0] # Accessing specific values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc3df1-c658-4a7e-b88d-e9a437f0d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_tensor[0][0]*rand_tensor[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627146f0-e19e-44fc-8bcc-771322470ea4",
   "metadata": {},
   "source": [
    "START HERE\n",
    "\n",
    "## Using GPU\n",
    "\n",
    "Moving to GPU: <font color='DodgerBlue'>move a **specified** tensor</font> from the <font color='DodgerBlue'>**CPU**</font> to the <font color='DodgerBlue'>**GPU**</font>.\n",
    " \n",
    "(If youâ€™re using Colab, allocate a GPU by going to Edit > Notebook Settings.)\n",
    "\n",
    "Let's move our `rand_tensor` to GPU if possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121038eb-e028-48f4-bba4-521ee5a06ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDA is available - congrats!')\n",
    "    tensor = rand_tensor.to('cuda')\n",
    "    display(tensor.device)\n",
    "\n",
    "    print('\\nExample GPU calculation - adding the tensor to itself:')\n",
    "    result_tensor = tensor + tensor\n",
    "    display(result_tensor)\n",
    "else:\n",
    "    print('CUDA is not available - bummer.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230fba6a-6caa-4263-b60a-bf33a15672b7",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "1. `Torch` has a lot of <font color='DodgerBlue'>similarity</font> to `NumPy`\n",
    "2. However, it can be used on a **GPU**\n",
    "3. Some special \"things\" that make it unique\n",
    "    - <font color='dodgerblue'>history accumulation</font> (coming below)\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747e2d54-163d-4b6d-8468-1b1aee60421e",
   "metadata": {},
   "source": [
    "<div class=\"alert block alert-info alert\">\n",
    "\n",
    "# <center>Neural Networks (NNs)</center>\n",
    "\n",
    "## Background:\n",
    "- https://en.wikipedia.org/wiki/Neural_network_(machine_learning)\n",
    "\n",
    "<font color='dodgerblue'>\"A machine learning model is a **function**, with **inputs** and **outputs**.\" [1]</font>\n",
    "\n",
    "- a **collection of <font color='dodgerblue'>nested functions</font>**\n",
    "    - these functions are **executed on input data**\n",
    "    - these functions are defined by parameters (stored in PyTorch tensors)\n",
    "        - **weights**, and \n",
    "        - **biases**\n",
    "\n",
    "<br>\n",
    "\n",
    "What is the difference between **weights** and **bias** values?\n",
    "\n",
    "If we have a **linear <font color='DodgerBlue'>\"activation function\"</font>**: $\\mathbf{y = mx + b}$\n",
    "- **weight** (i.e., the weight for x): <font color='DodgerBlue'>$\\mathbf{m}$</font>\n",
    "    - for polynomial functions (e.g., linear equation), the **weights** are the **coefficients** (see \"Extra Information\" for a nonlinear example)\n",
    "<br>\n",
    "- **bias** (i.e., the equation's bias): <font color='dodgerblue'>$\\mathbf{b}$</font>\n",
    "    - <font color='DodgerBlue'>offset</font> - a constant term (consider the phase shifts we covered in SciPy lecture)\n",
    "\n",
    "**What is an <font color='DodgerBlue'>activation function</font>**\n",
    "- A function (surprise ðŸ™‚)\n",
    "- Calculates a **node's output** (i.e., the <font color='DodgerBlue'>information</font>) that is **passed** to the **next node**, using *its* specific input parameters (i.e., weights and bias)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Additional Resources\n",
    "\n",
    "##### Great Tutorial Series: **3Blue1Brown**\n",
    "- https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&feature=shared\n",
    "    - DeepLearning Chapter 1 (DL1): \"What is a NN\"\n",
    "    - DL2: \"Gradient descent, how neural networks learn\"\n",
    "    - DL3: \"Backpropagation, step-by-step\" (meta-level)\n",
    "    - DL4: \"Backpropagation calculus\" (the mathematics/formulas)\n",
    "\n",
    "##### TensorFlow Playground\n",
    "- (For later)\n",
    "- https://playground.tensorflow.org\n",
    "\n",
    "**Sources**:\n",
    "1. https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0179057-8326-4f7d-aba8-180bedc8f4fe",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <center>\n",
    "    <img src=\"00_images/31_machine_learning/deep_neural_network.png\" style=\"width: 800px; margin: 0 40px;\"/>\n",
    "    <figcaption style=\"margin-top: 10px; color: black; font-style: italic;\">\n",
    "          <b>Figure 1</b>: Example of a Neural Network.<br>\n",
    "        <b>Image Source</b>: https://www.studytonight.com/post/understanding-deep-learning.\n",
    "    </figcaption>\n",
    "  </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7d885-818a-4209-8ebc-f4f7de5be361",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<hr style=\"border:1.5px dashed gray\"></hr>\n",
    "\n",
    "## Extra Information\n",
    "\n",
    "### Sigmoid Activation Functions\n",
    "- What might a weighting factor (i.e., $w$) look like\n",
    "\n",
    "Sigmoid Equation:\n",
    "\\begin{equation}\n",
    "\\huge y(x) = \\frac{1}{1 + \\exp^{w*x}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea739d-4ebd-4702-814f-df3837582477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sigmoid(x_data: np.array, weight: float):\n",
    "    ''' Simple Sigmoid function with a weight.\n",
    "\n",
    "        Args:\n",
    "            x_data: independent data\n",
    "            weight: weighting factor that adjusts the equation's shape\n",
    "\n",
    "        Return\n",
    "            results: calculated results\n",
    "    '''\n",
    "\n",
    "    result = 1/(1 + np.exp(weight*-x_data))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411333d6-e011-4ce7-93a9-b7b663fb18e5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Plot a sigmoid equation using different weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e94bc89-f6fe-41e4-982e-b1989ff4ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_list = [-1, 1, 5]\n",
    "\n",
    "x_values = np.linspace(-5, 5)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for weight in weight_list:\n",
    "    y_values = weighted_sigmoid(x_data=x_values, weight=weight)\n",
    "\n",
    "    ax.plot(x_values, y_values, label=f'{weight}')\n",
    "\n",
    "plt.legend(loc='right', title='weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac0cab-e149-465c-b50f-e41decbb3789",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "So, we see that the **weighting factor** can **adjust** the **function's shape**, and consequently **adjust** the **information** being **passed** from one node to another.\n",
    "    \n",
    "<hr style=\"border:1.5px dashed gray\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49615985-3aa6-49c4-a663-ad68b0ee12f9",
   "metadata": {},
   "source": [
    "## NN Training\n",
    "\n",
    "Training is done in **2 steps**\n",
    "\n",
    "1. **Forward Propagation**\n",
    "    - <font color='DodgerBlue'>Pass data</font> (observables, weights and biases) <font color='DodgerBlue'>forward</font> to **predict target observable**\n",
    "        - Compute the **activation function** that connects each node\n",
    "    - (Basically, running the model as it will be eventually used.)\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Backward Propagation**\n",
    "    - <font color='DodgerBlue'>Adjusts parameters</font> proportionally to the <font color='DodgerBlue'>observable error</font> (i.e., the forward propagation's output/result vs. the target)\n",
    "    - Done by\n",
    "        - traversing backwards from the output (i.e., looping back the first layer),\n",
    "        - collect the **derivatives** (i.e., <font color='DodgerBlue'>gradients</font>) of the error with respect to the functions' parameters,\n",
    "        - **optimizes** the parameters using <font color='DodgerBlue'>**gradient descent**</font> method.\n",
    "\n",
    "**`torch.autograd`** is the workhorse for <font color='DodgerBlue'>**backward propagation**</font>.\n",
    "\n",
    "**Source**: https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c10707f-818a-4e9a-ab01-6b2ebbc03f5c",
   "metadata": {},
   "source": [
    "### Tensor History\n",
    "\n",
    "A key concept in PyTorch is that it's <font color='dodgerblue'>tensor objects</font> can have a <font color='dodgerblue'>**\"history\"**</font> (i.e., **new data** that is **attached** to the object **after** it is used in a **calculation**).\n",
    "\n",
    "For example, using `torch.rand`:\n",
    "- `torch.rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None,` **`requires_grad=False,`**` pin_memory=False) â†’ Tensor`\n",
    "\n",
    "- **`requires_grad=True`**: for every computation that follows, **`autograd`** will <font color='dodgerblue'>record the computation's history</font> in the output tensors\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Example: A <font color='DodgerBlue'>Single Layer Perceptron</font> (SLP)\n",
    "- The simplest type of artificial NN\n",
    "    -  classify binary categories\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Perceptron\n",
    "\n",
    "<figure>\n",
    "  <center>\n",
    "    <img src=\"00_images/31_machine_learning/perceptron_english.png\" style=\"width: 500px; margin: 0 40px;\"/>\n",
    "    <figcaption style=\"margin-top: 10px; color: black; font-style: italic;\">\n",
    "          <b>Figure 2</b>: An illustration of a single layer perceptron.<br>\n",
    "        <b>Image Source</b>: https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png.\n",
    "    </figcaption>\n",
    "  </center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Our **example workflow** will be:\n",
    "\n",
    "1. **Create input data** (e.g., independent x data)\n",
    "    - `requires_grad=True`\n",
    "2. **Create** a simple **model** based on an equation: $\\mathbf{y = a*sin(b*x)}$\n",
    "    - the activation function \n",
    "4. **Forward Propagation**\n",
    "5. **Backwards Propagation** (i.e., compute & collect first derivatives)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. Create input data\n",
    "   - Everything is weighted equally (i.e., <font color='DodgerBlue'>no weights</font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be95448f-8131-465b-b9a7-ba82d0b1c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.linspace(0.0, 2.0*torch.pi, steps=25, requires_grad=True)\n",
    "\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fddefb7-c66f-4d63-9c86-8e2259c96f3d",
   "metadata": {},
   "source": [
    "Notice the `requires_grad=True` in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bbb2fd-54df-4b52-a90d-d09aa041b058",
   "metadata": {},
   "source": [
    "#### 2. Create a simple model based on an **activation equation**\n",
    "- Equation: $\\mathbf{y = a*sin(b*x)}$\n",
    "    - <font color='DodgerBlue'>Hyperparameters</font>\n",
    "        - $\\mathbf{a}$: amplitude\n",
    "        - $\\mathbf{b}$: period/frequency (a.k.a. wavenumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab273b-d3dd-4305-8892-df9df830bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine_activation_func(x: float, a: float, b: float) -> float:\n",
    "    ''' Sine function for use in PyTorch neural networks.\n",
    "    ''' \n",
    "    return a * torch.sin(b * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819448d7-dd11-4c4e-86da-0a55332f27f6",
   "metadata": {},
   "source": [
    "#### 3. Forward Propagation\n",
    "\n",
    "<font color='dodgerblue'>Activation Function: $\\mathbf{y = a*sin(b*x)}$</font>\n",
    "\n",
    "This is a <font color='dodgerblue'>\"**Single Layer Perceptron**\" NN:\n",
    "- **1 input layer** (n features $\\rightarrow$ n nodes),\n",
    "- **1 activation function** (i.e., a transfer function that gets the data ready)\n",
    "- **1 node (in 1 layer)** $\\rightarrow$ output\n",
    "- **Initial Guess**\n",
    "    - a = 1\n",
    "    - b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede742f1-b29f-4808-9490-7ea1c849a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_model = sine_activation_func(x=input_data, a=1, b=1)\n",
    "\n",
    "sin_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4194aec-97c2-4654-85c2-bb8ba9b86a71",
   "metadata": {},
   "source": [
    "What operations just happened:\n",
    "1. `torch.sin(b * x)` creates a tensor with `grad_fn=<SinBackward0>`.\n",
    "2. `a * (that sine result)` creates a new tensor with `grad_fn=<MulBackward0>`.\n",
    "\n",
    "Two things to note:\n",
    "- in the tensor output above, the **`grad_fn=<MulBackward0>`** tells us that this object <font color='DodgerBlue'>**is accumulating history**</font>.\n",
    "- That is the **first** output of our <font color='DodgerBlue'>**forward propogation**</font>.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Okay, let's compare this to what would have happened if `requires_grad=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846cb62d-1e60-4c08-a4d9-fe0775994a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_false = torch.linspace(0.0, 2.0*torch.pi, steps=25, requires_grad=False)\n",
    "\n",
    "print(input_data_false)\n",
    "\n",
    "torch.sin(input_data_false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df65483-4af6-4988-9d99-f9c79a5d28bc",
   "metadata": {},
   "source": [
    "Notice this changes the object - **no** `grad_fn=<SinBackward0>` - thus, **no history accumulation**.\n",
    "<br><br>\n",
    "\n",
    "#### History Accumulation Demonstration\n",
    "\n",
    "Okay, now back to our `sin_model`.\n",
    "\n",
    "\n",
    "##### <font color='DodgerBlue'>Visualizing Data</font>\n",
    "\n",
    "Let's first visualize what we are working with.\n",
    "\n",
    "- Since <font color='dodgerblue'>history is accumulated</font> in `sin_model`, we can now use `detach()` to <font color='dodgerblue'>**grab**</font> only the tensor **values**:\n",
    "    - `Tensor.detach`: Returns a new tensor, detached from the current graph (https://docs.pytorch.org/docs/stable/generated/torch.Tensor.detach.html).\n",
    "    - To **plot**, we must **detach** the tensor - otherwise we obtain **errors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc7f1c7-bd85-4d43-98a9-ceff972607ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_model.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693bd170-c49c-478b-a005-b75537239753",
   "metadata": {},
   "source": [
    "**Visualize** by plotting the\n",
    "- <font color='DodgerBlue'>independent</font> variable (i.e., `input_data.detach()`), and\n",
    "- <font color='DodgerBlue'>dependent</font> variable (i.e., `sin_model.detach()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d1ece-2a53-4a66-a5b3-9cd1d262151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(input_data.detach(), sin_model.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e767cd7-b29f-47d9-a89e-b1d859dbbb2f",
   "metadata": {},
   "source": [
    "##### <font color='DodgerBlue'>Adding to the History</font>\n",
    "\n",
    "Let's include **an additional operation** on the `sin_model` by summing values:\n",
    "\n",
    "- `torch.sum()`: https://pytorch.org/docs/stable/generated/torch.sum.html\n",
    "- Record to a new object (i.e., `sin_sum`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8daff1-5830-4e16-8081-4a0c2c01370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_sum = sin_model.sum()\n",
    "print(sin_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a1b2f-ee64-42d3-9f21-0963e7b78418",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sin_sum.grad_fn)\n",
    "print(sin_sum.grad_fn.next_functions)\n",
    "print(sin_sum.grad_fn.next_functions[0][0].next_functions)\n",
    "print(sin_sum.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(sin_sum.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions) \n",
    "print(sin_sum.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a518bd2-323f-4876-96aa-ced3525dbbee",
   "metadata": {},
   "source": [
    "We see that if we access the <font color='DodgerBlue'>\"history\"</font> portion of the object via `grad_fn`, we get a short explanation of what it is\n",
    "- <font color='DodgerBlue'>**6 total operations**</font>, from <font color='DodgerBlue'>**most recent**</font> operation $\\rightarrow$ <font color='DodgerBlue'>**oldest**</font> operation\n",
    "\n",
    "    #6: `SumBackward0` $\\rightarrow$ `sin_model.sum()`\n",
    "\n",
    "    #5: `MulBackward0` $\\rightarrow$ `b*x`\n",
    "       \n",
    "    #4: `SinBackward0` $\\rightarrow$ `torch.sin(input_data)`\n",
    "  \n",
    "    #3: `MulBackward0` $\\rightarrow$ `a*sin`\n",
    "  \n",
    "- Note: the above three operations come from the line: `sin_model = sine_activation_func(x=input_data, a=1, b=1)`\n",
    "  \n",
    "    #2: `AccumulateGrad` $\\rightarrow$ `torch.linspace(0.0, 2.0*np.pi, steps=25, requires_grad=True)`\n",
    "  \n",
    "    #1: `()` $\\rightarrow$ starting point\n",
    "\n",
    "With that basic understanding in place, we can now continue to the <font color='DodgerBlue'>backward propagation</font> idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28aa9a0-e69a-4d73-b763-8a3b40624f36",
   "metadata": {},
   "source": [
    "### Taking the first derivative of sin(x)\n",
    "- a.k.a the **\"gradient\"**\n",
    "\n",
    "\\begin{equation}\n",
    "    \\Large\n",
    "    \\frac{d}{dx} sin(x) = cos(x)\n",
    "\\end{equation}\n",
    "\n",
    "##### `Autograd`: <font color='DodgerBlue'>Computing gradients</font>\n",
    "\n",
    "- Note: `autograd` is used when calling `backward()` function (i.e., `autograd.backwards`)\n",
    "- https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed3a155-6b86-4236-89ff-24373072cd78",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<hr style=\"border:1.5px dashed gray\"></hr>\n",
    "\n",
    "## Extra Information\n",
    "\n",
    "### `autograd.grad`\n",
    "\n",
    "Both `autograd.grad` and `autograd.backwards` take derivatives. However, they differ concerning if they affect the accumulation history.\n",
    "\n",
    "`autograd.grad`: compute gradients for specific tensors without affecting others (e.g., **does not** accumulate history)\n",
    "\n",
    "Source: https://www.geeksforgeeks.org/understanding-pytorchs-autogradgrad-and-autogradbackward/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2801a8-e2a6-4f4c-bb91-71ea805c373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new object\n",
    "test_sin_model = torch.sin(input_data)\n",
    "test_sin_sum = test_sin_model.sum()\n",
    "test_grad = torch.autograd.grad(outputs=test_sin_sum, inputs=input_data)\n",
    "\n",
    "display(test_grad)\n",
    "display(test_grad[0].sum())\n",
    "# print(test_grad.grad_fn) ## demonstrates the lack of history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4f7bc6-f604-4677-a5ee-34959b31b6d1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<hr style=\"border:1.5px dashed gray\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a338827-1fb5-40fd-8cdb-76c34b87860f",
   "metadata": {},
   "source": [
    "#### 1. `backward()`: compute gradients\n",
    "\n",
    "- will be added to the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f560d2f3-8c39-4a61-a878-c0aaf4ac18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_sum.backward()\n",
    "sin_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fafdaf2-31d3-4cfd-b300-597fd3a72903",
   "metadata": {},
   "source": [
    "**Output explanation**:\n",
    "\n",
    "- `1.4504e-07`: <font color='DodgerBlue'>**sum**</font> of all **elements** in the `sin_model` **tensor**\n",
    "- `<SumBackward0>`: name of the PyTorch function (i.e., sum during backward propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83794cc4-3d39-4d36-b466-1dbf567859d6",
   "metadata": {},
   "source": [
    "Let's verify that `1.4504e-07` is indeed the sum of the tensor elements.\n",
    "\n",
    "- Recall that `sin_sum` was created using `sin_sum = sin_model.sum()`\n",
    "- Thus, we have to go back to `sin_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d262a2-aa5e-404a-8dcc-8f686f8d75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sin_model.sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee692bf6-0c8a-456a-9bd6-81b97c402065",
   "metadata": {},
   "source": [
    "So, everything looks okay.\n",
    "\n",
    "<br>\n",
    "\n",
    "- <font color='DodgerBlue'>**Uncovering what `backward()` actually did.**</font>\n",
    "\n",
    "    - What did `backward()` do with the history?\n",
    "\n",
    "`sin_sum.backward()` <font color='dodgerblue'>created a **`grad` property</font>** within the <font color='dodgerblue'>**original input data</font>**  (i.e., `input_data`) as part of its **accumulated history**.\n",
    "\n",
    "(A property of an object is coming from a Python class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0b1c05-4218-4bb0-8ffc-64e1617304ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Input values:'\n",
    "      f'\\n{input_data}')\n",
    "\n",
    "print()\n",
    "print(f'Gradient values:'\n",
    "      f'\\n{input_data.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff5c500-8685-4275-9efd-f8116c1f5c97",
   "metadata": {},
   "source": [
    "##### Side note: Verification of the derivative of sine\n",
    "\n",
    "Prove to ourselves that `backwards()` computes the sin(x) derivative:\n",
    "\n",
    "$$\\Large \\frac{d}{dx}sin(x) = cos(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6520496-690b-4652-af1c-0387d79815d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_derivative_sin = np.cos(input_data.detach().numpy())\n",
    "first_derivative_sin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd1b27-8ff6-4194-81cb-126161cc8fb9",
   "metadata": {},
   "source": [
    "Visualize <font color='DodgerBlue'>PyTorch's</font> **`sin_sum.backward()`** output versus <font color='DodgerBlue'>our computed derivative</font> (i.e., **`first_derivative_sin`**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb7b0a-4a81-4ae4-8c24-9f67d39cb965",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(input_data.detach(), input_data.grad.detach(), linestyle='-', linewidth=10)\n",
    "\n",
    "plt.plot(input_data.detach(), first_derivative_sin, linestyle='--', linewidth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c804ed-21eb-4611-8bdd-a3e0396a4afa",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "- We confirm that the `backwards()` does indeed take the derivative of a tensor.\n",
    "\n",
    "<font color='DodgerBlue'>**Important**</font>: What is left that is missing is the **weights** and **<font color='DodgerBlue'>bias optimization</font>** portion of the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd02c0c-d189-4058-995e-7fc62f48b45f",
   "metadata": {},
   "source": [
    "# Take-home\n",
    "\n",
    "1. PyTorch is similar to NumPy\n",
    "    - similar functions\n",
    "    - `torch` instead of `array`\n",
    "    - able to use GPUs (not just CPUs)\n",
    "2.  Basics of neural networks\n",
    "    - forward propagation\n",
    "    - backwards propagation\n",
    "3. History accumulation\n",
    "4. Example: A Single Layer Perceptron (SLP)\n",
    "\n",
    "The above forms the foundation for understading how PyTorch is \"typically\" used in a ML project -- a future lecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
