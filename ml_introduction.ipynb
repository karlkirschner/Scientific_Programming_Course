{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (ML)\n",
    "\n",
    "**\"Can machines think [in the way that we do]?\"** [1]\n",
    "\n",
    "\n",
    "- The ML term was <font color='dodgerblue'>first used in 1959</font> by Arthur Samuel (an IBM researcher)\n",
    "\n",
    "\n",
    "- Mathematical Foundation\n",
    "    - <font color='dodgerblue'>Statistics</font> (the \"work-horse\" of ML)\n",
    "    - Calculus (derivatives; optimizations)\n",
    "    - Algerbra (vectors, matrix, tensors)\n",
    "\n",
    "\n",
    "- Different components were developed by researchers for many years. Only recently they were collected into libraries that make the ideas more accessible.\n",
    "\n",
    "## Machine Learning Catagories\n",
    "\n",
    "1. <font color='dodgerblue'>Shallow learning (e.g. **s**ci**k**it-**learn** - a.k.a. **sklearn**)\n",
    "    - <font color='dodgerblue'>predefined features</font>\n",
    "\n",
    "1. Deep learning (e.g. TensorFlow, PyTorch)\n",
    "    - <font color='dodgerblue'>feature learning</font>\n",
    "    - mostly <font color='dodgerblue'>combines shallow learning<font color='dodgerblue'> instances together</font> into <font color='dodgerblue'>\"layers\"</font>\n",
    "\n",
    "\n",
    "**Sources**:\n",
    "1. Turing, Alan M. \"Computing machinery and intelligence.\" Parsing the Turing test. Springer, Dordrecht, 2009. 23-65.\n",
    "\n",
    "**Additional Resources**:\n",
    "1. https://en.wikipedia.org/wiki/Machine_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "# Shallow Learning\n",
    "\n",
    "## Catagories\n",
    "\n",
    "| Regression | Classification | Clustering | Dimension Reduction|\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| <font color='dodgerblue'>Linear</font> | Logistic Regression | <font color='dodgerblue'>K-means</font> | <font color='dodgerblue'>Principle Component Analysis</font> |\n",
    "| <font color='dodgerblue'>Polynomial</font> | <font color='dodgerblue'>Support Vector Machine</font> | Mean-Shift | Linear Discriminant Analysis |\n",
    "| StepWise | Naive Bayes | DBScan | Gernalized Discriminant Analysis |\n",
    "| Ridge | Nearest Neighbor | Agglomerative Hierachcial | Autoencoder |\n",
    "| Lasso | Decision Tree | Spectral Clustering | Non-Negative Matrix Factorization |\n",
    "| ElasticNet | <font color='dodgerblue'>Random Forest</font> | Gaussian Mixture | UMAP |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs. Unsupervised Learning\n",
    "\n",
    "1. **Supervised** - the **target information is known** in the data set, and we **train to reproduce** that information\n",
    "    - <font color='dodgerblue'>regression</font>\n",
    "    - <font color='dodgerblue'>classification</font>\n",
    "\n",
    "1. **Unsupervised** - the **target information is unknown**, with the goal to \n",
    "    - cluster the data's similarity (<font color='dodgerblue'>clustering</font>)\n",
    "    - determine the distribution of data (<font color='dodgerblue'>density estimation</font>)\n",
    "    - <font color='dodgerblue'>dimensionality reduction</font> for exploring and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"Accuracy vs Precision\" width=\"800\" src=\"00_images/31_machine_learning/scikit_learn_ml_map.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
    "\n",
    "Image Source (interactive): https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "# Deep Learning - Unsupervised Learning\n",
    "\n",
    "\n",
    "#### Neural network\n",
    "- **Input Layer**: <font color='dodgerblue'>features (observables)</font> should have some degree of correlation (i.e., structure; nonlinear relationships)\n",
    "- Encoder: input $\\rightarrow$ hidden layers (<font color='dodgerblue'>data reduction</font>)\n",
    "- **Hidden Layer**: a <font color='dodgerblue'>compressed knowledge representation</font> of the original input\n",
    "- Decoder: hidden layers $\\rightarrow$ <font color='dodgerblue'>output</font>\n",
    "- **Output Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"neural network\" width=\"800\" src=\"00_images/31_machine_learning/deep_neural_network.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
    "\n",
    "Image Source: https://www.studytonight.com/post/understanding-deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoders - generative models (i.e., <font color='dodgerblue'>creates new things</font>)\n",
    "\n",
    "Autoencoders are involved in deep learning algorithms. They **encode an input** (i.e., something that is human-relatable) and **transform it into a different representation** within the latent space, and then **decode** back to something **human-relatable**. This allows for new things to be generated.\n",
    "\n",
    "\n",
    "- https://www.jeremyjordan.me/autoencoders/\n",
    "- <font color='dodgerblue'>Sparse</font> Autoencoder\n",
    "    - **hidden** layers have the **same number of nodes** as the **input** and **output** layers\n",
    "    - loss function includes a penalty for \"activating\" a node within the hidden layer\n",
    "\n",
    "<br>\n",
    "\n",
    "- <font color='dodgerblue'>Denoising</font> Autoencoder\n",
    "    - slightly **corrupt** the **input data** (i.e., add noise) to help make the encoding/decoding more generalizable\n",
    "    - **target data** remains **uncorrupted**\n",
    "    - make the decoding (reconstruction function) insensitive to small changes in the input\n",
    "\n",
    "<br>\n",
    "\n",
    "- <font color='dodgerblue'>Contractive</font> Autoencoder\n",
    "    - make the **encoding** (feature extraction function) **less sensitive** to **small changes** within the **input data**\n",
    "    - learn similar encoding (hidden layer) for different inputs that vary slightly\n",
    "\n",
    "<br>\n",
    "\n",
    "- <font color='dodgerblue'>Variational</font> Autoencoder (VAE)\n",
    "    - https://arxiv.org/abs/1606.05908\n",
    "    - training using **backpropagation** (aka **backward propagation of error**)\n",
    "        - backpropagation - https://www.ibm.com/think/topics/backpropagation\n",
    "        - starting from an **output**, compute the **importance** (measured as a gradient) that each neural network **parameter** has on the final model's **error** (predicted values) (i.e., loss function)\n",
    "    - encoding is **regularized** (adding a penalty term to the model's loss function during the learning process) to ensure that the latent space has good properties (and thus, allowing us to have generative models to be created)\n",
    "        - regularization - https://en.wikipedia.org/wiki/Regularization_(mathematics)\n",
    "\n",
    "\n",
    "\n",
    "<!-- - Generative Adversarial Networks (GANs)\n",
    "    - two networks oppose each other (a generator and a discriminator), for which both iteratively improve -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
