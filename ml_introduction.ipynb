{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert block alert-info alert\">\n",
    "\n",
    "# <center> Scientific Programming in Python\n",
    "\n",
    "## <center>Karl N. Kirschner<br>Bonn-Rhein-Sieg University of Applied Sciences<br>Sankt Augustin, Germany\n",
    "\n",
    "# <center> Machine Learning (ML) Overview\n",
    "\n",
    "**\"Can machines think [in the way that we do]?\"** [1]\n",
    "\n",
    "\n",
    "- The ML term was <font color='dodgerblue'>**first used in 1959**</font> by Arthur Samuel (an IBM researcher)\n",
    "\n",
    "\n",
    "- Mathematical Foundation\n",
    "    - <font color='dodgerblue'>**Statistics**</font> (the \"work-horse\" of ML)\n",
    "    - Calculus (derivatives; optimizations)\n",
    "    - Algerbra (vectors, matrix, tensors)\n",
    "\n",
    "\n",
    "- Different components were developed by researchers for many years. Only recently they were collected into libraries that make the ideas more accessible.\n",
    "\n",
    "## Machine Learning Catagories\n",
    "\n",
    "1. <font color='dodgerblue'>**Shallow learning** (e.g. **s**ci**k**it-**learn** - a.k.a. **sklearn**)\n",
    "    - <font color='dodgerblue'>**predefined features**</font>\n",
    "\n",
    "1. Deep learning (e.g. TensorFlow, PyTorch)\n",
    "    - <font color='dodgerblue'>**feature learning**</font>\n",
    "    - mostly <font color='dodgerblue'>**combines shallow learning**</font> instances together into <font color='dodgerblue'>**\"layers\"**</font>\n",
    "\n",
    "\n",
    "**Sources**:\n",
    "1. Turing, Alan M. \"Computing machinery and intelligence.\" Parsing the Turing test. Springer, Dordrecht, 2009. 23-65.\n",
    "\n",
    "**Additional Resources**:\n",
    "1. https://en.wikipedia.org/wiki/Machine_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For extra information given within the lectures\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def set_code_background(color: str):\n",
    "    ''' Set the background color for code cells.\n",
    "\n",
    "        Source: psychemedia via https://stackoverflow.com/questions/49429585/\n",
    "                how-to-change-the-background-color-of-a-single-cell-in-a-jupyter-notebook-jupy\n",
    "\n",
    "        To match Jupyter's dev class colors:\n",
    "            \"alert alert-block alert-warning\" = #fcf8e3\n",
    "\n",
    "        Args:\n",
    "            color: HTML color, rgba, hex\n",
    "    '''\n",
    "\n",
    "    script = (\"var cell = this.closest('.code_cell');\"\n",
    "              \"var editor = cell.querySelector('.input_area');\"\n",
    "              f\"editor.style.background='{color}';\"\n",
    "              \"this.parentNode.removeChild(this)\")\n",
    "    display(HTML(f'<img src onerror=\"{script}\">'))\n",
    "\n",
    "\n",
    "set_code_background(color='#fcf8e3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two General Types of ML\n",
    "\n",
    "1. **Shallow Learning**\n",
    "2. **Deep Learning**\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "# Shallow Learning\n",
    "\n",
    "## Catagories\n",
    "\n",
    "| Regression | Classification | Clustering | Dimension Reduction|\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| <font color='dodgerblue'>Linear</font> | Logistic Regression | <font color='dodgerblue'>K-means</font> | <font color='dodgerblue'>Principle Component Analysis</font> |\n",
    "| <font color='dodgerblue'>Polynomial</font> | <font color='dodgerblue'>Support Vector Machine</font> | Mean-Shift | Linear Discriminant Analysis |\n",
    "| StepWise | Naive Bayes | DBScan | Gernalized Discriminant Analysis |\n",
    "| Ridge | Nearest Neighbor | Agglomerative Hierachcial | Autoencoder |\n",
    "| Lasso | Decision Tree | Spectral Clustering | Non-Negative Matrix Factorization |\n",
    "| ElasticNet | <font color='dodgerblue'>Random Forest</font> | Gaussian Mixture | UMAP |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs. Unsupervised Learning\n",
    "\n",
    "1. **Supervised** - the **target information is known** in the data set, and we **train to reproduce** that information\n",
    "    - <font color='dodgerblue'>regression</font>\n",
    "    - <font color='dodgerblue'>classification</font>\n",
    "\n",
    "1. **Unsupervised** - the **target information is unknown**, with the goal to \n",
    "    - cluster the data's similarity (<font color='dodgerblue'>clustering</font>)\n",
    "    - determine the distribution of data (<font color='dodgerblue'>density estimation</font>)\n",
    "    - <font color='dodgerblue'>dimensionality reduction</font> for exploring and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"Accuracy vs Precision\" width=\"800\" src=\"00_images/31_machine_learning/scikit_learn_ml_map.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
    "\n",
    "Image Source (interactive): https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "# Deep Learning\n",
    "- **Supervised Learning**: trained on <font color='dodgerblue'>labeled data</font>, with specified goals (clearly defined <font color='dodgerblue'>prediction/classification</font> goals)\n",
    "    - learn a <font color='dodgerblue'>mapping</font> from <font color='dodgerblue'>input features</font> to known <font color='dodgerblue'>output labels</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Unsupervised Learning**: trained on <font color='dodgerblue'>unlabeled data</font>, with specified goals (discovering and insights)\n",
    "    - discover hidden <font color='dodgerblue'>patterns, structures, relationships</font>, or <font color='dodgerblue'>insights</font> within the data itself.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Reinforcement Learning**: learning through interaction that includes maximizing rewards (no labeled data, no specified goal) \n",
    "\n",
    "<br>\n",
    "\n",
    "#### Neural network\n",
    "- **Input Layer**: <font color='dodgerblue'>features (observables)</font> should have some degree of correlation (i.e., structure; nonlinear relationships)\n",
    "- Encoder: input $\\rightarrow$ hidden layers (<font color='dodgerblue'>data reduction</font>)\n",
    "- **Hidden Layer**: a <font color='dodgerblue'>compressed knowledge representation</font> of the original input\n",
    "- Decoder: hidden layers $\\rightarrow$ <font color='dodgerblue'>output</font>\n",
    "- **Output Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"neural network\" width=\"800\" src=\"00_images/31_machine_learning/deep_neural_network.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
    "\n",
    "Image Source: https://www.studytonight.com/post/understanding-deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Type of Deep Learning Neural Networks (NN):\n",
    "1. Convolutional NN\n",
    "2. Recurrent NN (RNNs)\n",
    "    - Long Short-Term Memory (LSTM) Networks (a sepecial type of RNN)\n",
    "3. Generative Adversarial Networks (GANs)\n",
    "4. Reinforcement Learning (RL) with Deep Learning (Deep RL)\n",
    "\n",
    "\n",
    "## Python Libraries\n",
    "1. <font color='dodgerblue'>**TensorFlow**</font>\n",
    "    - open-source library\n",
    "3. <font color='dodgerblue'>**Keras**</font> (integrated into TensorFlow: `tf.keras`)\n",
    "    - designed for fast experimentation\n",
    "4. <font color='dodgerblue'>**PyTorch**</font>\n",
    "   - open-source library\n",
    "   - easy to use (Pythonic)\n",
    "   - easy prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<hr style=\"border:1.5px dashed gray\"></hr>\n",
    "\n",
    "\n",
    "#### Specific Example(s): Autoencoders - generative models (i.e., <font color='dodgerblue'>creates new things</font>)\n",
    "\n",
    "**Autoencoder** neural networks are an unsupervised (i.e., using unlabeled input data) learning model. They **encode an input** (i.e., something that is human-relatable) and **transform it into a different representation** within the latent space, and then **decode** back to something **human-relatable**. This allows for new things to be generated.\n",
    "\n",
    "\n",
    "- https://www.jeremyjordan.me/autoencoders/\n",
    "- <font color='dodgerblue'>Sparse</font> Autoencoder\n",
    "    - **hidden** layers have the **same number of nodes** as the **input** and **output** layers\n",
    "    - loss function includes a penalty for \"activating\" a node within the hidden layer\n",
    "\n",
    "<br>\n",
    "\n",
    "- <font color='dodgerblue'>Denoising</font> Autoencoder\n",
    "    - slightly **corrupt** the **input data** (i.e., add noise) to help make the encoding/decoding more generalizable\n",
    "    - **target data** remains **uncorrupted**\n",
    "    - make the decoding (reconstruction function) insensitive to small changes in the input\n",
    "\n",
    "<br>\n",
    "\n",
    "- <font color='dodgerblue'>Contractive</font> Autoencoder\n",
    "    - make the **encoding** (feature extraction function) **less sensitive** to **small changes** within the **input data**\n",
    "    - learn similar encoding (hidden layer) for different inputs that vary slightly\n",
    "\n",
    "<br>\n",
    "\n",
    "- <font color='dodgerblue'>Variational</font> Autoencoder (VAE)\n",
    "    - https://arxiv.org/abs/1606.05908\n",
    "    - training using **backpropagation** (aka **backward propagation of error**)\n",
    "        - backpropagation - https://www.ibm.com/think/topics/backpropagation\n",
    "        - starting from an **output**, compute the **importance** (measured as a gradient) that each neural network **parameter** has on the final model's **error** (predicted values) (i.e., loss function)\n",
    "    - encoding is **regularized** (adding a penalty term to the model's loss function during the learning process) to ensure that the latent space has good properties (and thus, allowing us to have generative models to be created)\n",
    "        - regularization - https://en.wikipedia.org/wiki/Regularization_(mathematics)\n",
    "\n",
    "\n",
    "<hr style=\"border:1.5px dashed gray\"></hr>\n",
    "<!-- - Generative Adversarial Networks (GANs)\n",
    "    - two networks oppose each other (a generator and a discriminator), for which both iteratively improve -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Workflow For Model Creation (and Prediction)\n",
    "\n",
    "1. **Understand your goal** - do you want to predict\n",
    "    - Categorical data (i.e., noncontinuous data)\n",
    "    - Numerical data (i.e., continuous data)\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Data**\n",
    "    - Collect, clean (e.g. drop rows with missing data) and adjust magnitudes (e.g., **normalize**)\n",
    "    - Determine training features versus target features (i.e. what you want to predict)\n",
    "        - Training features (**independent variables** -- x-axis data)\n",
    "        - Target features (**dependent variable(s)** -- y-axis data)\n",
    "    - Encode any categorical data present (i.e., provide numerical values)\n",
    "    - Data splitting (**training and test sets**)\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Model Exploration**\n",
    "    - Choose several models to try \n",
    "    - Default parameters (\"hyperparameters\")\n",
    "    - Identify good candidates (see #4 below)\n",
    "    - Optimize hyperparameters\n",
    "\n",
    "<br>\n",
    "\n",
    "4. **Model Evaluation and Determination**\n",
    "    - Choose and compute different metrics\n",
    "\n",
    "<br>\n",
    "\n",
    "5. **Apply Model using New Data**\n",
    "    - I.e., make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained Model Evaluation\n",
    "\n",
    "### Classification-Data Metrics:\n",
    "\n",
    "Evaluating classification models: How well they correctly assign instances?\n",
    "\n",
    "1. <font color='dodgerblue'>**Accuracy**</font>: The proportion of correctly classified instances out of the total instances.\n",
    "\n",
    "    **Caution**: Can be misleading with imbalanced datasets (e.g., 95% of data is in one class).\n",
    "\n",
    "<br>\n",
    "\n",
    "2. <font color='dodgerblue'>**Confusion Matrix**</font> (foundation for many other metrics): A summarizing performance matrix that shows how many:\n",
    "    - True Positives (**TP**): A model's outcome that **correctly** predicted the **positive** (e.g., \"yes\"; \"with disease\") class\n",
    "    - True Negatives (**TN**): A model's outcome that **correctly** predicted the **negative** (e.g., \"no\"; \"without disease\") class\n",
    "    - False Positives (**FP**) (a.k.a. Type I error): A model's outcome that **incorrectly** predicted the **positive** class\n",
    "    - False Negatives (**FN**) (a.k.a. Type II error): A model's outcome that **incorrectly** predicted the **negative** class\n",
    "\n",
    "<br>\n",
    "\n",
    "|               | **Actual (target) Positive** | **Actual (target) Negative** |\n",
    "| :------------ | :------------------ | :------------------ |\n",
    "| **Predicted Positive** | # True Positive (TP)  | # False Positive (FP) |\n",
    "| **Predicted Negative** | # False Negative (FN) | # True Negative (TN)  |\n",
    "\n",
    "An example for a \"good\" model (i.e., F1 score = 0.86):\n",
    "\n",
    "|               | **Actual (target) Positive** | **Actual (target) Negative** |\n",
    "| :------------ | :------------------ | :------------------ |\n",
    "| **Predicted Positive** | 150  | 30 |\n",
    "| **Predicted Negative** | 20 |200  |\n",
    "\n",
    "<br>\n",
    "\n",
    "3. <font color='dodgerblue'>**Precision**</font>: Of all instances predicted as positive, what proportion were actually positive?\n",
    "    - Formula: $\\Large\\frac{TP}{TP+FP}$\n",
    "    - Use: Minimizing false positives is critical (e.g., medical diagnosis).\n",
    "\n",
    "<br>\n",
    "\n",
    "4. <font color='dodgerblue'>**Recall**</font> (a.k.a. Sensitivity or True Positive Rate): Of all actual positive instances, what proportion did the model correctly identify?\n",
    "    - Formula: $\\Large\\frac{TP}{TP+FN}$\n",
    "    - Use: Minimizing false negatives is critical (e.g., fraud detection where missing actual fraud is very costly, or disease screening where missing a sick patient is dangerous).\n",
    "\n",
    "<br>\n",
    "\n",
    "5. <font color='dodgerblue'>**F1-Score**</font>: The harmonic mean of precision and recall. (It balances the precision and recall metrics.)\n",
    "    - Formula: $\\Large\\frac{2∗(Precision∗Recall)}{Precision + Recall}$\n",
    "    - Use: A single metric that balances precision and recall, especially where there is a class imbalance.\n",
    "\n",
    "<!-- \n",
    "<br>\n",
    "\n",
    "6. <font color='dodgerblue'>**ROC Curve** and AUC</font> (Area Under the Receiver Operating Characteristic Curve):\n",
    "    - ROC Curve: Plots the True Positive Rate (i.e., Recall) against the False Positive Rate at various classification thresholds.\n",
    "    - AUC: Overall classifier performance,irrespective of the classification threshold. (A higher AUC indicates better discrimination between classes.)\n",
    "    - Use: evaluate the model's ability to distinguish between classes across all possible thresholds (good for imbalanced datasets).\n",
    "\n",
    "<br>\n",
    "\n",
    "7. <font color='dodgerblue'>**Log Loss**</font> (Cross-Entropy Loss): Measures the performance of a classification model where the prediction is a probability. It penalizes confident wrong predictions heavily.\n",
    "    - Use: You need to evaluate probabilistic outputs from models like logistic regression or neural networks. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical-Data Metrics\n",
    "Typically are regression problems (e.g., house price, temperature, age).\n",
    "\n",
    "1. <font color='dodgerblue'>**Mean Absolute Error (MAE)**</font>:\n",
    "    - Formula: $\\Large MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $, where $\\Large\\hat{y}_i$ is the ideal/target value\n",
    "    - Interpretation: It tells you the average magnitude of the errors.\n",
    "    - Strengths:\n",
    "        - Easy to understand and interpret.\n",
    "        - Robust to outliers.\n",
    "        - Units are those of the target variable's units (i.e., intuitive)\n",
    "    - Weaknesses:\n",
    "        - Not differentiable (less suitable as a loss function in optimization algorithms).\n",
    "\n",
    "<br>\n",
    "\n",
    "2. <font color='dodgerblue'>**Mean Squared Error (MSE)**</font>:\n",
    "    - Formula: $\\Large MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
    "    - Interpretation: Penalizes larger errors more heavily than smaller ones.\n",
    "    - Strengths:\n",
    "        - Differentiable (e.g., usable in Linear Regression, Neural Networks)\n",
    "        - Penalizes large errors.\n",
    "    - Weaknesses:\n",
    "        - Units are the square of the target variable's units.\n",
    "        - Highly sensitive to outliers.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. <font color='dodgerblue'>**Root Mean Squared Error (RMSE)**</font>:\n",
    "    - Formula: $\\Large RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 } $\n",
    "    - Interpretation: It brings the error back to the original units of the target variable.\n",
    "    - Strengths:\n",
    "        - Commonly used and widely understood.\n",
    "        - Penalizes large errors more than MAE.\n",
    "        - Units are those of the target variable's units.\n",
    "    - Weaknesses:\n",
    "        - Sensitive to outliers (less so than raw MSE).\n",
    "\n",
    "<br>\n",
    "\n",
    "4. <font color='dodgerblue'>**R-squared (R2)**</font> (a.k.a. Coefficient of Determination):\n",
    "    - Formula: $\\Large R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $\n",
    "    - Interpretation: It measures how well the model explains the variability of the target variable.\n",
    "    - Strengths:\n",
    "        - Provides a relative measure of fit, ranging from $-\\infty$ (very poor) to $0$ (poor) to $1$ (a perfect fit)\n",
    "        - Easy to understand as a percentage\n",
    "    - Weaknesses:\n",
    "        - Can increase simply by adding more features (this can lead to overfitting).\n",
    "        - Doesn't tell you the errors' magnitude in the original units.\n",
    "\n",
    "<!-- Adjusted R-squared\n",
    "\n",
    "Formula: Radj2​=1−n−p−1(1−R2)(n−1)​ where n is the number of data points and p is the number of predictors (features).\n",
    "Interpretation: Adjusted R-squared accounts for the number of predictors in the model. It will only increase if the new features significantly improve the model, penalizing the addition of irrelevant features.\n",
    "Strengths:\n",
    "    More reliable than R2 for comparing models with different numbers of predictors.\n",
    "Weaknesses:\n",
    "    Still a relative measure.\n",
    "\n",
    "Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "Formula: MAPE=n1​∑i=1n​​yi​yi​−y^​i​​​×100%\n",
    "Interpretation: MAPE expresses the error as a percentage of the actual value. This makes it useful for comparing models across different scales.\n",
    "Strengths:\n",
    "    Scale-independent, good for comparing performance across different datasets or models where the target variable has different magnitudes.\n",
    "    Easy to understand as a percentage.\n",
    "Weaknesses:\n",
    "    Undefined if yi​ is zero.\n",
    "    Can heavily penalize errors when yi​ is very small.\n",
    "    Asymmetric (penalizes over-predictions differently from under-predictions).\n",
    "\n",
    "Root Mean Squared Logarithmic Error (RMSLE)\n",
    "\n",
    "Formula: RMSLE=n1​∑i=1n​(log(yi​+1)−log(y^​i​+1))2​\n",
    "Interpretation: RMSLE measures the ratio between actual and predicted values rather than the difference. It penalizes under-predictions more heavily than over-predictions and is robust to outliers, especially when the target variable has a wide range of values.\n",
    "Strengths:\n",
    "    Useful when you care about percentage errors, not just absolute errors (e.g., predicting prices where a $10 error on a $100 item is much worse than on a $1,000,000 item).\n",
    "    Less sensitive to large errors than RMSE.\n",
    "Weaknesses:\n",
    "    Cannot be used if yi​ or y^​i​ are negative.\n",
    "    The interpretation isn't as straightforward as MAE or RMSE. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='dodgerblue'>**Take Home Message**</font>: Each metric does something slightly **different**, and you have to **use it** and **discuss** it in the proper **context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
