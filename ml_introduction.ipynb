{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert block alert-info alert\">\n",
    "\n",
    "# <center> Scientific Programming in Python\n",
    "\n",
    "## <center>Karl N. Kirschner<br>Bonn-Rhein-Sieg University of Applied Sciences<br>Sankt Augustin, Germany\n",
    "\n",
    "# <center> Machine Learning (ML) Overview\n",
    "\n",
    "<center><b>Machine Learning</b> is the study and usage of computer algorithms that <b>learn</b> (i.e., improve automatically) through <b>experience</b> (i.e., data).</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "- **\"Can machines think [in the way that we do]?\"** [1]\n",
    "    - The ML term was <font color='dodgerblue'>**first used in 1959**</font> by Arthur Samuel (an IBM researcher)\n",
    "\n",
    "## Core Mathematical Foundations\n",
    "\n",
    "- The \"work-horse\" of ML is **Statistics** (analyzing and modeling data).\n",
    "\n",
    "- Other core disciplines include:\n",
    "    - **Calculus:** Used for **optimization** (finding the minimum of a cost/loss function) via **derivatives** (e.g., gradient descent).\n",
    "    - **Linear Algebra:** Used for representing data and operations efficiently using **vectors**, **matrices** and **tensors**.\n",
    "\n",
    "\n",
    "Researchers developed different components for many years. Only recently were they collected into code libraries, making the ideas more accessible.\n",
    "\n",
    "\n",
    "## Machine Learning Categories\n",
    "\n",
    "1. <font color='dodgerblue'>**Shallow learning**</font> (e.g., **s**ci**k**it-**learn**; a.k.a., **sklearn**)\n",
    "    - <font color='dodgerblue'>**Predefined features**</font>\n",
    "        - Relies on **Feature Engineering**: Features must be **manually selected, extracted, and pre-processed** by the user (domain expert).\n",
    "    - Models are simpler (e.g., Linear Regression, Decision Trees, Support Vector Machines).\n",
    "\n",
    "<br>\n",
    "\n",
    "2. <font color='dodgerblue'>**Deep learning**</font> (e.g., TensorFlow, PyTorch)\n",
    "    - <font color='dodgerblue'>**Feature learning**</font> (note: can also have feature engineering involved as a pre-processing step)\n",
    "        - Strength is **Feature Learning**: The model **automatically learns the optimal features** (i.e., representations) directly from the raw input data (e.g., pixels, raw text).\n",
    "    - Utilizes **Deep Neural Networks (DNNs)**: complex architectures where many simple algorithmic **\"layers\"** are stacked and trained end-to-end.\n",
    "        - Mostly <font color='dodgerblue'>**combines shallow learning**</font> instances together into the <font color='dodgerblue'>**\"layers\"**</font>\n",
    "\n",
    "\n",
    "## The Three Main ML Task Types\n",
    "\n",
    "Almost all ML problems fall into one of these <font color='dodgerblue'>three categories</font>:\n",
    "1.  <font color='dodgerblue'>**Supervised Learning:**</font> Training data includes the correct **labels** (answers). Used for **Prediction**.\n",
    "    - **Examples**\n",
    "        - **Classification** (e.g., Is this email spam?)\n",
    "        - **Regression** (e.g., What is the price of this house?).\n",
    "\n",
    "<br>\n",
    "\n",
    "2.  <font color='dodgerblue'>**Unsupervised Learning:**</font> Training data has **no labels**. Used for **Discovery**.\n",
    "    - **Examples**\n",
    "        - **Clustering** (grouping similar data points),\n",
    "        - **Dimensionality Reduction**.\n",
    "\n",
    "<br>\n",
    "\n",
    "3.  **Reinforcement Learning (RL):**</font> An **Agent** learns to make sequential **decisions** by interacting with an **Environment** to maximize a <font color='dodgerblue'>**Reward**</font>.\n",
    "    - **Example**\n",
    "        - Training autonomous vehicles, playing games (AlphaGo).\n",
    "<br>\n",
    "\n",
    "**Sources**:\n",
    "1. Turing, Alan M. \"Computing machinery and intelligence.\" Parsing the Turing test. Springer, Dordrecht, 2009. 23-65.\n",
    "\n",
    "**Additional Resources**:\n",
    "1. https://en.wikipedia.org/wiki/Machine_learning\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised vs. Unsupervised Learning\n",
    "\n",
    "Many traditional (Shallow) ML models fall into these two fundamental categories (excluding Reinforcement Learning).\n",
    "\n",
    "---\n",
    "\n",
    "## I. Supervised Learning (Learning with Target/Label Values)\n",
    "\n",
    "In supervised learning, the model is trained on data that includes target output, or **\"label\"** ($\\mathbf{y}$).    \n",
    "- **Goal**: to **learn a mapping function** $f: \\mathbf{X} \\rightarrow \\mathbf{y}$.\n",
    "    - **Training** features ($\\mathbf{X}$): **independent** variables (i.e., input).\n",
    "    - **Target** feature ($\\mathbf{y}$): **dependent** variable(s) (i.e., output).\n",
    "\n",
    "### Key <font color='dodgerblue'>Supervised</font> Learning Tasks\n",
    "\n",
    "| Regression (Predicts a **Continuous Value**) | Classification (Predicts a **Discrete Label**) |\n",
    "| :---: | :---: |\n",
    "| <font color='dodgerblue'>**Linear**</font> | Logistic Regression |\n",
    "| <font color='dodgerblue'>**Polynomial**</font> | <font color='dodgerblue'>**Support Vector Machine (SVM)**</font> |\n",
    "| StepWise | Naive Bayes</font> |\n",
    "| <font color='dodgerblue'>**Ridge**</font> | Nearest Neighbor |\n",
    "| <font color='dodgerblue'>**Lasso**</font> | Decision Tree |\n",
    "| <font color='dodgerblue'>**ElasticNet**</font> | <font color='dodgerblue'>**Random Forest**</font> |\n",
    "\n",
    "## II. Unsupervised Learning (Learning without Target/Label Values)\n",
    "\n",
    "In unsupervised learning, the model is trained on data with **no labels**.\n",
    "- **Goal**: to **discover hidden structure**, **patterns**, or **relationships** within the input data ($\\mathbf{X}$).\n",
    "\n",
    "### Key <font color='dodgerblue'>Unsupervised</font> Learning Tasks\n",
    "\n",
    "| Clustering (Group Data by **Similarity**) | Dimensionality Reduction (Simplify **Data Structure**) |\n",
    "| :---: | :---: |\n",
    "| <font color='dodgerblue'>**K-means**</font> | <font color='dodgerblue'>**Principal Component Analysis (PCA)**</font> |\n",
    "| Mean-Shift | Linear Discriminant Analysis (LDA) |\n",
    "| DBScan | Generalized Discriminant Analysis |\n",
    "| Agglomerative **Hierarchical** | Autoencoder |\n",
    "| Spectral | Non-Negative Matrix Factor Factorization |\n",
    "| Gaussian Mixture Model | UMAP (Uniform Manifold Approximation) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "# Deep Learning\n",
    "\n",
    "Deep Learning is defined by its use of **Deep Neural Networks (DNNs)**, which are models comprised of **hidden layers**.\n",
    "\n",
    "- DNNs are powerful because they can perform **<font color='dodgerblue'>Feature Learning</font>** versus human-controlled **<font color='dodgerblue'>Feature Engineering</font>** (Shallow Learning).\n",
    "    - **Feature Learning**: an algorithm automatically finds the **best data representations**\n",
    "\n",
    "<br>\n",
    "\n",
    "Deep Learning models can be applied to all **three major task types**:\n",
    "\n",
    "1. **Supervised Deep Learning:** Used for complex **Regression** (e.g., predicting sequence data) or **Classification** (e.g., image recognition) using models like\n",
    "    - **Convolutional Neural Networks (CNNs)**, or\n",
    "    - **Recurrent Neural Networks (RNNs)**.\n",
    "    - **Advantage**: trained on <font color='dodgerblue'>labeled data</font>, with specific and <font color='dodgerblue'>clearly defined goals</font>.\n",
    "        - Learn a <font color='dodgerblue'>mapping</font> from <font color='dodgerblue'>input features</font> to known <font color='dodgerblue'>output labels</font>.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Unsupervised Deep Learning:** Used for tasks like advanced **Clustering** or **Dimensionality Reduction** using models like\n",
    "    - **Autoencoders**, and\n",
    "    - **Generative Adversarial Networks (GANs)**.\n",
    "    - **Advantage**: Discover hidden <font color='dodgerblue'>patterns, structures, relationships</font>, or <font color='dodgerblue'>insights</font> within the data itself.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Reinforcement Learning (RL):** The learning agent is often a **Deep Neural Network** that learns the optimal policy through trial-and-error to maximize a **Reward** signal.\n",
    "\n",
    "<br>\n",
    "\n",
    "## The Neural Network Structure\n",
    "\n",
    "A **Neural Network** is a highly flexible function that maps inputs to outputs through a series of interconnected computational units called **neurons** (or **nodes**).\n",
    "\n",
    "\n",
    "1. **Input Layer:** Receives the raw **features** ($\\mathbf{X}$) of the dataset. Each node represents one feature (observable).\n",
    "2. **Encoder**: input $\\rightarrow$ hidden layers (focuses on <font color='dodgerblue'>data reduction</font> and <font color='dodgerblue'>representation</font>)\n",
    "3. **Hidden Layer(s):** Where the **actual learning occurs**. Each node in a hidden layer:\n",
    "    - Calculates a **weighted sum** of its inputs (from the previous layer).\n",
    "    - Applies a non-linear **<font color='dodgerblue'>Activation Function</font>** (e.g., ReLU, Sigmoid) to the sum.\n",
    "    - These layers create a **<font color='dodgerblue'>compressed knowledge representation</font>** of the original input. The more layers, the \"deeper\" the network.\n",
    "4. **Decoder**: hidden layers $\\rightarrow$ <font color='dodgerblue'>output</font>\n",
    "5. **Output Layer:** Returns the final **predicted result** ($\\mathbf{\\hat{Y}}$, where the \"hat\" indicates a prediction).\n",
    "    - The number of nodes will correspond to the task: one node for Regression; one node per class for Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img alt=\"neural network\" width=\"800\" src=\"00_images/31_machine_learning/deep_neural_network.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></center>\n",
    "\n",
    "<center>Image Source: https://www.studytonight.com/post/understanding-deep-learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "# <font color='dodgerblue'>Key Libraries</font> for Machine Learning\n",
    "\n",
    "## I. Foundational Libraries (Data Structure and Numerics)\n",
    "\n",
    "1.  **NumPy** (Numerical Python)\n",
    "2.  **Pandas**\n",
    "\n",
    "<br>\n",
    "\n",
    "## II. Machine Learning Toolkits\n",
    "\n",
    "### A. <font color='dodgerblue'>Shallow Learning</font>\n",
    "\n",
    "1.  **scikit-learn (sklearn):**\n",
    "    - The **standard library** for traditional, high-performance ML algorithms (Regression, Classification, Clustering, etc.).\n",
    "    - Known for its **unified API** (models use the same `.fit()`, `.predict()`, and `.transform()` methods), making it ideal for beginners.\n",
    "\n",
    "<center><img alt=\"Accuracy vs Precision\" width=\"800\" src=\"00_images/31_machine_learning/scikit_learn_ml_map.png\" align=\"center\"></center>\n",
    "\n",
    "<center>Image Source (interactive): https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html</center>\n",
    "\n",
    "### B. <font color='dodgerblue'>Deep Learning</font>\n",
    "\n",
    "1.  **TensorFlow (TF):**\n",
    "    - An open-source library developed by Google.\n",
    "    - Handles complex model building, GPU acceleration, and deployment in production environments.\n",
    "2.  **Keras (integrated into $\\text{tf.keras}$):**\n",
    "    - A high-level API designed to make building and training neural networks **simple and fast**.\n",
    "    - It acts as a user-friendly interface to TensorFlow's core functionality.\n",
    "3.  <font color='dodgerblue'>**PyTorch:**</font>\n",
    "    - An open-source library developed by Facebook's AI Research lab.\n",
    "    - Known for its **Pythonic** feel and **dynamic computation graphs**, making it highly popular for research and **easy prototyping**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "# General Workflow For Model Creation\n",
    "- The scikit-learn Workflow\n",
    "\n",
    "\n",
    "## 1. Understand Your Goal (Define the ML Task)\n",
    "\n",
    "This first step will dictate the type of algorithm you must use.\n",
    "\n",
    "- **Prediction Goal:** What type of data are you predicting?\n",
    "    - **Numerical/Continuous Data** $\\rightarrow$ **Regression** Task (e.g., predicting temperature).\n",
    "    - **Categorical/Discrete Data** $\\rightarrow$ **Classification** Task (e.g., predicting \"cat\" or \"dog\").\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2. Data Preparation and Splitting\n",
    "\n",
    "This step transforms the raw data into the necessary NumPy arrays ($\\mathbf{X}$ and $\\mathbf{y}$) and prepares them for training.\n",
    "\n",
    "- **Data Cleaning:** Collect, clean (e.g., handle missing values), and handle **outliers**.\n",
    "- **Define Features vs. Target:**\n",
    "    - **Training Features ($\\mathbf{X}$):** **Independent Variables** (i.e., input data, predictors).\n",
    "    - **Target Feature ($\\mathbf{y}$):** **Dependent Variable(s)** (i.e., output data, what we want to predict).\n",
    "- **Feature Scaling/Normalization:** Adjust magnitudes of features so they are on a similar scale (e.g., using `StandardScaler` or `MinMaxScaler`).\n",
    "- **Encoding:** Convert any categorical data (e.g., \"Male\", \"Female\")\n",
    "    - Into numerical representations (e.g., 0, 1) using techniques like **One-Hot Encoding**.\n",
    "- **Data Splitting:** Divide the prepared data into distinct sets to prevent **overfitting**.\n",
    "    - Use sklearn's `train_test_split` function.\n",
    "    - Results in four sets:\n",
    "        - $\\mathbf{X}_{\\text{train}}$, $\\mathbf{X}_{\\text{test}}$, and\n",
    "        - $\\mathbf{y}_{\\text{train}}$, $\\mathbf{y}_{\\text{test}}$.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 3. Model Exploration and Training (The $\\text{sklearn}$ API)\n",
    "\n",
    "The key to $\\text{sklearn}$ is the **unified API** - every model object follows the same fundamental pattern.\n",
    "\n",
    "- **Model Selection:** Choose several candidate models (e.g., `LinearRegression`, `RandomForestClassifier`).\n",
    "- **Training:** Fit the model to the training data. This is where the model learns the relationship between $\\mathbf{X}_{\\text{train}}$ and $\\mathbf{y}_{\\text{train}}$.\n",
    "    - **Example code:** model.fit($\\mathbf{X}_{\\text{train}}$, $\\mathbf{y}_{\\text{train}}$)\n",
    "- **Hyperparameter Tuning:** Systematically optimize model settings (e.g., tree depth, learning rate) that are not learned from the data.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 4. Model Evaluation and Determination\n",
    "\n",
    "We test the model's performance on the **unseen** $\\mathbf{X}_{\\text{test}}$ data.\n",
    "\n",
    "- **Prediction:** Generate predictions ($\\mathbf{\\hat{y}}$) using the input test data/features ($\\mathbf{X}_{\\text{test}}$).\n",
    "    - **Example code:** predictions = model.predict($\\mathbf{X}_{\\text{test}}$)\n",
    "- **Evaluation:** Compare the predictions ($\\mathbf{\\hat{y}}$) to the **actual test data** ($\\mathbf{y}_{\\text{test}}$) using appropriate metrics.\n",
    "    - Regression Metrics: Mean Squared Error (MSE), $R^2$.\n",
    "    - Classification Metrics: Accuracy, Precision, Recall, F1 Score.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 5. Apply the Model using New Data\n",
    "\n",
    "The finalized, best-performing model is deployed to make predictions on entirely new data.\n",
    "\n",
    "- **Final Outcome:** Apply the **trained model** to **new data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "# Trained Model Evaluation:\n",
    "\n",
    "## <font color='dodgerblue'>Classification Metrics</font>\n",
    "\n",
    "Evaluating classification models: How well do they correctly assign instances to the correct category?\n",
    "\n",
    "### Core Metrics Based on Counts\n",
    "\n",
    "In all formulas:\n",
    "\n",
    "- **True Positives (TP):** Model <font color='dodgerblue'>**correctly**</font> predicted the **positive** class.\n",
    "- **True Negatives (TN):** Model <font color='dodgerblue'>**correctly**</font> predicted the **negative** class.\n",
    "- **False Positives (FP):** Model <font color='red'>**incorrectly**</font> predicted **positive** (i.e., a false alarm).\n",
    "- **False Negatives (FN):** Model <font color='red'>**incorrectly**</font> predicted **negative** (i.e., a miss).\n",
    "\n",
    "\n",
    "### 1. Accuracy\n",
    "\n",
    "- **Definition:** The proportion of correctly classified instances out of the total instances.\n",
    "- **Formula:**\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "- **Caution:** Can be misleading with **imbalanced datasets**\n",
    "    - e.g., 95% of data is in one class\n",
    "- **sklearn Function:** `accuracy_score`\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Confusion Matrix (a summary table)\n",
    "\n",
    "The foundation for many other metrics. It summarizes performance by showing the counts of correct and incorrect predictions for each class.\n",
    "\n",
    "| | **Actual (Target) Positive** | **Actual (Target) Negative** |\n",
    "| :--- | :---: | :---: |\n",
    "| **Predicted Positive** | True Positive (**TP**) | False Positive (**FP**) (Type I Error) |\n",
    "| **Predicted Negative** | False Negative (**FN**) (Type II Error) | True Negative (**TN**) |\n",
    "\n",
    "---\n",
    "\n",
    "**Example for a \"good\" model** (F1 score $\\approx 0.86$):\n",
    "\n",
    "| | **Actual (target) Positive** | **Actual (target) Negative** |\n",
    "| :--- | :---: | :---: |\n",
    "| **Predicted Positive** | 150 | 30 |\n",
    "| **Predicted Negative** | 20 | 200 |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Recall (a.k.a. Sensitivity or True Positive Rate)\n",
    "\n",
    "- **Definition:** Of all **actual positive** instances, what proportion did the model correctly identify?\n",
    "- **Formula:**\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "- **Use Case:** Minimizing **False Negatives** is critical (e.g., medical screening where missing a sick patient is dangerous, or fraud detection).\n",
    "- **sklearn Function:** `recall_score`\n",
    "\n",
    "---\n",
    "\n",
    "### 4. F1-Score\n",
    "\n",
    "- **Definition:** The **harmonic mean** of precision and recall. It provides a single score that balances both metrics.\n",
    "- **Formula:**\n",
    "$$\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "- **Use Case:** A robust metric, especially when dealing with **class imbalance**.\n",
    "- **sklearn Function:** `f1_score`\n",
    "\n",
    "\n",
    "<!-- ## 2. Advanced Metrics\n",
    "\n",
    "### **ROC Curve and AUC** (Area Under the Curve)\n",
    "\n",
    "- **ROC Curve:** Plots the True Positive Rate (**Recall**) against the False Positive Rate (FPR) at various classification thresholds.\n",
    "- **Formula:** $$\\text{FPR} = \\frac{FP}{FP + TN}$$\n",
    "- **AUC:** The Area Under the ROC Curve.\n",
    "- **Use Case:** Evaluates the model's ability to distinguish between classes across **all possible decision thresholds**. A higher AUC (closer to 1.0) indicates better overall discrimination.\n",
    "\n",
    "### **Log Loss** (Cross-Entropy Loss)\n",
    "\n",
    "- **Definition:** Measures the performance of a classification model where the prediction is a **probability** (rather than a hard class label). It heavily penalizes confident predictions that are wrong.\n",
    "\n",
    "- **Use Case:** Essential for evaluating models (like Logistic Regression or Neural Networks) that output probability scores. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## <font color='dodgerblue'>Regression Metrics</font>\n",
    "\n",
    "These metrics are typically used for **Regression Problems** (i.e., predicting numerical, continuous data - e.g., house price, temperature, or age).\n",
    "\n",
    "In all formulas:\n",
    "- $\\mathbf{n}$ is the number of data points.\n",
    "- $\\mathbf{y}_i$ is the actual (target) value.\n",
    "- $\\mathbf{\\hat{y}}_i$ is the predicted value.\n",
    "- $\\mathbf{\\bar{y}}$ is the mean of the actual values.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Mean Absolute Error (MAE)\n",
    "\n",
    "- **Definition:** The average magnitude of the errors (the average absolute difference between the actual and predicted values).\n",
    "- **Formula:**\n",
    "$$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n",
    "- **Strengths:**\n",
    "    - Easy to understand and interpret.\n",
    "    - **Robust to outliers** (errors are scaled linearly).\n",
    "    - Units are the same as the target variable's units (i.e., intuitive).\n",
    "- **Weaknesses:**\n",
    "    - Not differentiable (less suitable as a primary loss function for gradient-based optimization).\n",
    "- **sklearn Function:** `mean_absolute_error`\n",
    "\n",
    "---\n",
    "## 2. Mean Squared Error (MSE)\n",
    "\n",
    "- **Definition:** The average of the squared errors.\n",
    "- **Formula:**\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "- **Strengths:**\n",
    "    - **Differentiable** (excellent for use as a loss function in algorithms like Linear Regression and Neural Networks).\n",
    "    - **Penalizes larger errors** more heavily due to the squaring operation.\n",
    "- **Weaknesses:**\n",
    "    - Highly **sensitive to outliers**.\n",
    "    - Units are the **square** of the target variable's units (less intuitive).\n",
    "- **sklearn Function:** `mean_squared_error`\n",
    "\n",
    "---\n",
    "## 3. Root Mean Squared Error (RMSE)\n",
    "\n",
    "* **Definition:** The square root of the MSE. This operation brings the error magnitude back to the original units of the target variable.\n",
    "* **Formula:**\n",
    "$$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
    "* **Strengths:**\n",
    "    * Commonly used and widely understood.\n",
    "    * Units are those of the target variable's units (intuitive).\n",
    "    * Penalizes large errors more than MAE.\n",
    "* **Weaknesses:**\n",
    "    * Still sensitive to outliers (though less so than raw MSE).\n",
    "* **sklearn Function:** This is typically calculated by taking the square root of `mean_squared_error`.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 4. R-squared ($\\mathbf{R^2}$) (a.k.a. Coefficient of Determination)\n",
    "\n",
    "- **Definition:** A measure of how well the model explains the variability of the target variable. It compares the model's error to the error of a simple mean-based model.\n",
    "- **Formula:**\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$$\n",
    "- **Interpretation:**\n",
    "    - Ranges from $1.0$ (a perfect fit) to $0$ (the model is no better than simply predicting the mean) and can be negative (worse than the mean).\n",
    "- **Strengths:**\n",
    "    - Provides a **relative measure** of fit, easy to understand as a percentage of explained variance.\n",
    "- **Weaknesses:**\n",
    "    - Can be misleading: it never decreases when adding features, even irrelevant ones (leading to potential overfitting).\n",
    "    - Doesn't tell you the errors' magnitude in the original units.\n",
    "- **sklearn Function:** `r2_score`\n",
    "\n",
    "\n",
    "<!-- Adjusted R-squared\n",
    "\n",
    "Formula: Radj2​=1−n−p−1(1−R2)(n−1)​ where n is the number of data points and p is the number of predictors (features).\n",
    "Interpretation: Adjusted R-squared accounts for the number of predictors in the model. It will only increase if the new features significantly improve the model, penalizing the addition of irrelevant features.\n",
    "Strengths:\n",
    "    More reliable than R2 for comparing models with different numbers of predictors.\n",
    "Weaknesses:\n",
    "    Still a relative measure.\n",
    "\n",
    "Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "Formula: MAPE=n1​∑i=1n​​yi​yi​−y^​i​​​×100%\n",
    "Interpretation: MAPE expresses the error as a percentage of the actual value. This makes it useful for comparing models across different scales.\n",
    "Strengths:\n",
    "    Scale-independent, good for comparing performance across different datasets or models where the target variable has different magnitudes.\n",
    "    Easy to understand as a percentage.\n",
    "Weaknesses:\n",
    "    Undefined if yi​ is zero.\n",
    "    Can heavily penalize errors when yi​ is very small.\n",
    "    Asymmetric (penalizes over-predictions differently from under-predictions).\n",
    "\n",
    "Root Mean Squared Logarithmic Error (RMSLE)\n",
    "\n",
    "Formula: RMSLE=n1​∑i=1n​(log(yi​+1)−log(y^​i​+1))2​\n",
    "Interpretation: RMSLE measures the ratio between actual and predicted values rather than the difference. It penalizes under-predictions more heavily than over-predictions and is robust to outliers, especially when the target variable has a wide range of values.\n",
    "Strengths:\n",
    "    Useful when you care about percentage errors, not just absolute errors (e.g., predicting prices where a $10 error on a $100 item is much worse than on a $1,000,000 item).\n",
    "    Less sensitive to large errors than RMSE.\n",
    "Weaknesses:\n",
    "    Cannot be used if yi​ or y^​i​ are negative.\n",
    "    The interpretation isn't as straightforward as MAE or RMSE. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "# Take Home Messages\n",
    "\n",
    "## Core Concepts & Structure\n",
    "\n",
    "### 1. The Data Structure\n",
    "* All ML models use **Input Features ($\\mathbf{X}$)** and, optionally, a **Target Variable ($\\mathbf{y}$)**, typically stored as NumPy arrays.\n",
    "* **Shallow Learning** (e.g., $\\text{sklearn}$) relies on **Feature Engineering**.\n",
    "* **Deep Learning** (e.g., TensorFlow, PyTorch) relies on **Feature Learning** using **Deep Neural Networks**.\n",
    "\n",
    "### 2. The Three ML Tasks\n",
    "| Task Type | Target ($\\mathbf{y}$) | Goal | Primary Tools |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| <font color='dodgerblue'>**Supervised**</font> | Known (Labeled) | Prediction or Classification | Regression, Classification |\n",
    "| <font color='dodgerblue'>**Unsupervised**</font> | Unknown (Unlabeled) | Discovery of structure | Clustering, Dimensionality Reduction |\n",
    "| **Reinforcement** | No target; uses **Reward** | Sequential decision-making | Deep Q-Learning (often uses DNNs) |\n",
    "\n",
    "---\n",
    "\n",
    "## The General $\\text{sklearn}$ Workflow\n",
    "\n",
    "A five-step process for building and evaluating traditional ML models:\n",
    "\n",
    "1.  **Goal Definition:** Determine if the task is **Regression** (continuous $\\mathbf{y}$) or **Classification** (discrete $\\mathbf{y}$).\n",
    "2.  **Data Preparation:** Clean, encode, scale features, and use `train_test_split` to create **Training** ($\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}}$) and **Test** ($\\mathbf{X}_{\\text{test}}, \\mathbf{y}_{\\text{test}}$) sets.\n",
    "3.  **Model Creation and Training**: create a model object (e.g., `LinearRegression`) and train it (`fit`).\n",
    "4.  **Model Evaluation:** Evaluate the model's performance on unseen data $\\mathbf{X}_{\\text{test}}$ and $\\mathbf{y}_{\\text{test}}$ (e.g., Accuracy, F1-Score, or RMSE).\n",
    "5.  **Model Application:** Apply the model for what it was designed to do (new input data).\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<hr style=\"border:1.5px dashed gray\"></hr>\n",
    "\n",
    "\n",
    "#### Specific Example(s): Autoencoders - generative models (i.e., <font color='dodgerblue'>creates new things</font>)\n",
    "\n",
    "**Autoencoder** neural networks are an unsupervised (i.e., using unlabeled input data) learning model. They **encode an input** (i.e., something that is human-relatable) and **transform it into a different representation** within the latent space, and then **decode** back to something **human-relatable**. This allows for new things to be generated.\n",
    "\n",
    "\n",
    "- https://www.jeremyjordan.me/autoencoders/\n",
    "- <font color='dodgerblue'>Sparse</font> Autoencoder\n",
    "    - **hidden** layers have the **same number of nodes** as the **input** and **output** layers\n",
    "    - loss function includes a penalty for \"activating\" a node within the hidden layer\n",
    "\n",
    "<br>\n",
    "\n",
    "- <font color='dodgerblue'>Denoising</font> Autoencoder\n",
    "    - slightly **corrupt** the **input data** (i.e., add noise) to help make the encoding/decoding more generalizable\n",
    "    - **target data** remains **uncorrupted**\n",
    "    - make the decoding (reconstruction function) insensitive to small changes in the input\n",
    "\n",
    "<br>\n",
    "\n",
    "- <font color='dodgerblue'>Contractive</font> Autoencoder\n",
    "    - make the **encoding** (feature extraction function) **less sensitive** to **small changes** within the **input data**\n",
    "    - learn similar encoding (hidden layer) for different inputs that vary slightly\n",
    "\n",
    "<br>\n",
    "\n",
    "- <font color='dodgerblue'>Variational</font> Autoencoder (VAE)\n",
    "    - https://arxiv.org/abs/1606.05908\n",
    "    - training using **backpropagation** (aka **backward propagation of error**)\n",
    "        - backpropagation - https://www.ibm.com/think/topics/backpropagation\n",
    "        - starting from an **output**, compute the **importance** (measured as a gradient) that each neural network **parameter** has on the final model's **error** (predicted values) (i.e., loss function)\n",
    "    - encoding is **regularized** (adding a penalty term to the model's loss function during the learning process) to ensure that the latent space has good properties (and thus, allowing us to have generative models to be created)\n",
    "        - regularization - https://en.wikipedia.org/wiki/Regularization_(mathematics)\n",
    "\n",
    "\n",
    "<hr style=\"border:1.5px dashed gray\"></hr>\n",
    "<!-- - Generative Adversarial Networks (GANs)\n",
    "    - two networks oppose each other (a generator and a discriminator), for which both iteratively improve -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
