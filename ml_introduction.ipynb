{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert block alert-info alert\">\n",
    "\n",
    "# <center> Scientific Programming in Python\n",
    "\n",
    "## <center>Karl N. Kirschner<br>Bonn-Rhein-Sieg University of Applied Sciences<br>Sankt Augustin, Germany\n",
    "\n",
    "# <center> Machine Learning (ML) Overview\n",
    "\n",
    "**\"Can machines think [in the way that we do]?\"** [1]\n",
    "\n",
    "\n",
    "- The ML term was <font color='dodgerblue'>**first used in 1959**</font> by Arthur Samuel (an IBM researcher)\n",
    "\n",
    "<center>**Machine Learning (ML)** is the study of computer algorithms that improve automatically through **experience** (data).</center>\n",
    "\n",
    "\n",
    "ML is used to make **predictions** or **decisions** without being explicitly programmed to perform the task.\n",
    "\n",
    "## Core Mathematical Foundations\n",
    "\n",
    "The \"work-horse\" of ML is **Statistics** (analyzing and modeling data).\n",
    "\n",
    "Other core disciplines include:\n",
    "- **Calculus:** Used for **optimization** (finding the minimum of a cost/loss function) via **derivatives** (e.g., gradient descent).\n",
    "- **Linear Algebra:** Used for representing data and operations efficiently using **vectors**, **matrices**, and **tensors** (data structures in $\\mathbb{R}^n$).\n",
    "\n",
    "\n",
    "Different components were developed by researchers for many years. Only recently were they collected into libraries that make the ideas more accessible.\n",
    "\n",
    "\n",
    "## Machine Learning Categories\n",
    "\n",
    "1. <font color='dodgerblue'>**Shallow learning** (e.g. **s**ci**k**it-**learn** - a.k.a. **sklearn**)\n",
    "    - <font color='dodgerblue'>**predefined features**</font>\n",
    "        - Relies on **Feature Engineering**: Features must be **manually selected, extracted, and pre-processed** by the user (domain expert).\n",
    "    - Models are typically simpler (e.g., Linear Regression, Decision Trees, Support Vector Machines).\n",
    "\n",
    "\n",
    "2. Deep learning (e.g. TensorFlow, PyTorch)\n",
    "    - <font color='dodgerblue'>**feature learning**</font>\n",
    "        - Relies on **Feature Learning**: The model **automatically learns the optimal features** (representations) directly from the raw input data (e.g., pixels, raw text).\n",
    "    - Utilizes **Deep Neural Networks (DNNs)**: complex architectures where many simple algorithmic **\"layers\"** are stacked and trained end-to-end.\n",
    "        - Mostly <font color='dodgerblue'>**combines shallow learning**</font> instances together into <font color='dodgerblue'>**\"layers\"**</font>\n",
    "\n",
    "\n",
    "## The Three Main ML Task Types\n",
    "\n",
    "Almost all ML problems fall into one of these three categories:\n",
    "1.  **Supervised Learning:** Training data includes the correct **labels** (answers). Used for **Prediction**.\n",
    "    - **Examples**\n",
    "        - **Classification** (e.g., Is this email spam?)\n",
    "        - **Regression** (e.g., What is the price of this house?).\n",
    "2.  **Unsupervised Learning:** Training data has **no labels**. Used for **Discovery**.\n",
    "    - **Examples**\n",
    "        - **Clustering** (grouping similar data points),\n",
    "        - **Dimensionality Reduction**.\n",
    "3.  **Reinforcement Learning (RL):** An **Agent** learns to make sequential **decisions** by interacting with an **Environment** to maximize a **Reward**.\n",
    "    - **Example**\n",
    "        - Training autonomous vehicles, playing games (AlphaGo).\n",
    "<br>\n",
    "\n",
    "**Sources**:\n",
    "1. Turing, Alan M. \"Computing machinery and intelligence.\" Parsing the Turing test. Springer, Dordrecht, 2009. 23-65.\n",
    "\n",
    "**Additional Resources**:\n",
    "1. https://en.wikipedia.org/wiki/Machine_learning\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised vs. Unsupervised Learning\n",
    "\n",
    "All traditional (Shallow) ML models solve problems that fall into one of these two fundamental categories (excluding Reinforcement Learning).\n",
    "\n",
    "---\n",
    "\n",
    "## I. Supervised Learning (Learning with Target/Label Values)\n",
    "\n",
    "In supervised learning, the model is trained on data that includes target output, or **\"label\"** ($\\mathbf{y}$).\n",
    "- The goal is to **learn a mapping function** $f: \\mathbf{X} \\rightarrow \\mathbf{y}$.\n",
    "\n",
    "### Key Supervised Learning Tasks\n",
    "\n",
    "## Catagories\n",
    "\n",
    "| Regression (Predicts a **Continuous Value**) | Classification (Predicts a **Discrete Label**) |\n",
    "| :---: | :---: |\n",
    "| <font color='dodgerblue'>**Linear**</font> | Logistic Regression |\n",
    "| <font color='dodgerblue'>**Polynomial**</font> | <font color='dodgerblue'>**Support Vector Machine (SVM)**</font> |\n",
    "| StepWise | Naive Bayes</font> |\n",
    "| <font color='dodgerblue'>**Ridge**</font> | Nearest Neighbor |\n",
    "| <font color='dodgerblue'>**Lasso**</font> | Decision Tree |\n",
    "| <font color='dodgerblue'>**ElasticNet**</font> | <font color='dodgerblue'>**Random Forest**</font> |\n",
    "\n",
    "## II. Unsupervised Learning (Learning without Target/Label Values)\n",
    "\n",
    "In unsupervised learning, the model is trained on data with **no labels**.\n",
    "- The goal is to **discover hidden structure**, **patterns**, or **relationships** within the input data ($\\mathbf{X}$).\n",
    "\n",
    "### Key Unsupervised Learning Tasks\n",
    "\n",
    "| Clustering (Group Data by **Similarity**) | Dimensionality Reduction (Simplify **Data Structure**) |\n",
    "| :---: | :---: |\n",
    "| <font color='dodgerblue'>**K-means**</font> | <font color='dodgerblue'>**Principal Component Analysis (PCA)**</font> |\n",
    "| Mean-Shift | Linear Discriminant Analysis (LDA) |\n",
    "| DBScan | Generalized Discriminant Analysis |\n",
    "| Agglomerative **Hierarchical** | Autoencoder |\n",
    "| Spectral | Non-Negative Matrix Factor Factorization |\n",
    "| Gaussian Mixture Model | UMAP (Uniform Manifold Approximation) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"Accuracy vs Precision\" width=\"800\" src=\"00_images/31_machine_learning/scikit_learn_ml_map.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
    "\n",
    "Image Source (interactive): https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "# Deep Learning\n",
    "- **Supervised Learning**: trained on <font color='dodgerblue'>labeled data</font>, with specified goals (clearly defined <font color='dodgerblue'>prediction/classification</font> goals)\n",
    "    - learn a <font color='dodgerblue'>mapping</font> from <font color='dodgerblue'>input features</font> to known <font color='dodgerblue'>output labels</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Unsupervised Learning**: trained on <font color='dodgerblue'>unlabeled data</font>, with specified goals (discovering and insights)\n",
    "    - discover hidden <font color='dodgerblue'>patterns, structures, relationships</font>, or <font color='dodgerblue'>insights</font> within the data itself.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Reinforcement Learning**: learning through interaction that includes maximizing rewards (no labeled data, no specified goal) \n",
    "\n",
    "<br>\n",
    "\n",
    "#### Neural network\n",
    "- **Input Layer**: <font color='dodgerblue'>features (observables)</font> should have some degree of correlation (i.e., structure; nonlinear relationships)\n",
    "- Encoder: input $\\rightarrow$ hidden layers (<font color='dodgerblue'>data reduction</font>)\n",
    "- **Hidden Layer**: a <font color='dodgerblue'>compressed knowledge representation</font> of the original input\n",
    "- Decoder: hidden layers $\\rightarrow$ <font color='dodgerblue'>output</font>\n",
    "- **Output Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Deep Learning is defined by its use of **Deep Neural Networks (DNNs)**, which are models comprised of multiple **hidden layers**. DNNs are powerful because they perform **Feature Learning** — automatically finding the optimal data representations—in contrast to the manual **Feature Engineering** required for Shallow Learning.\n",
    "\n",
    "Deep Learning models can be applied to all three major task types:\n",
    "\n",
    "* **Supervised Deep Learning:** Used for complex Regression (e.g., predicting sequence data) or Classification (e.g., image recognition) using **Convolutional Neural Networks (CNNs)** or **Recurrent Neural Networks (RNNs)**.\n",
    "* **Unsupervised Deep Learning:** Used for tasks like advanced Clustering or Dimensionality Reduction using models like **Autoencoders** and **Generative Adversarial Networks (GANs)**.\n",
    "* **Reinforcement Learning (RL):** The learning agent is often a **Deep Neural Network** that learns the optimal policy through trial-and-error to maximize a **Reward** signal.\n",
    "\n",
    "---\n",
    "\n",
    "## The Neural Network Structure\n",
    "\n",
    "A **Neural Network** is a highly flexible function that maps inputs to outputs through a series of interconnected computational units called **neurons** (or **nodes**).\n",
    "\n",
    "\n",
    "\n",
    "[Image of a basic three-layer neural network diagram showing the input layer, two hidden layers, and the output layer, with arrows illustrating the flow of data]\n",
    "\n",
    "\n",
    "* **Input Layer:** Receives the raw **features** ($\\mathbf{X}$) of the dataset. Each node represents one feature (observable).\n",
    "* **Hidden Layer(s):** Where the actual computation and learning occur. Each node in a hidden layer:\n",
    "    * Calculates a **weighted sum** of its inputs (from the previous layer).\n",
    "    * Applies a non-linear **Activation Function** (e.g., ReLU, Sigmoid) to the sum.\n",
    "    * These layers create a **compressed knowledge representation** of the original input. The more layers, the \"deeper\" the network.\n",
    "* **Output Layer:** Returns the final result.\n",
    "    * The number of nodes corresponds to the task: one node for Regression, or typically one node per class for Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"neural network\" width=\"800\" src=\"00_images/31_machine_learning/deep_neural_network.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
    "\n",
    "Image Source: https://www.studytonight.com/post/understanding-deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Type of Deep Learning Neural Networks (NN):\n",
    "1. Convolutional NN\n",
    "2. Recurrent NN (RNNs)\n",
    "    - Long Short-Term Memory (LSTM) Networks (a sepecial type of RNN)\n",
    "3. Generative Adversarial Networks (GANs)\n",
    "4. Reinforcement Learning (RL) with Deep Learning (Deep RL)\n",
    "\n",
    "\n",
    "## Python Libraries\n",
    "1. <font color='dodgerblue'>**TensorFlow**</font>\n",
    "    - open-source library\n",
    "3. <font color='dodgerblue'>**Keras**</font> (integrated into TensorFlow: `tf.keras`)\n",
    "    - designed for fast experimentation\n",
    "4. <font color='dodgerblue'>**PyTorch**</font>\n",
    "   - open-source library\n",
    "   - easy to use (Pythonic)\n",
    "   - easy prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<hr style=\"border:1.5px dashed gray\"></hr>\n",
    "\n",
    "\n",
    "#### Specific Example(s): Autoencoders - generative models (i.e., <font color='dodgerblue'>creates new things</font>)\n",
    "\n",
    "**Autoencoder** neural networks are an unsupervised (i.e., using unlabeled input data) learning model. They **encode an input** (i.e., something that is human-relatable) and **transform it into a different representation** within the latent space, and then **decode** back to something **human-relatable**. This allows for new things to be generated.\n",
    "\n",
    "\n",
    "- https://www.jeremyjordan.me/autoencoders/\n",
    "- <font color='dodgerblue'>Sparse</font> Autoencoder\n",
    "    - **hidden** layers have the **same number of nodes** as the **input** and **output** layers\n",
    "    - loss function includes a penalty for \"activating\" a node within the hidden layer\n",
    "\n",
    "<br>\n",
    "\n",
    "- <font color='dodgerblue'>Denoising</font> Autoencoder\n",
    "    - slightly **corrupt** the **input data** (i.e., add noise) to help make the encoding/decoding more generalizable\n",
    "    - **target data** remains **uncorrupted**\n",
    "    - make the decoding (reconstruction function) insensitive to small changes in the input\n",
    "\n",
    "<br>\n",
    "\n",
    "- <font color='dodgerblue'>Contractive</font> Autoencoder\n",
    "    - make the **encoding** (feature extraction function) **less sensitive** to **small changes** within the **input data**\n",
    "    - learn similar encoding (hidden layer) for different inputs that vary slightly\n",
    "\n",
    "<br>\n",
    "\n",
    "- <font color='dodgerblue'>Variational</font> Autoencoder (VAE)\n",
    "    - https://arxiv.org/abs/1606.05908\n",
    "    - training using **backpropagation** (aka **backward propagation of error**)\n",
    "        - backpropagation - https://www.ibm.com/think/topics/backpropagation\n",
    "        - starting from an **output**, compute the **importance** (measured as a gradient) that each neural network **parameter** has on the final model's **error** (predicted values) (i.e., loss function)\n",
    "    - encoding is **regularized** (adding a penalty term to the model's loss function during the learning process) to ensure that the latent space has good properties (and thus, allowing us to have generative models to be created)\n",
    "        - regularization - https://en.wikipedia.org/wiki/Regularization_(mathematics)\n",
    "\n",
    "\n",
    "<hr style=\"border:1.5px dashed gray\"></hr>\n",
    "<!-- - Generative Adversarial Networks (GANs)\n",
    "    - two networks oppose each other (a generator and a discriminator), for which both iteratively improve -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Workflow For Model Creation (and Prediction)\n",
    "\n",
    "1. **Understand your goal** - do you want to predict\n",
    "    - Categorical data (i.e., noncontinuous data)\n",
    "    - Numerical data (i.e., continuous data)\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Data**\n",
    "    - Collect, clean (e.g. drop rows with missing data) and adjust magnitudes (e.g., **normalize**)\n",
    "    - Determine training features versus target features (i.e. what you want to predict)\n",
    "        - Training features (**independent variables** -- x-axis data)\n",
    "        - Target features (**dependent variable(s)** -- y-axis data)\n",
    "    - Encode any categorical data present (i.e., provide numerical values)\n",
    "    - Data splitting (**training and test sets**)\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Model Exploration**\n",
    "    - Choose several models to try \n",
    "    - Default parameters (\"hyperparameters\")\n",
    "    - Identify good candidates (see #4 below)\n",
    "    - Optimize hyperparameters\n",
    "\n",
    "<br>\n",
    "\n",
    "4. **Model Evaluation and Determination**\n",
    "    - Choose and compute different metrics\n",
    "\n",
    "<br>\n",
    "\n",
    "5. **Apply Model using New Data**\n",
    "    - I.e., make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained Model Evaluation\n",
    "\n",
    "### Classification-Data Metrics:\n",
    "\n",
    "Evaluating classification models: How well they correctly assign instances?\n",
    "\n",
    "1. <font color='dodgerblue'>**Accuracy**</font>: The proportion of correctly classified instances out of the total instances.\n",
    "\n",
    "    **Caution**: Can be misleading with imbalanced datasets (e.g., 95% of data is in one class).\n",
    "\n",
    "<br>\n",
    "\n",
    "2. <font color='dodgerblue'>**Confusion Matrix**</font> (foundation for many other metrics): A summarizing performance matrix that shows how many:\n",
    "    - True Positives (**TP**): A model's outcome that **correctly** predicted the **positive** (e.g., \"yes\"; \"with disease\") class\n",
    "    - True Negatives (**TN**): A model's outcome that **correctly** predicted the **negative** (e.g., \"no\"; \"without disease\") class\n",
    "    - False Positives (**FP**) (a.k.a. Type I error): A model's outcome that **incorrectly** predicted the **positive** class\n",
    "    - False Negatives (**FN**) (a.k.a. Type II error): A model's outcome that **incorrectly** predicted the **negative** class\n",
    "\n",
    "<br>\n",
    "\n",
    "|               | **Actual (target) Positive** | **Actual (target) Negative** |\n",
    "| :------------ | :------------------ | :------------------ |\n",
    "| **Predicted Positive** | # True Positive (TP)  | # False Positive (FP) |\n",
    "| **Predicted Negative** | # False Negative (FN) | # True Negative (TN)  |\n",
    "\n",
    "An example for a \"good\" model (i.e., F1 score = 0.86):\n",
    "\n",
    "|               | **Actual (target) Positive** | **Actual (target) Negative** |\n",
    "| :------------ | :------------------ | :------------------ |\n",
    "| **Predicted Positive** | 150  | 30 |\n",
    "| **Predicted Negative** | 20 |200  |\n",
    "\n",
    "<br>\n",
    "\n",
    "3. <font color='dodgerblue'>**Precision**</font>: Of all instances predicted as positive, what proportion were actually positive?\n",
    "    - Formula: $\\Large\\frac{TP}{TP+FP}$\n",
    "    - Use: Minimizing false positives is critical (e.g., medical diagnosis).\n",
    "\n",
    "<br>\n",
    "\n",
    "4. <font color='dodgerblue'>**Recall**</font> (a.k.a. Sensitivity or True Positive Rate): Of all actual positive instances, what proportion did the model correctly identify?\n",
    "    - Formula: $\\Large\\frac{TP}{TP+FN}$\n",
    "    - Use: Minimizing false negatives is critical (e.g., fraud detection where missing actual fraud is very costly, or disease screening where missing a sick patient is dangerous).\n",
    "\n",
    "<br>\n",
    "\n",
    "5. <font color='dodgerblue'>**F1-Score**</font>: The harmonic mean of precision and recall. (It balances the precision and recall metrics.)\n",
    "    - Formula: $\\Large\\frac{2∗(Precision∗Recall)}{Precision + Recall}$\n",
    "    - Use: A single metric that balances precision and recall, especially where there is a class imbalance.\n",
    "\n",
    "<!-- \n",
    "<br>\n",
    "\n",
    "6. <font color='dodgerblue'>**ROC Curve** and AUC</font> (Area Under the Receiver Operating Characteristic Curve):\n",
    "    - ROC Curve: Plots the True Positive Rate (i.e., Recall) against the False Positive Rate at various classification thresholds.\n",
    "    - AUC: Overall classifier performance,irrespective of the classification threshold. (A higher AUC indicates better discrimination between classes.)\n",
    "    - Use: evaluate the model's ability to distinguish between classes across all possible thresholds (good for imbalanced datasets).\n",
    "\n",
    "<br>\n",
    "\n",
    "7. <font color='dodgerblue'>**Log Loss**</font> (Cross-Entropy Loss): Measures the performance of a classification model where the prediction is a probability. It penalizes confident wrong predictions heavily.\n",
    "    - Use: You need to evaluate probabilistic outputs from models like logistic regression or neural networks. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical-Data Metrics\n",
    "Typically are regression problems (e.g., house price, temperature, age).\n",
    "\n",
    "1. <font color='dodgerblue'>**Mean Absolute Error (MAE)**</font>:\n",
    "    - Formula: $\\Large MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $, where $\\Large\\hat{y}_i$ is the ideal/target value\n",
    "    - Interpretation: It tells you the average magnitude of the errors.\n",
    "    - Strengths:\n",
    "        - Easy to understand and interpret.\n",
    "        - Robust to outliers.\n",
    "        - Units are those of the target variable's units (i.e., intuitive)\n",
    "    - Weaknesses:\n",
    "        - Not differentiable (less suitable as a loss function in optimization algorithms).\n",
    "\n",
    "<br>\n",
    "\n",
    "2. <font color='dodgerblue'>**Mean Squared Error (MSE)**</font>:\n",
    "    - Formula: $\\Large MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
    "    - Interpretation: Penalizes larger errors more heavily than smaller ones.\n",
    "    - Strengths:\n",
    "        - Differentiable (e.g., usable in Linear Regression, Neural Networks)\n",
    "        - Penalizes large errors.\n",
    "    - Weaknesses:\n",
    "        - Units are the square of the target variable's units.\n",
    "        - Highly sensitive to outliers.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. <font color='dodgerblue'>**Root Mean Squared Error (RMSE)**</font>:\n",
    "    - Formula: $\\Large RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 } $\n",
    "    - Interpretation: It brings the error back to the original units of the target variable.\n",
    "    - Strengths:\n",
    "        - Commonly used and widely understood.\n",
    "        - Penalizes large errors more than MAE.\n",
    "        - Units are those of the target variable's units.\n",
    "    - Weaknesses:\n",
    "        - Sensitive to outliers (less so than raw MSE).\n",
    "\n",
    "<br>\n",
    "\n",
    "4. <font color='dodgerblue'>**R-squared (R2)**</font> (a.k.a. Coefficient of Determination):\n",
    "    - Formula: $\\Large R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $\n",
    "    - Interpretation: It measures how well the model explains the variability of the target variable.\n",
    "    - Strengths:\n",
    "        - Provides a relative measure of fit, ranging from $-\\infty$ (very poor) to $0$ (poor) to $1$ (a perfect fit)\n",
    "        - Easy to understand as a percentage\n",
    "    - Weaknesses:\n",
    "        - Can increase simply by adding more features (this can lead to overfitting).\n",
    "        - Doesn't tell you the errors' magnitude in the original units.\n",
    "\n",
    "<!-- Adjusted R-squared\n",
    "\n",
    "Formula: Radj2​=1−n−p−1(1−R2)(n−1)​ where n is the number of data points and p is the number of predictors (features).\n",
    "Interpretation: Adjusted R-squared accounts for the number of predictors in the model. It will only increase if the new features significantly improve the model, penalizing the addition of irrelevant features.\n",
    "Strengths:\n",
    "    More reliable than R2 for comparing models with different numbers of predictors.\n",
    "Weaknesses:\n",
    "    Still a relative measure.\n",
    "\n",
    "Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "Formula: MAPE=n1​∑i=1n​​yi​yi​−y^​i​​​×100%\n",
    "Interpretation: MAPE expresses the error as a percentage of the actual value. This makes it useful for comparing models across different scales.\n",
    "Strengths:\n",
    "    Scale-independent, good for comparing performance across different datasets or models where the target variable has different magnitudes.\n",
    "    Easy to understand as a percentage.\n",
    "Weaknesses:\n",
    "    Undefined if yi​ is zero.\n",
    "    Can heavily penalize errors when yi​ is very small.\n",
    "    Asymmetric (penalizes over-predictions differently from under-predictions).\n",
    "\n",
    "Root Mean Squared Logarithmic Error (RMSLE)\n",
    "\n",
    "Formula: RMSLE=n1​∑i=1n​(log(yi​+1)−log(y^​i​+1))2​\n",
    "Interpretation: RMSLE measures the ratio between actual and predicted values rather than the difference. It penalizes under-predictions more heavily than over-predictions and is robust to outliers, especially when the target variable has a wide range of values.\n",
    "Strengths:\n",
    "    Useful when you care about percentage errors, not just absolute errors (e.g., predicting prices where a $10 error on a $100 item is much worse than on a $1,000,000 item).\n",
    "    Less sensitive to large errors than RMSE.\n",
    "Weaknesses:\n",
    "    Cannot be used if yi​ or y^​i​ are negative.\n",
    "    The interpretation isn't as straightforward as MAE or RMSE. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='dodgerblue'>**Take Home Message**</font>: Each metric does something slightly **different**, and you have to **use it** and **discuss** it in the proper **context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
