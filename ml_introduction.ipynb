{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (ML; a.k.a. self-teaching computers)\n",
    "\n",
    "**\"Can machines think [in the way that we do]?\"** [1]\n",
    "\n",
    "\n",
    "- The ML term was first used in 1959 by Arthur Samuel (an IMB researcher)\n",
    "\n",
    "\n",
    "- Mathematical Foundation\n",
    "    - Statistics (the \"work-horse\" of ML)\n",
    "    - Calculus (derivatives; optimizations)\n",
    "    - Algerbra (vectors, matrix, tensors)\n",
    "\n",
    "\n",
    "- Different components have been developed by researchers for a long time, but recently collected together into libraries that make the ideas more accessible.\n",
    "\n",
    "## Machine Learning Catagories\n",
    "\n",
    "1. Shallow learning (e.g. scikit-learn - a.k.a. sklearn)\n",
    "    - predefined features\n",
    "\n",
    "1. Deep learning (e.g. tensorflow, pytorch)\n",
    "    - feature learning\n",
    "    - mostly combines shallow learning together into \"layers\"\n",
    "\n",
    "\n",
    "**Sources**:\n",
    "1. Turing, Alan M. \"Computing machinery and intelligence.\" Parsing the turing test. Springer, Dordrecht, 2009. 23-65.\n",
    "\n",
    "**Additional Resources**:\n",
    "1. https://en.wikipedia.org/wiki/Machine_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Shallow Learning\n",
    "\n",
    "## Catagories\n",
    "\n",
    "| Regression | Classification | Clustering | Dimension Reduction|\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| Linear | Logistic Regression | K-means | Principle Component Analysis |\n",
    "| Polynomial | Support Vector Machine | Mean-Shift | Linear Discriminant Analysis |\n",
    "| StepWise | Naive Bayes | DBScan | Gernalized Discriminant Analysis |\n",
    "| Ridge | Nearest Neighbor | Agglomerative Hierachcial | Autoencoder |\n",
    "| Lasso | Decision Tree | Spectral Clustering | Non-Negative Matrix Factorization |\n",
    "| ElasticNet | Random Forest | Gaussian Mixture | UMAP |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs. Unsupervised Learning\n",
    "\n",
    "1. **Supervised** - the **target information is known** in the data set, and we **train to reproduce** that information\n",
    "    - regression\n",
    "    - classification\n",
    "\n",
    "1. **Unsupervised** - the **target information is unknown**, with the goal to \n",
    "    - cluster the data's similarity (clustering)\n",
    "    - determine the distribution of data (density estimation)\n",
    "    - reduce the dimensions for purpose of visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"Accuracy vs Precision\" width=\"800\" src=\"00_images/31_machine_learning/scikit_learn_ml_map.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
    "\n",
    "Source (interactive): https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Deep Learning - Unspervised Learning\n",
    "\n",
    "\n",
    "#### Neural network\n",
    "- Input Layer - feature should have some degree of correlation (i.e. structure; nonlinear relationships)\n",
    "- Hidden Layer (\"bottleneck\") - a compressed knowledge representation of the original input\n",
    "- Output Layer\n",
    "- Encoder - input-> hidden layers (data reduction)\n",
    "- Decoder - hidden layers -> output\n",
    "\n",
    "\n",
    "<!-- 2. Generative models\n",
    "    - High dimensional data\n",
    "    - Usually interested in generating data that is like the input data (but not the same) -->\n",
    "\n",
    "#### Autoencoders - generative models (i.e. creates new things)\n",
    "- https://www.jeremyjordan.me/autoencoders/\n",
    "- Sparse Autoencoder\n",
    "    - hidden layers have the same number of nodes as the input and output layers\n",
    "    - loss function include a penalty for \"activating\" a node within the hidden layer\n",
    "\n",
    "- Denosing Autoencoder\n",
    "    - slighly corrupt the input data (i.e. add noise) to help make the encoding/decoding more generalizable\n",
    "    - target data remins uncorrupted\n",
    "    - make the decoding (reconstruction function) insensitive to small changes in the input\n",
    "- Contractive Autoencoder\n",
    "    - make the encoding (feature extraction function) less sensitive to small changes within the input data\n",
    "    - learn similar encoding (hidden layer) for different inputs that vary slightly\n",
    "\n",
    "- Variational Autoencoder (VAE)\n",
    "    - https://arxiv.org/abs/1606.05908\n",
    "    - training using backpropogation\n",
    "    - encoding is regularized (during the learning process) to ensure that the latent space has good properties (and thus, allowing us to have generative models to be created)\n",
    "\n",
    "- Generative Adversarial Networks (GANs)\n",
    "    - two networks oppose each other (a generator and a discriminator), for which both iteratively improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
