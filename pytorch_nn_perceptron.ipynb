{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e3d087-efd5-423a-a353-c94db02dedc7",
   "metadata": {},
   "source": [
    "<div class=\"alert block alert-info alert\">\n",
    "\n",
    "# <center> Scientific Programming in Python\n",
    "\n",
    "## <center>Karl N. Kirschner<br>Bonn-Rhein-Sieg University of Applied Sciences<br>Sankt Augustin, Germany\n",
    "\n",
    "# <center> PyTorch: Simple Neural Network Example\n",
    "\n",
    "## <center>  with a Perceptron\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c15d01d",
   "metadata": {},
   "source": [
    "This lecture will parallel the perceptron example written using NumPy, allowing you to compare the approaches directly.\n",
    "\n",
    "<br>\n",
    "<center><img src=\"00_images/31_machine_learning/nn_perceptron_example_nodes.png\" alt=\"nn_percepton\" style=\"width: 1000px;\"/></center>\n",
    "\n",
    "<center><img src=\"00_images/31_machine_learning/nn_perceptron_example.png\" alt=\"nn_percepton\" style=\"width: 1000px;\"/></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Terminalogy for describing the shape and capability of a neural network; for example:\n",
    "- Size: number of nodes in the model\n",
    "- Width: number of nodes in a specific layer\n",
    "- Depth: number of layers in a neural network\n",
    "- Capacity: sophistication - a higher capacity means a network can model more relationships between more variables\n",
    "- Architecture: specific arrangement of the layers and nodes within the network.\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "In this lecture, the neural network (NN) will be written in two ways:\n",
    "1. <font color='dodgerblue'>Basic</font> - to explicitly show all of the steps in a neural network training\n",
    "2. <font color='dodgerblue'>Advance</font> - to show the typical way that PyTorch is implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c0bf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b75bd67",
   "metadata": {},
   "source": [
    "Create a helper function that allows us to investigate the different arrays that are used below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb9c19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_array_specs(in_arrays: dict):\n",
    "    ''' Helper function for nicely printing NumPy and\n",
    "        PyTorch arrays.\n",
    "\n",
    "        Print: shape, data type and values.\n",
    "    '''\n",
    "    for key, value in in_arrays.items():\n",
    "        print(f'{key}:\\n{value.shape}, {value.dtype}')\n",
    "        print(f'{value}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cbd5f2",
   "metadata": {},
   "source": [
    "## Basic Example\n",
    "\n",
    "#### Define the toy data (input values, target values and initial weights):\n",
    "\n",
    "##### A reminder from the NumPy lecture\n",
    "\n",
    "A random **seed** will be **explicitly set**, allowing for **reproducible results** (i.e., for teaching purposes). The first epoch data generated below should correspond to the numeric values given in the figure above.\n",
    "\n",
    "The object naming will also be done to parallel the figure above.\n",
    "\n",
    "Random Number Generator in NumPy:\n",
    "- `np.random.default_rng`: https://numpy.org/doc/stable/reference/random/generator.html\n",
    "- `numpy.random.Generator.normal`: https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "Side Note Normalization versus \n",
    "**Important Note**: Normally with <font color='dodgerblue'>real-world data</font>, one often should <font color='dodgerblue'>normalize</font> (e.g., **transpose** the date to a range [0, 1]) or <font color='dodgerblue'>scale</font> the <font color='dodgerblue'>input data</font>. This helps the mathematics when different input features have **large magnitude differences** (e.g., 1.5 and 2.5e6).\n",
    "- https://en.wikipedia.org/wiki/Normalization_(statistics)\n",
    "- `sklearn.preprocessing.normalize`: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html\n",
    "\n",
    "The goal is to ensure that no single feature dominates the others due to its magnitude.\n",
    "\n",
    "**Normalizing** transforms the data to a standard scale, typically between 0 and 1.\n",
    "- adjust each feature's values based on their minimum and maximum values.\n",
    "- mathematically, there are multiple approaches for this\n",
    "    - Minimum-Maximum (a.k.a. rescaling): $x' = \\frac{x − x_{min}}{x_{max} − x_{min}}$\n",
    "    - Absolute Maximum: $x' = \\frac{x}{∣x_{max}∣}$\n",
    "    - Mean: $x' = \\frac{x − \\bar{x}}{x_{max} − x_{min}}$ centers the data about the mean, with a range from [-1, 1].\n",
    "    - Z-score (a.k.a Standardization): $x' = \\frac{x − \\bar{x}}{\\sigma}$ ($\\sigma$ is the standard deviation) good for when original data follows a normal distribution \n",
    "    - Log: used to reduce the effects of extreme values\n",
    "\n",
    "\n",
    "- https://www.geeksforgeeks.org/normalization-and-scaling/\n",
    "- https://en.wikipedia.org/wiki/Feature_scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dcc959-1453-43c4-b54e-e2edd6b3a01a",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "In this example, we don't need to worry about normalizing since we generate the toy data that has the same magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b00bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=12345)\n",
    "\n",
    "input_X1_np = rng.normal(size=(2, 10))\n",
    "target_Y2_np = rng.normal(size=(2, 1))\n",
    "\n",
    "weight_W1_np = rng.normal(size=(10, 3))\n",
    "weight_W2_np = rng.normal(size=(3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0682b4",
   "metadata": {},
   "source": [
    "Examine the different NumPy arrays:\n",
    "- shapes (important for matrix multiplication)\n",
    "- data types (need to be same types)\n",
    "- values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a136911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_X1:\n",
      "(2, 10), float64\n",
      "[[-1.42382504  1.26372846 -0.87066174 -0.25917323 -0.07534331 -0.74088465\n",
      "  -1.3677927   0.6488928   0.36105811 -1.95286306]\n",
      " [ 2.34740965  0.96849691 -0.75938718  0.90219827 -0.46695317 -0.06068952\n",
      "   0.78884434 -1.25666813  0.57585751  1.39897899]]\n",
      "\n",
      "target_Y2:\n",
      "(2, 1), float64\n",
      "[[ 1.32229806]\n",
      " [-0.29969852]]\n",
      "\n",
      "weight_W1:\n",
      "(10, 3), float64\n",
      "[[ 0.90291934 -1.62158273 -0.15818926]\n",
      " [ 0.44948393 -1.34360107 -0.08168759]\n",
      " [ 1.72473993  2.61815943  0.77736134]\n",
      " [ 0.8286332  -0.95898831 -1.20938829]\n",
      " [-1.41229201  0.54154683  0.7519394 ]\n",
      " [-0.65876032 -1.22867499  0.25755777]\n",
      " [ 0.31290292 -0.13081169  1.26998312]\n",
      " [-0.09296246 -0.06615089 -1.10821447]\n",
      " [ 0.13595685  1.34707776  0.06114402]\n",
      " [ 0.0709146   0.43365454  0.27748366]]\n",
      "\n",
      "weight_W2:\n",
      "(3, 1), float64\n",
      "[[0.53025239]\n",
      " [0.53672097]\n",
      " [0.61835001]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "objects_ini = {'input_X1': input_X1_np, 'target_Y2': target_Y2_np,\n",
    "               'weight_W1': weight_W1_np, 'weight_W2': weight_W2_np}\n",
    "\n",
    "print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63486b13",
   "metadata": {},
   "source": [
    "#### Initialize import parameters\n",
    "\n",
    "**Neural Network Architecture**\n",
    "- <font color='dodgerblue'>input_size</font>: how many **data points** are in each **feature** (i.e., each node) within the **input layer**\n",
    "- <font color='dodgerblue'>hidden_size</font>: how many **data points** are in **each node** within the **hidden layer**\n",
    "- <font color='dodgerblue'>output_size</font>: how many **data points** are in **each node** within the **output layer**\n",
    "\n",
    "**Training Parameters**\n",
    "- <font color='dodgerblue'>learning_rate</font>: **step size** for **gradient descent**\n",
    "- <font color='dodgerblue'>num_epochs</font>: how many **training epochs** to **run** (instead of having a convergence cutoff criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11050373",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "hidden_size = 3\n",
    "output_size = 2\n",
    "\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf02001",
   "metadata": {},
   "source": [
    "### Now Focus on PyTorch\n",
    "\n",
    "##### Prepare data\n",
    "\n",
    "- The <font color='dodgerblue'>NumPy-generated input</font> and initial data values need to be <font color='dodgerblue'>converted to torch arrays</font> using **`torch.from_numpy()`**.\n",
    "\n",
    "- We can also <font color='dodgerblue'>improve upon</font> the original Numpy model by <font color='dodgerblue'>including biases</font>. These will be used in the <font color='dodgerblue'>linear transform</font> (e.g., **`torch.mm(input_X1, weight_W1) + bias_B1`**).\n",
    "\n",
    "- Care must be given to specify that **`torch.autograd.backwards()`** (done below) should <font color='dodgerblue'>record operations</font> for the <font color='dodgerblue'>weights and biases</font> (i.e., **calculation history**), using **`requires_grad_(requires_grad=True)`**.\n",
    "    - Reminder: <font color='dodgerblue'>only the weights and biases</font> need to be <font color='dodgerblue'>updated</font> based on the <font color='dodgerblue'>loss gradient</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6904f925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_X1:\n",
      "torch.Size([2, 10]), torch.float64\n",
      "tensor([[-1.4238,  1.2637, -0.8707, -0.2592, -0.0753, -0.7409, -1.3678,  0.6489,\n",
      "          0.3611, -1.9529],\n",
      "        [ 2.3474,  0.9685, -0.7594,  0.9022, -0.4670, -0.0607,  0.7888, -1.2567,\n",
      "          0.5759,  1.3990]], dtype=torch.float64)\n",
      "\n",
      "target_Y2:\n",
      "torch.Size([2, 1]), torch.float64\n",
      "tensor([[ 1.3223],\n",
      "        [-0.2997]], dtype=torch.float64)\n",
      "\n",
      "weight_W1:\n",
      "torch.Size([10, 3]), torch.float64\n",
      "tensor([[ 0.9029, -1.6216, -0.1582],\n",
      "        [ 0.4495, -1.3436, -0.0817],\n",
      "        [ 1.7247,  2.6182,  0.7774],\n",
      "        [ 0.8286, -0.9590, -1.2094],\n",
      "        [-1.4123,  0.5415,  0.7519],\n",
      "        [-0.6588, -1.2287,  0.2576],\n",
      "        [ 0.3129, -0.1308,  1.2700],\n",
      "        [-0.0930, -0.0662, -1.1082],\n",
      "        [ 0.1360,  1.3471,  0.0611],\n",
      "        [ 0.0709,  0.4337,  0.2775]], dtype=torch.float64, requires_grad=True)\n",
      "\n",
      "input_B1:\n",
      "torch.Size([3]), torch.float32\n",
      "tensor([0., 0., 0.], requires_grad=True)\n",
      "\n",
      "weight_W2:\n",
      "torch.Size([3, 1]), torch.float64\n",
      "tensor([[0.5303],\n",
      "        [0.5367],\n",
      "        [0.6184]], dtype=torch.float64, requires_grad=True)\n",
      "\n",
      "input_B2:\n",
      "torch.Size([2]), torch.float32\n",
      "tensor([0., 0.], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_X1 = torch.from_numpy(input_X1_np)\n",
    "target_Y2 = torch.from_numpy(target_Y2_np)\n",
    "\n",
    "weight_W1 = torch.from_numpy(weight_W1_np).requires_grad_(requires_grad=True)\n",
    "weight_W2 = torch.from_numpy(weight_W2_np).requires_grad_(requires_grad=True)\n",
    "\n",
    "bias_B1 = torch.zeros(hidden_size, requires_grad=True)\n",
    "bias_B2 = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "objects_ini = {'input_X1': input_X1, 'target_Y2': target_Y2,\n",
    "               'weight_W1': weight_W1, 'input_B1': bias_B1,\n",
    "               'weight_W2': weight_W2, 'input_B2': bias_B2}\n",
    "\n",
    "print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479506d3",
   "metadata": {},
   "source": [
    "#### Model Training\n",
    "\n",
    "**Multiplying two matrices** (dot product/matrix multiplication):\n",
    "- `torch.mm(mat1, mat2)`\n",
    "    - https://pytorch.org/docs/stable/generated/torch.mm.html\n",
    "    - <font color='dodgerblue'>2-D tensors</font> as inputs\n",
    "\n",
    "- `torch.matmul(mat1, mat2)`\n",
    "    - https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
    "    - <font color='dodgerblue'>more versatile</font>: matrix x matrix, matrix x vector and vector x vector operations\n",
    "        - (see `broadcasting` for more info: https://www.geeksforgeeks.org/understanding-broadcasting-in-pytorch/)\n",
    "\n",
    "Both functions are equivalent below in ***this*** particular example. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Element-wise Multiplication** (e.g., <font color='dodgerblue'>multiplying a float</font> and a <font color='dodgerblue'>matrix</font>):\n",
    "- `torch.mul(input, other)`\n",
    "    - https://pytorch.org/docs/stable/generated/torch.mul.html\n",
    "    - `input`: tensor\n",
    "    - `other`: tensor or number\n",
    "\n",
    "- Could also use `*`\n",
    "\n",
    "Both functions are demonstrated below. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Further Explanations**\n",
    "- `activation = torch.nn.ReLU()`: specify a <font color='dodgerblue'>**callable**</font> for the <font color='dodgerblue'>ReLU</font> activation function\n",
    "    - https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "\n",
    "<br>\n",
    "\n",
    "- `torch.autograd.backward` (i.e., `loss.backward()`):\n",
    "    - https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html\n",
    "    - https://www.geeksforgeeks.org/python-pytorch-backward-function/\n",
    "    - a major <font color='dodgerblue'>**workhorse**</font> in PyTorch\n",
    "    - computes the **gradient** (<font color='dodgerblue'>in the backward pass</font>) in the **entire neural network** for objects that have **`requires_grad=True`**\n",
    "\n",
    "<br>\n",
    "\n",
    "- `with torch.no_grad()`: required because the weights and biases require grad\n",
    "    - https://pytorch.org/docs/stable/generated/torch.no_grad.html\n",
    "    - <font color='dodgerblue'>Reduce memory consumption</font> for computations versus those that `requires_grad=True` \n",
    "    - If you tried to assign `weight_W1`, `bias_B1`, `weight_W2` and `bias_B2` without this `with torch.no_grad()` you would obtain the following error:\n",
    "        - `RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.`\n",
    "\n",
    "<br>\n",
    "\n",
    "- `torch.Tensor.zero_`: fills a given tensor with zeros\n",
    "    - https://pytorch.org/docs/stable/generated/torch.Tensor.zero_.html\n",
    "    - If this was **not done**, the gradients <font color='dodgerblue'>would be accumulated</font> during `.backwards()`, which would not be correct for each <font color='dodgerblue'>forward pass evaluation</font>\n",
    "    - The **`_`** indicates an **`inplace`** operation (like what we know from Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "697a4ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 3.320\n",
      "Epoch 2: Loss = 3.225\n",
      "Epoch 3: Loss = 3.135\n",
      "Epoch 4: Loss = 3.049\n",
      "Epoch 5: Loss = 2.968\n",
      "Epoch 6: Loss = 2.890\n",
      "Epoch 7: Loss = 2.817\n",
      "Epoch 8: Loss = 2.746\n",
      "Epoch 9: Loss = 2.679\n",
      "Epoch 10: Loss = 2.615\n",
      "Epoch 11: Loss = 2.554\n",
      "Epoch 12: Loss = 2.495\n",
      "Epoch 13: Loss = 2.439\n",
      "Epoch 14: Loss = 2.385\n",
      "Epoch 15: Loss = 2.334\n",
      "Epoch 16: Loss = 2.285\n",
      "Epoch 17: Loss = 2.238\n",
      "Epoch 18: Loss = 2.193\n",
      "Epoch 19: Loss = 2.149\n",
      "Epoch 20: Loss = 2.107\n",
      "Epoch 21: Loss = 2.067\n",
      "Epoch 22: Loss = 2.032\n",
      "Epoch 23: Loss = 2.007\n",
      "Epoch 24: Loss = 1.982\n",
      "Epoch 25: Loss = 1.958\n",
      "Epoch 26: Loss = 1.935\n",
      "Epoch 27: Loss = 1.912\n",
      "Epoch 28: Loss = 1.890\n",
      "Epoch 29: Loss = 1.869\n",
      "Epoch 30: Loss = 1.848\n",
      "Epoch 31: Loss = 1.828\n",
      "Epoch 32: Loss = 1.809\n",
      "Epoch 33: Loss = 1.790\n",
      "Epoch 34: Loss = 1.771\n",
      "Epoch 35: Loss = 1.754\n",
      "Epoch 36: Loss = 1.736\n",
      "Epoch 37: Loss = 1.719\n",
      "Epoch 38: Loss = 1.703\n",
      "Epoch 39: Loss = 1.687\n",
      "Epoch 40: Loss = 1.671\n",
      "Epoch 41: Loss = 1.656\n",
      "Epoch 42: Loss = 1.641\n",
      "Epoch 43: Loss = 1.627\n",
      "Epoch 44: Loss = 1.612\n",
      "Epoch 45: Loss = 1.599\n",
      "Epoch 46: Loss = 1.585\n",
      "Epoch 47: Loss = 1.572\n",
      "Epoch 48: Loss = 1.560\n",
      "Epoch 49: Loss = 1.547\n",
      "Epoch 50: Loss = 1.535\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    X2 = torch.mm(input_X1, weight_W1) + bias_B1\n",
    "\n",
    "    activation = torch.nn.ReLU()\n",
    "    Y1 = activation(X2)\n",
    "    \n",
    "    output_Y2 = torch.matmul(Y1, weight_W2) + bias_B2\n",
    "\n",
    "    loss = torch.mean(torch.square(torch.subtract(output_Y2, target_Y2))) # mean( (Y2 - y_target)^2 )\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimization: update weights and biases\n",
    "    with torch.no_grad():\n",
    "        weight_W1 -= torch.mul(learning_rate, weight_W1.grad)\n",
    "        bias_B1 -= torch.mul(learning_rate, bias_B1.grad)\n",
    "        weight_W2 -= learning_rate * weight_W2.grad\n",
    "        bias_B2 -= learning_rate * bias_B2.grad\n",
    "\n",
    "        # Reset the gradients to zero\n",
    "        weight_W1.grad.zero_()\n",
    "        bias_B1.grad.zero_()\n",
    "        weight_W2.grad.zero_()\n",
    "        bias_B2.grad.zero_()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.item():.3f}')\n",
    "\n",
    "    # objects_ini = {'weight_W1': weight_W1, 'bias_B1': bias_B1,\n",
    "    #            'weight_W2': weight_W2, 'bias_B2': bias_B2}\n",
    "    # print()\n",
    "    # print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc52857",
   "metadata": {},
   "source": [
    "#### Summary of Basic Example:\n",
    "- <font color='dodgerblue'>Tensor creation</font>: Using PyTorch's `from_numpy()` and `zeros()`\n",
    "- <font color='dodgerblue'>backward (autograd)</font>: Will know what automatic differentiation to do based on objects with `requires_grad_()`\n",
    "- Matrix operations: <font color='dodgerblue'>Matrix multiplication</font> (`torch.matmul`).\n",
    "- <font color='dodgerblue'>Activation functions</font>: Implementing a **ReLU** activation function\n",
    "- <font color='dodgerblue'>Gradients</font>: All computed in **one function call** of `backward()`\n",
    "- <font color='dodgerblue'>Loss function</font>: Calculating **mean squared error loss** (self encoded)\n",
    "- <font color='dodgerblue'>Optimization</font>: Performing **manual gradient descent**\n",
    "- <font color='dodgerblue'>Reset</font> the weight and bias <font color='dodgerblue'>gradients</font>: PyTorch's `.grad.zero_()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b19041",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "## Advance Example\n",
    "\n",
    "Create the same neural network, but now make it even better (readable, K.I.S.S., reusable) using PyTorch:\n",
    "\n",
    "- uses `torch.nn`: **modules/functions** for **building** a **neural networks**\n",
    "    - https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "<br>\n",
    "\n",
    "- uses a class\n",
    "    - the NN is defined as a subclass of **`nn.Module`**: the <font color='dodgerblue'>base class</font> for all <font color='dodgerblue'>neural network modules</font>\n",
    "        - https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\n",
    "        - Enables **easier organization** and **management** of **layers** and **parameters**\n",
    "    - classes are basically a <font color='dodgerblue'>blueprint</font> that can be <font color='dodgerblue'>reused</font>\n",
    "        - contains a collection of related functions\n",
    "        - **Personal Opinon**: they are **often unnecessary** - must have a good reason to implement\n",
    "\n",
    "<br>\n",
    "\n",
    "- `torch.nn.Linear`: applies a <font color='dodgerblue'>linear transformation</font> to the <font color='dodgerblue'>incoming data</font>\n",
    "    - https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
    "    - below, `fc1` and `fc2` represent **\"<font color='dodgerblue'>f</font>ully <font color='dodgerblue'>c</font>onnected\"** <font color='dodgerblue'>layers</font> <font color='dodgerblue'>**1**</font> and <font color='dodgerblue'>**2**</font>\n",
    "    - **weights** and **biases** are <font color='dodgerblue'>**automatically initialized**</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "- `torch.nn.ReLU`: **ReLU** activation function\n",
    "    -  https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU\n",
    "\n",
    "<br>\n",
    "\n",
    "- use a **built-in optimizer**\n",
    "\n",
    "#### Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "711c3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(torch.nn.Module):\n",
    "    \"\"\" This class defines a simple feedforward neural network with \n",
    "        one input, one hidden and one output layer, making use of a\n",
    "        ReLU activation function.\n",
    "\n",
    "        Attributes:\n",
    "            input_size (int): Size of the input layer.\n",
    "            hidden_size (int): Size of the hidden layer.\n",
    "            output_size (int): Size of the output layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.ReLU = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Forward pass of the SimpleNN.\n",
    "\n",
    "            Args:\n",
    "                x: Input data tensor (i.e., features)\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: Output data tensor after neural network forward pass\n",
    "        \"\"\"\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "        else:\n",
    "            x = self.fc1(x)\n",
    "            x = self.ReLU(x)\n",
    "            x = self.fc2(x)\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed70b70",
   "metadata": {},
   "source": [
    "#### Revisiting the toy data\n",
    "Some of PyTorch's functions require the numbers to be **`float32`** (GPUs are optimized for these). Our above **`input_X1`** and **`input_Y2`** tensors have numbers that are **`float64`**.\n",
    "\n",
    "- `to(torch.float32)`: changes the tensor item's **type** (`dtype`)\n",
    "\n",
    "Alter the existing data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a75e1f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_X1:\n",
      "torch.Size([2, 10]), torch.float32\n",
      "tensor([[-1.4238,  1.2637, -0.8707, -0.2592, -0.0753, -0.7409, -1.3678,  0.6489,\n",
      "          0.3611, -1.9529],\n",
      "        [ 2.3474,  0.9685, -0.7594,  0.9022, -0.4670, -0.0607,  0.7888, -1.2567,\n",
      "          0.5759,  1.3990]])\n",
      "\n",
      "target_Y2:\n",
      "torch.Size([2, 1]), torch.float32\n",
      "tensor([[ 1.3223],\n",
      "        [-0.2997]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_X1 = input_X1.to(torch.float32)\n",
    "target_Y2 = target_Y2.to(torch.float32)\n",
    "\n",
    "objects_ini = {'input_X1': input_X1, 'target_Y2': target_Y2}\n",
    "print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562ba64",
   "metadata": {},
   "source": [
    "#### Model, Loss and Optimizer\n",
    "- create the <font color='dodgerblue'>NN model</font>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "- define the **optimizing function** (i.e., `optim.SGD`) for adjusting the **weights** and **biases**\n",
    "    - Optimization overview: https://pytorch.org/docs/stable/optim.html#module-torch.optim\n",
    "    - **Available algorithms**: https://pytorch.org/docs/stable/optim.html#algorithms\n",
    "        - **gradient decent**: https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n",
    "     \n",
    "<br>\n",
    "\n",
    "- define the **loss function** to use\n",
    "    - `torch.nn.MSELoss`: <font color='dodgerblue'>mean squared error</font> (a.k.a., Loss2; L2)\n",
    "        - https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "##### Sidenote: Coding concept concerning the assignment of a function to a variable/object\n",
    "\n",
    "<br>\n",
    "\n",
    "For example: `loss_function = torch.nn.MSELoss()` that is given in the next code cell\n",
    "\n",
    "<br>\n",
    "\n",
    "Why do this?\n",
    "\n",
    "- Quickly and easily change an overall code's behavior: **reassign** the **variable** to a **different function**\n",
    "\n",
    "    - <font color='dodgerblue'>explore different ideas</font>\n",
    "\n",
    "- **Abstraction**: abstract away the specific implementation details\n",
    "    - Idea: <font color='dodgerblue'>**Focus** on the **what**, **not** the **how**</font>\n",
    "        - more <font color='dodgerblue'>readable</font>\n",
    "        - easier to understand **concepts** (e.g., the <font color='dodgerblue'>science</font>) - don't get lost in the details\n",
    "        - easier to <font color='dodgerblue'>maintain</font>\n",
    "\n",
    "    - Related terms:\n",
    "        - <font color='dodgerblue'>encapsulation</font>: **grouping data** (information) and the **methods** (functions) that are **related** within a single unit (e.g. a class)\n",
    "        - <font color='dodgerblue'>modularity/decomposition</font>: **breaking down** a **large program** into **smaller**, **independent** components (e.g., **functions**)\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6913bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_function = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b1f0e5",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "- `zero_grad()`: **set/reset** the **gradients** of all **optimized tensors** (i.e, for the **weights** and **biases**)\n",
    "    - https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html\n",
    "    - this is the same concept as above when we used `torch.Tensor.zero` in the basic example\n",
    "        - this is necessary since <font color='dodgerblue'>`.backward()` accumulates the gradients</font> **each time** it is **called**\n",
    "\n",
    "<br>\n",
    "\n",
    "- `torch.optim.Optimizer.step`: perform an **optimization step** based on the **current gradients** (stored in `.grad`), which is coming from **`.backward()`** \n",
    "    - https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2409575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.190\n",
      "Epoch 2: Loss = 1.188\n",
      "Epoch 3: Loss = 1.186\n",
      "Epoch 4: Loss = 1.184\n",
      "Epoch 5: Loss = 1.182\n",
      "Epoch 6: Loss = 1.180\n",
      "Epoch 7: Loss = 1.178\n",
      "Epoch 8: Loss = 1.176\n",
      "Epoch 9: Loss = 1.174\n",
      "Epoch 10: Loss = 1.172\n",
      "Epoch 11: Loss = 1.170\n",
      "Epoch 12: Loss = 1.168\n",
      "Epoch 13: Loss = 1.166\n",
      "Epoch 14: Loss = 1.164\n",
      "Epoch 15: Loss = 1.162\n",
      "Epoch 16: Loss = 1.160\n",
      "Epoch 17: Loss = 1.158\n",
      "Epoch 18: Loss = 1.156\n",
      "Epoch 19: Loss = 1.154\n",
      "Epoch 20: Loss = 1.152\n",
      "Epoch 21: Loss = 1.150\n",
      "Epoch 22: Loss = 1.148\n",
      "Epoch 23: Loss = 1.146\n",
      "Epoch 24: Loss = 1.144\n",
      "Epoch 25: Loss = 1.142\n",
      "Epoch 26: Loss = 1.140\n",
      "Epoch 27: Loss = 1.138\n",
      "Epoch 28: Loss = 1.136\n",
      "Epoch 29: Loss = 1.134\n",
      "Epoch 30: Loss = 1.132\n",
      "Epoch 31: Loss = 1.130\n",
      "Epoch 32: Loss = 1.128\n",
      "Epoch 33: Loss = 1.126\n",
      "Epoch 34: Loss = 1.125\n",
      "Epoch 35: Loss = 1.123\n",
      "Epoch 36: Loss = 1.121\n",
      "Epoch 37: Loss = 1.119\n",
      "Epoch 38: Loss = 1.117\n",
      "Epoch 39: Loss = 1.115\n",
      "Epoch 40: Loss = 1.113\n",
      "Epoch 41: Loss = 1.111\n",
      "Epoch 42: Loss = 1.109\n",
      "Epoch 43: Loss = 1.107\n",
      "Epoch 44: Loss = 1.105\n",
      "Epoch 45: Loss = 1.104\n",
      "Epoch 46: Loss = 1.102\n",
      "Epoch 47: Loss = 1.100\n",
      "Epoch 48: Loss = 1.098\n",
      "Epoch 49: Loss = 1.096\n",
      "Epoch 50: Loss = 1.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karl/programs/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([2, 1])) that is different to the input size (torch.Size([2, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    output_Y2 = model(input_X1)\n",
    "\n",
    "    loss = loss_function(output_Y2, target_Y2)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimization: update weights and biases\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca28acf7-7669-49df-86d1-341020d0d982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[ 0.1209, -0.1781, -0.2986, -0.2995, -0.3051, -0.1803, -0.1093,  0.1260,\n",
       "                       -0.1575,  0.0985],\n",
       "                      [-0.2347,  0.0828,  0.1411,  0.1737,  0.1222,  0.0510,  0.0126,  0.2039,\n",
       "                        0.0022,  0.0635],\n",
       "                      [ 0.0112, -0.1346,  0.0927, -0.1011,  0.2960, -0.1541,  0.0456, -0.2401,\n",
       "                       -0.2132,  0.2132]])),\n",
       "             ('fc1.bias', tensor([-0.0292,  0.1374,  0.0915])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.3437,  0.2566, -0.4031],\n",
       "                      [ 0.1033,  0.0458,  0.4265]])),\n",
       "             ('fc2.bias', tensor([-0.0965, -0.2914]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d785e0b-8c1f-415d-84e6-6c4c73ecd538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Output: \n",
      " tensor([[-0.0249, -0.2709],\n",
      "        [-0.1819, -0.2025]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "fc1.weight:\n",
      "torch.Size([3, 10]), torch.float32\n",
      "tensor([[ 0.1209, -0.1781, -0.2986, -0.2995, -0.3051, -0.1803, -0.1093,  0.1260,\n",
      "         -0.1575,  0.0985],\n",
      "        [-0.2347,  0.0828,  0.1411,  0.1737,  0.1222,  0.0510,  0.0126,  0.2039,\n",
      "          0.0022,  0.0635],\n",
      "        [ 0.0112, -0.1346,  0.0927, -0.1011,  0.2960, -0.1541,  0.0456, -0.2401,\n",
      "         -0.2132,  0.2132]])\n",
      "\n",
      "fc1.bias:\n",
      "torch.Size([3]), torch.float32\n",
      "tensor([-0.0292,  0.1374,  0.0915])\n",
      "\n",
      "fc2.weight:\n",
      "torch.Size([2, 3]), torch.float32\n",
      "tensor([[-0.3437,  0.2566, -0.4031],\n",
      "        [ 0.1033,  0.0458,  0.4265]])\n",
      "\n",
      "fc2.bias:\n",
      "torch.Size([2]), torch.float32\n",
      "tensor([-0.0965, -0.2914])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final outputs, weights and biases\n",
    "print(f'\\nFinal Output: \\n {output_Y2}\\n')\n",
    "objects_ini = model.state_dict()\n",
    "\n",
    "print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a5e38-2277-4126-a5f8-ce5e813fbf4b",
   "metadata": {},
   "source": [
    "#### Using the trained (optimized) model\n",
    "\n",
    "Create random new data:\n",
    "- 5 new samples\n",
    "- 10 data points in each new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82a5595c-8f9d-4127-829d-645b1312b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = torch.randn(50, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bef9f7fa-55be-4f2d-9b5c-dea08efbf220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3284, -0.0460],\n",
       "        [-0.1936, -0.0705],\n",
       "        [-0.0965, -0.2914],\n",
       "        [-0.3311,  0.1044],\n",
       "        [-0.1260, -0.2602],\n",
       "        [-0.0055, -0.2245],\n",
       "        [-0.0457, -0.2823],\n",
       "        [-0.3517, -0.1254],\n",
       "        [-0.3416, -0.0261],\n",
       "        [-0.3708,  0.0109],\n",
       "        [ 0.1994, -0.2386],\n",
       "        [-0.6323,  0.2755],\n",
       "        [-0.1503, -0.1872],\n",
       "        [-0.4430,  0.1001],\n",
       "        [ 0.0905, -0.2580],\n",
       "        [ 0.0252, -0.2697],\n",
       "        [-0.2123, -0.0916],\n",
       "        [-0.4105, -0.0556],\n",
       "        [-0.0159, -0.2770],\n",
       "        [-0.0300, -0.2421],\n",
       "        [-0.8982,  0.2370],\n",
       "        [-0.0302, -0.2796],\n",
       "        [ 0.0454, -0.1480],\n",
       "        [-0.1892, -0.1907],\n",
       "        [-0.4155,  0.0386],\n",
       "        [-0.5264,  0.0510],\n",
       "        [-0.4296,  0.1302],\n",
       "        [-0.5925,  0.2980],\n",
       "        [ 0.2246, -0.2341],\n",
       "        [-0.1036, -0.2297],\n",
       "        [-0.1843, -0.1189],\n",
       "        [-0.1112, -0.1007],\n",
       "        [-0.0209, -0.2019],\n",
       "        [-0.3946,  0.0093],\n",
       "        [-0.0368, -0.1697],\n",
       "        [-0.4301,  0.0226],\n",
       "        [-0.5594, -0.0965],\n",
       "        [-0.3930, -0.1621],\n",
       "        [-0.2526, -0.1284],\n",
       "        [-0.3534, -0.1096],\n",
       "        [-0.1758, -0.2676],\n",
       "        [ 0.1509, -0.2472],\n",
       "        [-0.4231,  0.0284],\n",
       "        [-0.2226, -0.0165],\n",
       "        [ 0.0074, -0.2729],\n",
       "        [-0.3543, -0.1003],\n",
       "        [-0.0542, -0.2581],\n",
       "        [-0.0581, -0.2714],\n",
       "        [-0.5933,  0.0560],\n",
       "        [ 0.0238, -0.2699]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c58b5a5-992a-44a9-91e2-30d67a39744f",
   "metadata": {},
   "source": [
    "### Creating a customized activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "012856b1-5e19-4f52-a705-8fc3c9803d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedRelu(torch.nn.Module):\n",
    "    ''' Modified ReLU activation function.\n",
    "\n",
    "        Class that implements a modified ReLU function that adds\n",
    "        1.0 to the input. \n",
    "\n",
    "        Attributes:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: modified ReLU activation output\n",
    "    ''' \n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the modified_relu activation function.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying the modified\n",
    "            ReLU activation.\n",
    "        \"\"\"\n",
    "        if not isinstance(input, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\") \n",
    "        else:\n",
    "            mod_relu = torch.maximum(input+1.0, torch.zeros_like(input))\n",
    "\n",
    "            ## Comment out to see how mod_relu operates on the input\n",
    "            # print(input, \"\\n\", mod_relu)\n",
    "            return mod_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d56941a3-41ef-40b2-9afe-e7da4a5ed82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedNN(torch.nn.Module):\n",
    "    \"\"\" Modified Neural Network.\n",
    "\n",
    "        This class defines a simple feedforward neural network with \n",
    "        one hidden layer and uses the ModifiedReLU activation function.\n",
    "\n",
    "        Attributes:\n",
    "            input_size (int): Size of the input layer.\n",
    "            hidden_size (int): Size of the hidden layer.\n",
    "            output_size (int): Size of the output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ModifiedNN, self).__init__()\n",
    "        \n",
    "        if not all(isinstance(param, int) for param in [input_size, hidden_size, output_size]):\n",
    "            raise TypeError(\"All input parameters must be an integer\")\n",
    "        else:\n",
    "            self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "            self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "            self.modified_relu = ModifiedRelu()\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Forward pass of the ModifiedNN.\n",
    "\n",
    "            Args:\n",
    "                x: Input data tensor (i.e., features)\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: Output data tensor after neural network forward pass\n",
    "        \"\"\"\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "        else:\n",
    "            x = self.fc1(x)\n",
    "            x = self.modified_relu(x)\n",
    "            x = self.fc2(x)\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12455b-f0cd-45f4-8975-94d05cf623db",
   "metadata": {},
   "source": [
    "1. Create a new model using the modified_nn architecture\n",
    "2. pass it to the gradient descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e85b6524-dacc-4ae0-9329-cb33694e203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = ModifiedNN(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "optimizer = optim.SGD(params=new_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6408b2a-ea13-40b7-9c10-13317c316ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 2.453\n",
      "Epoch 2: Loss = 2.403\n",
      "Epoch 3: Loss = 2.353\n",
      "Epoch 4: Loss = 2.304\n",
      "Epoch 5: Loss = 2.257\n",
      "Epoch 6: Loss = 2.211\n",
      "Epoch 7: Loss = 2.166\n",
      "Epoch 8: Loss = 2.122\n",
      "Epoch 9: Loss = 2.079\n",
      "Epoch 10: Loss = 2.037\n",
      "Epoch 11: Loss = 1.997\n",
      "Epoch 12: Loss = 1.957\n",
      "Epoch 13: Loss = 1.918\n",
      "Epoch 14: Loss = 1.880\n",
      "Epoch 15: Loss = 1.842\n",
      "Epoch 16: Loss = 1.806\n",
      "Epoch 17: Loss = 1.770\n",
      "Epoch 18: Loss = 1.736\n",
      "Epoch 19: Loss = 1.702\n",
      "Epoch 20: Loss = 1.669\n",
      "Epoch 21: Loss = 1.636\n",
      "Epoch 22: Loss = 1.605\n",
      "Epoch 23: Loss = 1.574\n",
      "Epoch 24: Loss = 1.543\n",
      "Epoch 25: Loss = 1.514\n",
      "Epoch 26: Loss = 1.485\n",
      "Epoch 27: Loss = 1.456\n",
      "Epoch 28: Loss = 1.429\n",
      "Epoch 29: Loss = 1.401\n",
      "Epoch 30: Loss = 1.375\n",
      "Epoch 31: Loss = 1.349\n",
      "Epoch 32: Loss = 1.324\n",
      "Epoch 33: Loss = 1.299\n",
      "Epoch 34: Loss = 1.274\n",
      "Epoch 35: Loss = 1.251\n",
      "Epoch 36: Loss = 1.227\n",
      "Epoch 37: Loss = 1.204\n",
      "Epoch 38: Loss = 1.182\n",
      "Epoch 39: Loss = 1.160\n",
      "Epoch 40: Loss = 1.139\n",
      "Epoch 41: Loss = 1.118\n",
      "Epoch 42: Loss = 1.097\n",
      "Epoch 43: Loss = 1.077\n",
      "Epoch 44: Loss = 1.058\n",
      "Epoch 45: Loss = 1.038\n",
      "Epoch 46: Loss = 1.019\n",
      "Epoch 47: Loss = 1.001\n",
      "Epoch 48: Loss = 0.983\n",
      "Epoch 49: Loss = 0.965\n",
      "Epoch 50: Loss = 0.948\n",
      "\n",
      "Final Output: \n",
      " tensor([[ 0.0728,  0.2088],\n",
      "        [-0.1280, -1.2799]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "fc1.weight:\n",
      "torch.Size([3, 10]), torch.float32\n",
      "tensor([[-0.0614,  0.2178, -0.2448,  0.2310,  0.1246, -0.1567, -0.0189,  0.3045,\n",
      "         -0.2908, -0.1689],\n",
      "        [ 0.0970, -0.2443,  0.0104,  0.0716, -0.2994, -0.0495, -0.0088,  0.0992,\n",
      "         -0.2483,  0.1724],\n",
      "        [-0.2214,  0.0196,  0.0032,  0.3112,  0.1054, -0.1007,  0.0261, -0.2950,\n",
      "         -0.2884, -0.0378]])\n",
      "\n",
      "fc1.bias:\n",
      "torch.Size([3]), torch.float32\n",
      "tensor([ 0.1436, -0.2238, -0.1624])\n",
      "\n",
      "fc2.weight:\n",
      "torch.Size([2, 3]), torch.float32\n",
      "tensor([[-0.2393, -0.0237,  0.4518],\n",
      "        [ 0.0044,  0.2874,  0.3652]])\n",
      "\n",
      "fc2.bias:\n",
      "torch.Size([2]), torch.float32\n",
      "tensor([0.2120, 0.2612])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    output_Y2 = new_model(input_X1)\n",
    "\n",
    "    loss = loss_function(output_Y2, target_Y2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.item():.3f}')\n",
    "\n",
    "print(f'\\nFinal Output: \\n {output_Y2}\\n')\n",
    "\n",
    "objects_ini = model.state_dict()\n",
    "print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ea05a",
   "metadata": {},
   "source": [
    "**Notice**  that the weights' shapes are not yet transposed, as done in the above basic example.\n",
    "\n",
    "**Notice** that the architecture specification for the network is very short and simple.\n",
    "\n",
    "Once a neural network model has been trained, it is ready to be used (i.e., to make predictions).\n",
    "\n",
    "Reusability: how can others use the trained model? What is required is:\n",
    "1. the neural network architecture\n",
    "    - number of layers and number of nodes in each layer\n",
    "    - how the nodes are connected\n",
    "    - the activation functions and their placement within the network\n",
    "3. the optimized parameters\n",
    "    - the optimized weights\n",
    "    - the optimized biases\n",
    "4. other parameters (i.e., called hyperparameters)\n",
    "    - learning rate\n",
    "    - optimization cutoff thresholds or maximum number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "baf0f85a-d217-49d5-8797-eef25c048076",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c32b0e5b-9612-4373-8880-35c09436f2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3622, -0.5315],\n",
       "        [ 0.3424, -0.3077]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d32d8-d073-4c63-b79b-567f2cb5616f",
   "metadata": {},
   "source": [
    "#### Training and Testing\n",
    "Recall that in our shallow learning lecture, we discussed the concept of splitting a dataset into a training dataset and a test dataset. The same things idea is still utilized for neural networks.\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "#### Summary of Advance Example:\n",
    "- A <font color='dodgerblue'>class</font> (like a blueprint) and <font color='dodgerblue'>`nn.Module`</font>: a structured PyTorch approach for **defining a neural network**\n",
    "    - e.g., architecture, activation functions\n",
    "    - allows for easy/better organization and code reusability\n",
    "- Built-in <font color='dodgerblue'>Activation</font>: `torch.nn.ReLU`\n",
    "- Built-in <font color='dodgerblue'>Loss</font>: `torch.nn.MSELoss` for mean squared loss (i.e., Loss2; L2)\n",
    "- All <font color='dodgerblue'>gradients</font> needed in backward propagation done using `autograd.backwards()`\n",
    "- Built-in <font color='dodgerblue'>Optimizer</font>: `optim.SGD` for gradient descent and usage of `.step()`\n",
    "- Create a <font color='dodgerblue'>customized activation</font> function and implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ac293-7c35-4607-a733-52d88734292d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
