{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e3d087-efd5-423a-a353-c94db02dedc7",
   "metadata": {},
   "source": [
    "<div class=\"alert block alert-info alert\">\n",
    "\n",
    "# <center> Scientific Programming in Python\n",
    "\n",
    "## <center>Karl N. Kirschner<br>Bonn-Rhein-Sieg University of Applied Sciences<br>Sankt Augustin, Germany\n",
    "\n",
    "# <center> PyTorch: Simple Neural Network Example\n",
    "\n",
    "## <center>  with a Perceptron\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c15d01d",
   "metadata": {},
   "source": [
    "This lecture will parallel the perceptron example written using NumPy, allowing you to compare the approaches directly.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"00_images/31_machine_learning/nn_perceptron_example.png\" alt=\"nn_percepton\" style=\"width: 1000px;\"/></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "The neural network (NN) will be written in two ways:\n",
    "1. Basic - to explicitly show all of the steps in a neural network training\n",
    "2. Advance - to show the basics for how most PyTorch code is actually written\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c0bf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b75bd67",
   "metadata": {},
   "source": [
    "Create a helper function that allows us to investigate the different arrays that are used below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb9c19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_array_specs(in_arrays: dict):\n",
    "    ''' Helper function for nicely printing NumPy and\n",
    "        PyTorch arrays.\n",
    "\n",
    "        Print: shape, data type and values.\n",
    "    '''\n",
    "    for key, value in in_arrays.items():\n",
    "        print(f'{key}:\\n{value.shape}, {value.dtype}')\n",
    "        print(f'{value}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cbd5f2",
   "metadata": {},
   "source": [
    "## Basic Example\n",
    "\n",
    "#### Define the toy data (input values, target values and initial weights):\n",
    "\n",
    "##### A reminder from the NumPy lecture\n",
    "\n",
    "A random **seed** will be **explicitly set**, allowing for **reproducible results** (i.e., for teaching purposes). The first epoch data generated below should correspond to the numeric values given in the figure above.\n",
    "\n",
    "The object naming will also be done to parallel the figure above.\n",
    "\n",
    "Random Number Generator in NumPY:\n",
    "- `np.random.default_rng`: https://numpy.org/doc/stable/reference/random/generator.html\n",
    "- `numpy.random.Generator.normal`: https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html\n",
    "\n",
    "<br>\n",
    "\n",
    "**Important Note**: Normally with <font color='dodgerblue'>real-world data</font>, one often should <font color='dodgerblue'>normalize</font> (e.g., **transpose** the date to a range [0, 1]) the <font color='dodgerblue'>input data</font>. This helps the mathematics when different input features have **large magnitude differences** (e.g., 1.5 and 2.5e6).\n",
    "- https://en.wikipedia.org/wiki/Normalization_(statistics)\n",
    "- `sklearn.preprocessing.normalize`: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html\n",
    "\n",
    "In this example, we don't need to worry about this due to how we generate the toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b00bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=12345)\n",
    "\n",
    "input_X1_np = rng.normal(size=(2, 10))\n",
    "target_Y2_np = rng.normal(size=(2, 1))\n",
    "\n",
    "weight_W1_np = rng.normal(size=(10, 3))\n",
    "weight_W2_np = rng.normal(size=(3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0682b4",
   "metadata": {},
   "source": [
    "Examine the different NumPy arrays:\n",
    "- shapes (important for matrix multiplication)\n",
    "- data types (need to be same types)\n",
    "- values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a136911",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_ini = {'input_X1': input_X1_np, 'target_Y2': target_Y2_np,\n",
    "               'weight_W1': weight_W1_np, 'weight_W2': weight_W2_np}\n",
    "\n",
    "print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63486b13",
   "metadata": {},
   "source": [
    "#### Initialize import parameters\n",
    "\n",
    "**Neural Network Architecture**\n",
    "- <font color='dodgerblue'>input_size</font>: how many **data points** are in each **feature** (i.e., each node) within the **input layer**\n",
    "- <font color='dodgerblue'>hidden_size</font>: how many **data points** are in **each node** within the **hidden layer**\n",
    "- <font color='dodgerblue'>output_size</font>: how many **data points** are in **each node** within the **output layer**\n",
    "\n",
    "**Training Parameters**\n",
    "- <font color='dodgerblue'>learning_rate</font>: **step size** for **gradient descent**\n",
    "- <font color='dodgerblue'>num_epochs</font>: how many **training epochs** to **run** (instead of having a convergence cutoff criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11050373",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "hidden_size = 3\n",
    "output_size = 2\n",
    "\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf02001",
   "metadata": {},
   "source": [
    "### Now Focus on PyTorch\n",
    "\n",
    "##### Prepare data\n",
    "\n",
    "- The <font color='dodgerblue'>NumPy-generated input</font> and initial data values need to be <font color='dodgerblue'>converted to torch arrays</font> using **`torch.from_numpy()`**.\n",
    "\n",
    "- We can also <font color='dodgerblue'>improve upon</font> the original Numpy model by <font color='dodgerblue'>including biases</font>. These will be used in the <font color='dodgerblue'>linear transform</font> (e.g., **`torch.mm(input_X1, weight_W1) + bias_B1`**).\n",
    "\n",
    "- Care must be given to specify that **`autograd`** should <font color='dodgerblue'>record operations</font> for the <font color='dodgerblue'>weights and biases</font> (i.e., calculation history), using **`requires_grad_(requires_grad=True)`**.\n",
    "    - Reminder: <font color='dodgerblue'>only the weights and biases</font> need to be <font color='dodgerblue'>updated</font> based on the <font color='dodgerblue'>loss gradient</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X1 = torch.from_numpy(input_X1_np)\n",
    "target_Y2 = torch.from_numpy(target_Y2_np)\n",
    "\n",
    "weight_W1 = torch.from_numpy(weight_W1_np).requires_grad_(requires_grad=True)\n",
    "weight_W2 = torch.from_numpy(weight_W2_np).requires_grad_(requires_grad=True)\n",
    "\n",
    "bias_B1 = torch.zeros(hidden_size, requires_grad=True)\n",
    "bias_B2 = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "objects_ini = {'input_X1': input_X1, 'target_Y2': target_Y2,\n",
    "               'weight_W1': weight_W1, 'input_B1': bias_B1,\n",
    "               'weight_W2': weight_W2, 'input_B2': bias_B2}\n",
    "\n",
    "print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479506d3",
   "metadata": {},
   "source": [
    "#### Model Training\n",
    "\n",
    "**Multiplying two matrices** (dot product/matrix multiplication):\n",
    "- `torch.mm(mat1, mat2)`\n",
    "    - https://pytorch.org/docs/stable/generated/torch.mm.html\n",
    "    - <font color='dodgerblue'>2-D tensors</font> as inputs\n",
    "\n",
    "- `torch.matmul(mat1, mat2)`\n",
    "    - https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
    "    - <font color='dodgerblue'>more versatile</font>: matrix x matrix, matrix x vector and vector x vector operations\n",
    "        - (see `broadcasting` for more info: https://www.geeksforgeeks.org/understanding-broadcasting-in-pytorch/)\n",
    "\n",
    "Both functions are equivalent below in ***this*** particular example. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Element-wise Multiplication** (e.g., <font color='dodgerblue'>multiplying a float</font> and a <font color='dodgerblue'>matrix</font>):\n",
    "- `torch.mul(input, other)`\n",
    "    - https://pytorch.org/docs/stable/generated/torch.mul.html\n",
    "    - `input`: tensor\n",
    "    - `other`: tensor or number\n",
    "\n",
    "- Could also use `*`\n",
    "\n",
    "Both functions are demonstrated below. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Further Explanations**\n",
    "- `activation = torch.nn.ReLU()`: specify a <font color='dodgerblue'>**callable**</font> for the <font color='dodgerblue'>ReLU</font> activation function\n",
    "    - https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "\n",
    "<br>\n",
    "\n",
    "- `torch.autograd.backward` computes the **gradient** (<font color='dodgerblue'>backward pass</font>) in the entire neural network for objects that have **`requires_grad=True`**\n",
    "    - https://www.geeksforgeeks.org/python-pytorch-backward-function/\n",
    "\n",
    "<br>\n",
    "\n",
    "- `with torch.no_grad()`: required because the weights and biases require grad\n",
    "    - https://pytorch.org/docs/stable/generated/torch.no_grad.html\n",
    "    - <font color='dodgerblue'>Reduce memory consumption</font> for computations versus those that `requires_grad=True` \n",
    "    - If you tried to assign `weight_W1`, `bias_B1`, `weight_W2` and `bias_B2` without this `with torch.no_grad()` you would obtain the following error:\n",
    "        - `RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.`\n",
    "\n",
    "<br>\n",
    "\n",
    "- `torch.Tensor.zero_`: fills a given tensor with zeros\n",
    "    - https://pytorch.org/docs/stable/generated/torch.Tensor.zero_.html\n",
    "    - If this was **not done**, the gradients <font color='dodgerblue'>would be accumulated</font> during `.backwards()`, which would not be correct for each <font color='dodgerblue'>forward pass evaluation</font>\n",
    "    - The **`_`** indicates an **`inplace`** operation (like what we know from Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a4ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    X2 = torch.mm(input_X1, weight_W1) + bias_B1\n",
    "    \n",
    "    # activation = torch.nn.LeakyReLU(0.1)\n",
    "    activation = torch.nn.ReLU()\n",
    "    Y1 = activation(X2)\n",
    "    \n",
    "    output_Y2 = torch.matmul(Y1, weight_W2) + bias_B2\n",
    "\n",
    "    loss = torch.mean(torch.square(torch.subtract(output_Y2, target_Y2))) # mean( (Y2-y_target)^2 )\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimization: update weights and biases\n",
    "    with torch.no_grad():\n",
    "        weight_W1 -= torch.mul(learning_rate, weight_W1.grad)\n",
    "        bias_B1 -= torch.mul(learning_rate, bias_B1.grad)\n",
    "        weight_W2 -= learning_rate * weight_W2.grad\n",
    "        bias_B2 -= learning_rate * bias_B2.grad\n",
    "        print(f'bias 1 grad: {loss.grad}, {bias_B1}, {bias_B1.grad}, {torch.mul(learning_rate, bias_B1.grad)}')\n",
    "        print(f'bias 2 grad: {loss.grad}, {bias_B2}, {bias_B2.grad}, {torch.mul(learning_rate, bias_B2.grad)}')\n",
    "\n",
    "        # Reset the gradients to zero\n",
    "        weight_W1.grad.zero_()\n",
    "        bias_B1.grad.zero_()\n",
    "        weight_W2.grad.zero_()\n",
    "        bias_B2.grad.zero_()\n",
    "\n",
    "    # print(weight_W1.grad) # visual proof that they are zero\n",
    "\n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.item():.3f}')\n",
    "\n",
    "    objects_ini = {'weight_W1': weight_W1, 'bias_B1': bias_B1,\n",
    "               'weight_W2': weight_W2, 'bias_B2': bias_B2}\n",
    "    print()\n",
    "    print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c455479-0e53-4ffd-a039-e0e29993c701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.42382504,  1.26372846, -0.87066174, -0.25917323, -0.07534331,\n",
       "        -0.74088465, -1.3677927 ,  0.6488928 ,  0.36105811, -1.95286306],\n",
       "       [ 2.34740965,  0.96849691, -0.75938718,  0.90219827, -0.46695317,\n",
       "        -0.06068952,  0.78884434, -1.25666813,  0.57585751,  1.39897899]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 8.57617496, 11.26372846,  9.12933826,  9.74082677,  9.92465669,\n",
       "         9.25911535,  8.6322073 , 10.6488928 , 10.36105811,  8.04713694],\n",
       "       [12.34740965, 10.96849691,  9.24061282, 10.90219827,  9.53304683,\n",
       "         9.93931048, 10.78884434,  8.74333187, 10.57585751, 11.39897899]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Adding arrays\n",
    "display(input_X1_np)\n",
    "\n",
    "weight_example = np.full((10), 10)\n",
    "\n",
    "input_X1_np + weight_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc52857",
   "metadata": {},
   "source": [
    "#### Summary of Basic Example:\n",
    "- <font color='dodgerblue'>Tensor creation</font>: Using PyTorch's `from_numpy()` and `zeros()`\n",
    "- <font color='dodgerblue'>Autograd</font>: Utilizing `requires_grad_()` for automatic differentiation\n",
    "- Matrix operations: <font color='dodgerblue'>Matrix multiplication</font> (`torch.mm` and `torch.matmul`).\n",
    "- <font color='dodgerblue'>Activation functions</font>: Implementing a **ReLU** activation function\n",
    "- <font color='dodgerblue'>Gradients</font>: All computed in **one function call** of `backward()`\n",
    "- <font color='dodgerblue'>Loss function</font>: Calculating **mean squared error loss**\n",
    "- <font color='dodgerblue'>Optimization</font>: Performing **manual gradient descent**\n",
    "- <font color='dodgerblue'>Reset</font> the weight and bias <font color='dodgerblue'>gradients</font>: PyTorch's `.grad.zero_()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b19041",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "## Advance Example\n",
    "\n",
    "Create the same neural network, but now make it even better (readable, K.I.S.S., reusable) using PyTorch:\n",
    "\n",
    "- uses `torch.nn`: **modules/functions** for **building** a **neural networks**\n",
    "    - https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "<br>\n",
    "\n",
    "- Uses a class\n",
    "    - the NN is defined as a subclass of **`nn.Module`**: the <font color='dodgerblue'>base class</font> for all <font color='dodgerblue'>neural network modules</font>\n",
    "        - https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\n",
    "        - Enables **easier organization** and **management** of **layers** and **parameters**\n",
    "    - classes are basically a <font color='dodgerblue'>blueprint</font> that can be <font color='dodgerblue'>reused</font>\n",
    "        - contains a collection of related functions\n",
    "        - **Personal Opinon**: they are **often unnecessary** - must have a good reason to implement\n",
    "\n",
    "<br>\n",
    "\n",
    "- `torch.nn.Linear`: applies a <font color='dodgerblue'>linear transformation</font> to the <font color='dodgerblue'>incoming data</font>\n",
    "    - https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
    "    - below, `fc1` and `fc2` represent **\"fully connected\"** <font color='dodgerblue'>layers</font>\n",
    "    - **weights** and **biases** are <font color='dodgerblue'>**automatically initialized**</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "- `torch.nn.ReLU`: **ReLU** activation function\n",
    "    -  https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU\n",
    "\n",
    "<br>\n",
    "\n",
    "- use a **built-in optimizer**\n",
    "\n",
    "#### Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.ReLU = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed70b70",
   "metadata": {},
   "source": [
    "#### Revisiting the toy data\n",
    "Some of PyTorch's functions require the numbers to be `float32` (GPUs are optimized for these). Our above `input_X1` and `input_Y2` tensors have numbers that are `float64`.\n",
    "- `to(torch.float32)`: changes the tensor's `dtype`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e1f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "input_X1 = input_X1.to(torch.float32)\n",
    "target_Y2 = target_Y2.to(torch.float32)\n",
    "\n",
    "objects_ini = {'input_X1': input_X1, 'target_Y2': target_Y2}\n",
    "\n",
    "print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562ba64",
   "metadata": {},
   "source": [
    "#### Model, Loss and Optimizer\n",
    "- create the NN model\n",
    "\n",
    "<br>\n",
    "\n",
    "- define the **loss function** to use\n",
    "    - `torch.nn.MSELoss`: mean squared error (a.k.a., Loss2; L2)\n",
    "        - https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss\n",
    "\n",
    "<br>\n",
    "\n",
    "- define the **optimizing function** (i.e., `optim.SGD`) for adjusting the weights and biases\n",
    "    - Optimization overview: https://pytorch.org/docs/stable/optim.html#module-torch.optim\n",
    "    - Available algorithms: https://pytorch.org/docs/stable/optim.html#algorithms\n",
    "        - **gradient decent**: https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n",
    "     \n",
    "##### Coding concept: assigning functions to variables\n",
    "For example: `loss_function = torch.nn.MSELoss()`\n",
    "\n",
    "Why do this?\n",
    "- Changing code's behavior: reassign the variable to a different function (e.g., explore different ideas)\n",
    "- Abstraction: abstract away the specific implementation details\n",
    "    - more readable\n",
    "    - more modular\n",
    "    - easier to maintain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6913bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(input_size, hidden_size, output_size)\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b1f0e5",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "- `zero_grad()`: reset the gradients of all optimized tensors\n",
    "    - https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html\n",
    "    - this is the same concept as above when we used `torch.Tensor.zero` in the basic example\n",
    "        - this is necessary since `.backward()` accumulates the gradients each time it is called\n",
    " \n",
    "- `torch.optim.Optimizer.step`: perform an **optimization step** based on the **current gradients** (stored in `.grad`), which is coming from **`.backward()`** \n",
    "    - https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2409575",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    output_Y2 = model(input_X1)\n",
    "\n",
    "    loss = loss_function(output_Y2, target_Y2)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimization: update weights and biases\n",
    "    optimizer.step()\n",
    "    \n",
    "# Final outputs, weights and biases\n",
    "print(f'\\nFinal Output: \\n {output_Y2}\\n')\n",
    "objects_ini = model.state_dict()\n",
    "\n",
    "print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ea05a",
   "metadata": {},
   "source": [
    "Notice the shapes of the weights - they are not yet transposed as done in the above basic example.\n",
    "\n",
    "#### Summary of Advance Example:\n",
    "- A class (like a blueprint) and `nn.Module`: a structured PyTorch approach for defining a neural network (e.g., better organization and code reusability)\n",
    "- Built-in Activation: `torch.nn.ReLU`\n",
    "- Built-in Loss: `torch.nn.MSELoss` for mean squared loss (i.e., Loss2; L2)\n",
    "- Built-in Optimizer: `optim.SGD` for gradient descent and usage of `.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2921845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional.regression import r2_score\n",
    "from torcheval.metrics import R2Score\n",
    "metric = R2Score()\n",
    "metric.update(input, target)\n",
    "metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
