{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e3d087-efd5-423a-a353-c94db02dedc7",
   "metadata": {},
   "source": [
    "<div class=\"alert block alert-info alert\">\n",
    "\n",
    "# <center> Scientific Programming in Python\n",
    "\n",
    "## <center>Karl N. Kirschner<br>Bonn-Rhein-Sieg University of Applied Sciences<br>Sankt Augustin, Germany\n",
    "\n",
    "# <center> PyTorch: Simple Neural Network Example\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0bf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib # for the version\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffae47c-a09b-4924-aaa3-8875eadaa612",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'matplotlib v.: {matplotlib.__version__}')\n",
    "print(f'NumPy v.: {np.__version__}')\n",
    "print(f'Torch v.: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c15d01d",
   "metadata": {},
   "source": [
    "This lecture will parallel the feed-forward NN (i.e., a multi-layer perceptron) example written using NumPy, allowing you to compare the approaches directly.\n",
    "\n",
    "<br>\n",
    "\n",
    "<figure>\n",
    "  <center>\n",
    "    <img src=\"00_images/31_machine_learning/nn_perceptron_example_nodes.png\" style=\"width: 500px; margin: 0 0px;\"/><br>\n",
    "    <img src=\"00_images/31_machine_learning/nn_perceptron_example.png\" style=\"width: 1000px; margin: 0 0px;\"/>\n",
    "    <figcaption style=\"margin-top: 10px; color: black; font-style: italic;\">\n",
    "          <b>Figure 1</b>: Illustration of the NN that we will create (top). The NumPy details of this NN (bottom).<br>\n",
    "    </figcaption>\n",
    "  </center>\n",
    "</figure>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Terminology for describing a neural network:\n",
    "- <font color='DodgerBlue'>**Width**</font>: number of nodes in a specific layer\n",
    "- <font color='DodgerBlue'>**Depth**</font>: number of layers in a neural network\n",
    "- <font color='DodgerBlue'>**Architecture**</font>: specific arrangement of the layers and nodes within the network, and their connectivity.\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "### Normalization\n",
    "\n",
    "Normally, with <font color='dodgerblue'>real-world data</font>, one often should <font color='DodgerBlue'>**normalize**</font> (e.g., **transpose** the date to a range [0, 1]) or <font color='DodgerBlue'>scale</font> the <font color='DodgerBlue'>input data</font>. This helps the mathematics when different input features have **large magnitude differences** (e.g., 1.5 and 2.5e6).\n",
    "- https://en.wikipedia.org/wiki/Normalization_(statistics)\n",
    "- `sklearn.preprocessing.normalize`: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html\n",
    "\n",
    "The goal is to ensure that no single feature dominates the others due to its magnitude.\n",
    "\n",
    "**Normalizing** transforms the data to a standard scale, typically between 0 and 1.\n",
    "- adjust each feature's values based on their minimum and maximum values.\n",
    "- mathematically, there are multiple approaches for this\n",
    "    - <font color='DodgerBlue'>Minimum-Maximum</font> (a.k.a. rescaling): $\\Large\\mathbf{x' = \\frac{x − x_{min}}{x_{max} − x_{min}}}$\n",
    "    - <font color='DodgerBlue'>Absolute Maximum</font>: $\\Large\\mathbf{x' = \\frac{x}{∣x_{max}∣}}$\n",
    "    - <font color='DodgerBlue'>Mean</font>: $\\Large\\mathbf{x' = \\frac{x − \\bar{x}}{x_{max} − x_{min}}}$ centers the data about the mean, with a range from [-1, 1].\n",
    "    - <font color='DodgerBlue'>Z-score</font> (a.k.a Standardization): $\\Large\\mathbf{x' = \\frac{x − \\bar{x}}{\\sigma}}$ ($\\sigma$ is the standard deviation), good for when original data follows a normal distribution \n",
    "    - <font color='DodgerBlue'>Log</font>: used to reduce the effects of extreme values\n",
    "\n",
    "<br>\n",
    "\n",
    "**Sources**:\n",
    "- https://www.geeksforgeeks.org/normalization-and-scaling/\n",
    "- https://en.wikipedia.org/wiki/Feature_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed15429-8ca1-4934-8b33-e6c8e0832e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f615f0f-931c-4319-8feb-5ea36eb33127",
   "metadata": {},
   "source": [
    "Generate some data that we can normalize/scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e66c5f5-f7d4-4b58-8112-51674daacf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = np.linspace(100.0, 140.0, 5)\n",
    "print(example_data.shape)\n",
    "example_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d201a15b-cdba-412d-90f5-9fddc95031e7",
   "metadata": {},
   "source": [
    "This is a 1D array. However, `preprocessing.normalize` requires 2D arrays.\n",
    "\n",
    "`reshape` the array:\n",
    "- https://numpy.org/doc/stable/reference/generated/numpy.reshape.html\n",
    "    - `-1`: the value is inferred from the length of the array and remaining dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141156d5-8cbe-4718-b875-172225e85641",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = example_data.reshape(-1, 1)\n",
    "print(example_data.shape)\n",
    "example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d9677-688b-4157-a98f-5b1f4f8a3aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data_norm = preprocessing.normalize(example_data, norm='max', axis=0)\n",
    "print(example_data_norm.shape)\n",
    "example_data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c169ec-adab-4c01-b186-7f4fe0c216d7",
   "metadata": {},
   "source": [
    "**Side Note**: We can\n",
    "- reproduce what `preprocessing.normalize` manually, and\n",
    "- reverse the data back to the unnormalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da2e7d-9f56-49d3-b160-22695e15f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_data/np.max(example_data))\n",
    "print()\n",
    "print(example_data_norm*np.max(example_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b75bd67",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "### Helper Print Function\n",
    "Create a helper function that allows us to investigate the different arrays that are used below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_array_specs(in_arrays: dict):\n",
    "    ''' Helper function for nicely printing NumPy and\n",
    "        PyTorch arrays.\n",
    "\n",
    "        Print: a) shape, b) data type, and\n",
    "               c) values.\n",
    "    '''\n",
    "    for key, value in in_arrays.items():\n",
    "        print(f'{key}:\\n{value.shape}, {value.dtype}')\n",
    "        print(f'{value}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cbd5f2",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "## Basic PyTorch Example (showing some of the details)\n",
    "\n",
    "#### Define the toy data (input values, target values and initial weights):\n",
    "\n",
    "##### A reminder from the NumPy lecture\n",
    "\n",
    "A random **seed** will be **explicitly set**, allowing for **reproducible results** (i.e., for teaching purposes). The first epoch data generated below should correspond to the numeric values given in the figure above.\n",
    "\n",
    "The object naming will also be done to parallel the figure above.\n",
    "\n",
    "Random Number Generator in NumPy:\n",
    "- `np.random.default_rng`: https://numpy.org/doc/stable/reference/random/generator.html\n",
    "- `numpy.random.Generator.normal`: https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dcc959-1453-43c4-b54e-e2edd6b3a01a",
   "metadata": {},
   "source": [
    "<font color='dodgerblue'>Should the data be normalized?</font>\n",
    "\n",
    "In this example, we don't need to worry about normalizing since we generate the toy data that has the same magnitude using NumPy's random number generator.\n",
    "\n",
    "- `input_X1_np = rng.normal(size=(2, 10))`\n",
    "    - `2`: **number** of <font color='dodgerblue'>**input**</font> data **samples** (e.g., 2 houses)\n",
    "    - `10`: **number of features** (width) that describe each sample (e.g., number of rooms, size, etc.)\n",
    "\n",
    "<br>\n",
    "\n",
    "- `target_Y2_np = rng.normal(size=(2, 1))`\n",
    "    - `2`: **number** of <font color='dodgerblue'>**output** data **samples**</font> (i.e., **every input sample needs an output**)\n",
    "    - `1`: number of **predicted features** (width) (e.g., house price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b00bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=12345)\n",
    "\n",
    "input_X1_np = rng.normal(size=(2, 10))\n",
    "target_Y2_np = rng.normal(size=(2, 1))\n",
    "\n",
    "weight_W1_np = rng.normal(size=(10, 3))\n",
    "weight_W2_np = rng.normal(size=(3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0682b4",
   "metadata": {},
   "source": [
    "Examine the different NumPy arrays:\n",
    "- shapes (important for matrix multiplication)\n",
    "- data types (need to be same types)\n",
    "- values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a136911",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_ini = {'input_X1': input_X1_np, 'target_Y2': target_Y2_np,\n",
    "               'weight_W1': weight_W1_np, 'weight_W2': weight_W2_np}\n",
    "\n",
    "print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63486b13",
   "metadata": {},
   "source": [
    "#### Initialize import parameters\n",
    "\n",
    "**Neural Network Specification**\n",
    "- <font color='DodgerBlue'>**input_width**</font>: how many **features** (i.e., nodes) are in each **data sample**  within the **input layer**\n",
    "    - 10 features that describe the input data \n",
    "- <font color='DodgerBlue'>**hidden_width**</font>: how many **learned features** are within the **hidden layer**\n",
    "    - 3 learned features\n",
    "- <font color='DodgerBlue'>**output_width**</font>: how many **features** are within the **output layer**\n",
    "    - 1 feature that is predicted from the 10 input features\n",
    "\n",
    "**Training Parameters**\n",
    "- <font color='DodgerBlue'>**learning_rate**</font>: **step size** for **gradient descent**\n",
    "- <font color='DodgerBlue'>**num_epochs**</font>: how many **training epochs** to **run** (instead of having a convergence cutoff criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11050373",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width = 10\n",
    "hidden_width = 3\n",
    "output_width = 1\n",
    "\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf02001",
   "metadata": {},
   "source": [
    "### Now Focus on PyTorch\n",
    "\n",
    "##### Prepare data\n",
    "\n",
    "- The <font color='dodgerblue'>NumPy-generated input</font> arrays need to be <font color='dodgerblue'>converted to torch tensors</font> using **`torch.from_numpy()`**.\n",
    "\n",
    "- <font color='dodgerblue'>Including biases</font> - these will be used in the <font color='dodgerblue'>linear transform</font> (e.g., **`torch.matmul(input_X1, weight_W1) + bias_B1`**).\n",
    "\n",
    "- Care must be given to specify that **`torch.autograd.backwards()`** should <font color='dodgerblue'>record the operations</font> for the <font color='dodgerblue'>weights and biases</font> (i.e., **calculation history**), using **`requires_grad_(requires_grad=True)`**.\n",
    "    - Reminder: <font color='dodgerblue'>only the weights and biases</font> need to be <font color='dodgerblue'>updated</font>, which is done based on the <font color='dodgerblue'>loss gradient</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X1 = torch.from_numpy(input_X1_np)\n",
    "target_Y2 = torch.from_numpy(target_Y2_np)\n",
    "\n",
    "weight_W1 = torch.from_numpy(weight_W1_np).requires_grad_(requires_grad=True)\n",
    "weight_W2 = torch.from_numpy(weight_W2_np).requires_grad_(requires_grad=True)\n",
    "\n",
    "bias_B1 = torch.zeros(hidden_width, requires_grad=True)\n",
    "bias_B2 = torch.zeros(output_width, requires_grad=True)\n",
    "\n",
    "objects_ini = {'input_X1': input_X1, 'target_Y2': target_Y2,\n",
    "               'weight_W1': weight_W1, 'input_B1': bias_B1,\n",
    "               'weight_W2': weight_W2, 'input_B2': bias_B2}\n",
    "\n",
    "print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479506d3",
   "metadata": {},
   "source": [
    "#### Model Training\n",
    "\n",
    "**Multiplying two matrices** (dot product/matrix multiplication):\n",
    "- `torch.matmul(mat1, mat2)`\n",
    "    - <font color='dodgerblue'>versatile</font>: $[matrix]\\times[matrix]$, $[matrix]\\times(vector)$, and $(vector)\\times(vector)$ operations\n",
    "        - (advance: see `broadcasting` for more info - https://www.geeksforgeeks.org/understanding-broadcasting-in-pytorch)\n",
    "    - https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
    "\n",
    "<br>\n",
    "\n",
    "**Element-wise Multiplication** (e.g., <font color='dodgerblue'>multiplying a float</font> and a <font color='dodgerblue'>matrix</font>):\n",
    "- `torch.mul(input, other)`\n",
    "    - `input`: tensor\n",
    "    - `other`: tensor or number\n",
    "    - https://pytorch.org/docs/stable/generated/torch.mul.html\n",
    "\n",
    "- Could also use `*`\n",
    "\n",
    "Both functions are demonstrated below. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Further Explanations**\n",
    "- `activation = torch.nn.ReLU()`:\n",
    "    - specify a <font color='dodgerblue'>**callable object**</font> (i.e., `activation`) for the <font color='dodgerblue'>ReLU</font> activation function\n",
    "    - https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "\n",
    "<br>\n",
    "\n",
    "- `torch.autograd.backward`:\n",
    "    - a major <font color='dodgerblue'>**workhorse**</font> in PyTorch\n",
    "    - computes the **gradient** (<font color='dodgerblue'>during the backward pass</font>) in the **entire neural network** for objects that have **`requires_grad=True`**\n",
    "    - https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html\n",
    "    - https://www.geeksforgeeks.org/python-pytorch-backward-function\n",
    "\n",
    "<br>\n",
    "\n",
    "- `with torch.no_grad()`:\n",
    "    - required because the weights and biases require grad\n",
    "    - <font color='dodgerblue'>Reduce memory consumption</font> for computations versus those that have `requires_grad=True` \n",
    "    - If you tried to assign `weight_W1`, `bias_B1`, `weight_W2` and `bias_B2` without using `with torch.no_grad()`, the following error would occur:\n",
    "        - `RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.`\n",
    "    - https://pytorch.org/docs/stable/generated/torch.no_grad.html\n",
    "\n",
    "<br>\n",
    "\n",
    "- `torch.grad.zero_()`:\n",
    "    - fills a tensor with zeros\n",
    "    - If this was **not done**, the gradients <font color='dodgerblue'>would be accumulated</font> during `.backwards()`\n",
    "        - <font color='dodgerblue'>Why would the be incorrect?</font> The **previous** iterations' gradients would be **added** to the **current** computed **gradients**.\n",
    "    - The **`_`** indicates an **`inplace`** operation (like what we know from Pandas)\n",
    "    - https://pytorch.org/docs/stable/generated/torch.Tensor.zero_.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a4ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    X2 = torch.matmul(input_X1, weight_W1) + bias_B1\n",
    "\n",
    "    activation = torch.nn.ReLU()\n",
    "    Y1 = activation(X2)\n",
    "\n",
    "    output_Y2 = torch.matmul(Y1, weight_W2) + bias_B2\n",
    "\n",
    "    loss = torch.mean(torch.square(torch.subtract(output_Y2, target_Y2))) # mean( (Y2 - y_target)^2 )\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimization: update weights and biases, don't record operations\n",
    "    with torch.no_grad():\n",
    "        weight_W1 -= torch.mul(learning_rate, weight_W1.grad)\n",
    "        bias_B1 -= torch.mul(learning_rate, bias_B1.grad)\n",
    "        weight_W2 -= learning_rate * weight_W2.grad\n",
    "        bias_B2 -= learning_rate * bias_B2.grad\n",
    "\n",
    "        # Reset the gradients to zero\n",
    "        weight_W1.grad.zero_()\n",
    "        bias_B1.grad.zero_()\n",
    "        weight_W2.grad.zero_()\n",
    "        bias_B2.grad.zero_()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.item():.3f}')\n",
    "\n",
    "    # objects_ini = {'weight_W1': weight_W1, 'bias_B1': bias_B1,\n",
    "    #            'weight_W2': weight_W2, 'bias_B2': bias_B2}\n",
    "    # print()\n",
    "    # print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc52857",
   "metadata": {},
   "source": [
    "#### Summary of Basic Example:\n",
    "- <font color='dodgerblue'>Tensor creation</font>: Using PyTorch's `from_numpy()` and `zeros()`\n",
    "- <font color='dodgerblue'>backward (autograd)</font>: Will know what differentiation to compute based on objects with `requires_grad_()`\n",
    "- Matrix operations:\n",
    "    - <font color='dodgerblue'>Matrix multiplication</font> - (`torch.matmul`)\n",
    "    - <font color='dodgerblue'>Element-wise Multiplication</font> (e.g., multiplying a float and a matrix) - (`torch.mul`)\n",
    "- <font color='dodgerblue'>Activation functions</font>: Implementing a **ReLU** activation function\n",
    "- <font color='dodgerblue'>Gradients</font>: **All** computed in **one function call** of `backward()`\n",
    "- <font color='dodgerblue'>Loss function</font>: Calculating **mean squared error loss** (manually encoded)\n",
    "- <font color='dodgerblue'>Optimization</font>: Performing **manual gradient descent** (manually encoded)\n",
    "- <font color='dodgerblue'>Reset</font> the weight and bias <font color='dodgerblue'>gradients</font>: PyTorch's `.grad.zero_()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b19041",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "## Advance Example (How it is actually done)\n",
    "\n",
    "Create the same neural network, but now make it even better (readable, K.I.S.S., reusable) using PyTorch:\n",
    "\n",
    "**Note**: Because of the Numpy neural network **lecture** and the **above example**, we can **understand** what is happening **\"under-the-hood\"** in the following.\n",
    "\n",
    "- uses `torch.nn`: **modules/functions** for **building** a **neural networks**\n",
    "    - https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "<br>\n",
    "\n",
    "- uses a <font color='dodgerblue'>**class**</font>\n",
    "    - the NN is defined as a subclass of **`nn.Module`**: the <font color='dodgerblue'>base class</font> for all <font color='dodgerblue'>neural network modules</font>\n",
    "        - https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\n",
    "        - Enables **easier organization** and **management** of **layers** and **parameters**\n",
    "    - <font color='dodgerblue'>**classes**</font> are basically a <font color='dodgerblue'>blueprint</font> that can be <font color='dodgerblue'>reused</font>\n",
    "        - contains a collection of related functions\n",
    "        - **Personal Opinon**: they are **often unnecessary** - must have a good reason to implement\n",
    "\n",
    "<br>\n",
    "\n",
    "- `torch.nn.Linear`: applies a <font color='dodgerblue'>linear transformation</font> to the <font color='dodgerblue'>incoming data</font>\n",
    "    - https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
    "    - below, `fc1` and `fc2` represent **\"<font color='dodgerblue'>f</font>ully <font color='dodgerblue'>c</font>onnected\"** <font color='dodgerblue'>layers</font> <font color='dodgerblue'>**1**</font> and <font color='dodgerblue'>**2**</font>\n",
    "    - **weights** and **biases** are <font color='dodgerblue'>**automatically initialized**</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "- `torch.nn.ReLU`: **ReLU** activation function\n",
    "    -  https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU\n",
    "\n",
    "<br>\n",
    "\n",
    "- use a **built-in optimizer**\n",
    "\n",
    "#### Define the neural network\n",
    "\n",
    "##### <font color='dodgerblue'>Classes</font>\n",
    "\n",
    "Coding Convention:\n",
    "- **PascalCase** naming convention\n",
    "- type hinting\n",
    "- <font color='dodgerblue'>docstrings</font>\n",
    "- isinstance\n",
    "- **methods** separated by single blank lines\n",
    "    - <font color='dodgerblue'>methods</font> are <font color='dodgerblue'>functions</font> that <font color='dodgerblue'>**belong to a class**</font>\n",
    "    - versus **functions** (e.g., user-defined functions) that are **entirely independent**\n",
    "\n",
    "Brief description:\n",
    "- `__init__` (\"Constructor\"): initialize the attributes (variables) of the class's (e.g., `SimpleNN`) object\n",
    "- `super(SimpleNN, self).__init__()`: calls the `__init__` method (for properly initializing `SimpleNN`) of the parent class (i.e., `torch.nn.Module`).\n",
    "    - `super()` is a Python built-in function: https://docs.python.org/3/library/functions.html#super\n",
    "- `self.fc1` and `self.fc2`: create the fully linearly connected layers\n",
    "- `self.ReLU = torch.nn.ReLU()`: creates an instance of the ReLU\n",
    "- `def forward`: function for PyTorch's forward pass mechanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(torch.nn.Module):\n",
    "    ''' This class defines a simple feedforward perceptron neural\n",
    "        network with one input, one hidden and one output layer,\n",
    "        making use of a ReLU activation function.\n",
    "\n",
    "        Attributes:\n",
    "            input_width (int): number of nodes in the input layer\n",
    "            hidden_width (int): number of nodes in the hidden layer\n",
    "            output_width (int): number of nodes in the output layer\n",
    "    '''\n",
    "    def __init__(self, input_width: int, hidden_width: int, output_width: int):\n",
    "        ''' Initialize the attributes (i.e. variables).\n",
    "\n",
    "            Defines fully connected layers 1 and 2's input and output width,\n",
    "            and the activation function.\n",
    "        '''\n",
    "        if not all(isinstance(param, int) for param in [input_width, hidden_width, output_width]):\n",
    "            raise TypeError(\"All input parameters must be an integer\")\n",
    "        else:\n",
    "            super(SimpleNN, self).__init__()\n",
    "    \n",
    "            self.fc1 = torch.nn.Linear(input_width, hidden_width)\n",
    "            self.fc2 = torch.nn.Linear(hidden_width, output_width)\n",
    "            self.activate_function = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, in_data: torch.Tensor) -> torch.Tensor:\n",
    "        ''' Forward pass of the SimpleNN.\n",
    "\n",
    "            Args:\n",
    "                in_data: Input data tensor (i.e., feature data)\n",
    "\n",
    "            Returns:\n",
    "                forward_data: Output data tensor after neural network forward pass\n",
    "        '''\n",
    "        if not isinstance(in_data, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "        else:\n",
    "            forward_data = self.fc1(in_data)\n",
    "            forward_data = self.activate_function(forward_data)\n",
    "            forward_data = self.fc2(forward_data)\n",
    "\n",
    "            return forward_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed70b70",
   "metadata": {},
   "source": [
    "#### Revisiting the toy data\n",
    "Some of **PyTorch's functions require** the numbers to be **`float32`** (<font color='dodgerblue'>GPUs are optimized for these</font>).\n",
    "\n",
    "Our above **`input_X1`** and **`input_Y2`** tensors have numbers that are **`float64`**.\n",
    "- `to(torch.float32)`: changes the tensor item's **type** (i.e., `dtype`)\n",
    "\n",
    "Alter the existing data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e1f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X1 = input_X1.to(torch.float32)\n",
    "target_Y2 = target_Y2.to(torch.float32)\n",
    "\n",
    "objects_ini = {'input_X1': input_X1, 'target_Y2': target_Y2}\n",
    "print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562ba64",
   "metadata": {},
   "source": [
    "#### Model, Loss and Optimizer\n",
    "- create the <font color='dodgerblue'>NN model</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "- define the **optimizing function** (i.e., `optim.SGD`) for adjusting the **weights** and **biases**\n",
    "    - Optimization overview: https://pytorch.org/docs/stable/optim.html#module-torch.optim\n",
    "    - **Available algorithms**: https://pytorch.org/docs/stable/optim.html#algorithms\n",
    "        - **gradient decent**: https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n",
    "        - **adam**: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam\n",
    "     \n",
    "<br>\n",
    "\n",
    "- define the **loss function** to use\n",
    "    - `torch.nn.MSELoss`: <font color='dodgerblue'>mean squared error</font> (a.k.a., Loss2; L2)\n",
    "        - https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "##### Sidenote\n",
    "<font color='dodgerblue'>**Coding concept**</font>: **assigning** a **function** to a **variable**\n",
    "\n",
    "<br>\n",
    "\n",
    "For example:\n",
    "\n",
    "`loss_function = torch.nn.MSELoss()` that is given in the next code cell\n",
    "\n",
    "`self.activate_function = torch.nn.ReLU()` given in the above code\n",
    "\n",
    "<br>\n",
    "\n",
    "**Why do this?**\n",
    "\n",
    "- Quickly and easily change an overall code's behavior: **reassign** the **variable** to a **different function**\n",
    "\n",
    "    - <font color='dodgerblue'>explore different ideas</font> (e.g., different loss functions)\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Abstraction**: abstract away the specific implementation details\n",
    "    - Idea: <font color='dodgerblue'>**Focus** on the **what**, **not** the **how**</font>\n",
    "        - more <font color='dodgerblue'>readable</font>\n",
    "        - easier to understand **concepts** (e.g., the <font color='dodgerblue'>science</font>) - don't get lost in the details\n",
    "        - easier to <font color='dodgerblue'>maintain</font>\n",
    "        - can be **harder** to **understand** the **details**\n",
    " \n",
    "    - Related terms:\n",
    "        - <font color='dodgerblue'>**encapsulation**</font>: **grouping data** (information) and the **methods** (functions) that are **related** within a single unit (e.g. a class)\n",
    "        - <font color='dodgerblue'>**modularity/decomposition**</font>: **breaking down** a **large program** into **smaller**, **independent** components (e.g., **functions**)\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6913bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(input_width=input_width, hidden_width=hidden_width, output_width=output_width)\n",
    "\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_function = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b1f0e5",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "- `zero_grad()`: **set/reset** the **gradients** of all **optimized tensors** (i.e, for the **weights** and **biases**)\n",
    "    - https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html\n",
    "    - this is the <font color='dodgerblue'>same concept as above</font> when we used `torch.Tensor.zero` in the basic example\n",
    "        - this is necessary since <font color='dodgerblue'>`.backward()` accumulates the gradients</font> **each time** it is **called**\n",
    "\n",
    "<br>\n",
    "\n",
    "- `torch.optim.Optimizer.step`: perform an **optimization step** based on the **current gradients** (i.e., those stored in `.grad`), which is coming from **`.backward()`** \n",
    "    - https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2409575",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    output_Y2 = model(input_X1)\n",
    "\n",
    "    loss = loss_function(output_Y2, target_Y2)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimization: update weights and biases\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77342867-7f19-437f-81c1-0e01555cb556",
   "metadata": {},
   "source": [
    "Final output can be given in an dict using `state_dict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca28acf7-7669-49df-86d1-341020d0d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d6a01d-a2ee-4439-8007-1daed8407c9f",
   "metadata": {},
   "source": [
    "Nicely printed out, including the tensor sizes the `dtype`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d785e0b-8c1f-415d-84e6-6c4c73ecd538",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_ini = model.state_dict()\n",
    "\n",
    "print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a5e38-2277-4126-a5f8-ce5e813fbf4b",
   "metadata": {},
   "source": [
    "#### Using the trained (optimized) model\n",
    "\n",
    "Once a neural network model has been well-trained, it is ready to be used (i.e., to make predictions).\n",
    "\n",
    "**Important for science!**\n",
    "\n",
    "<font color='dodgerblue'>**Usability**</font> and <font color='dodgerblue'>**Reproducibility**</font>: how can others use the trained model?\n",
    "\n",
    "(In other words, **users** must perform a **forward pass** on your **network** to make a **prediction**.)\n",
    "\n",
    "<br>\n",
    "\n",
    "What is required is (must be given - e.g., on a GitHub repo, in a paper, etc.):\n",
    "1. the neural network architecture\n",
    "    - <font color='dodgerblue'>number of layers</font> and <font color='dodgerblue'>number of nodes</font> in each layer\n",
    "    - <font color='dodgerblue'>how the nodes are connected</font> (linearly)\n",
    "    - the <font color='dodgerblue'>activation functions</font> and their placement within the network\n",
    "    - the <font color='dodgerblue'>optmizer</font>\n",
    "3. the optimized parameters\n",
    "    - the <font color='dodgerblue'>optimized weights</font>\n",
    "    - the <font color='dodgerblue'>optimized biases</font>\n",
    "4. other parameters (i.e., called hyperparameters)\n",
    "    - <font color='dodgerblue'>learning rate</font>\n",
    "    - optimization cutoff thresholds or maximum number of epochs\n",
    "    - model-specific unique details\n",
    "\n",
    "\n",
    "**Note**: often (natural) scientists are **publishing** their developed neural network research, and **releasing** their **code**. However, they **do not include** the optimized **weights**, **biases**, and **hyperparameters** details. Consequently, if anyone wants to use their models or reproduce their work, they must **redo the training** (expensive).\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Create random new data:\n",
    "- 5 new samples\n",
    "    - **Important note**: one would never train on such a small data samples (2) and then make predictions - it is done here to simplify the teaching example.\n",
    "- 10 features in each new sample (as required by the model specification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5595c-8f9d-4127-829d-645b1312b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = torch.randn(5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9924f2f3-4d92-4744-951b-15a0d6032de3",
   "metadata": {},
   "source": [
    "Make predictions by passing the new input data to the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9f7fa-55be-4f2d-9b5c-dea08efbf220",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c58b5a5-992a-44a9-91e2-30d67a39744f",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "### Advance Concept: Creating a customized activation function\n",
    "\n",
    "- Create a **class** that contains a new activation function, which contains a **module** named `forward` (as needed by PyTorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012856b1-5e19-4f52-a705-8fc3c9803d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedRelu(torch.nn.Module):\n",
    "    ''' Modified ReLU activation function.\n",
    "\n",
    "        Class that implements a modified ReLU function that adds\n",
    "        1.0 to the input. \n",
    "\n",
    "        Attributes:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: modified ReLU activation output\n",
    "    ''' \n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the modified_relu activation function.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying the modified\n",
    "            ReLU activation.\n",
    "        \"\"\"\n",
    "        if not isinstance(input, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\") \n",
    "        else:\n",
    "            mod_relu = torch.maximum(input+1.0, torch.zeros_like(input))\n",
    "\n",
    "            ## Comment out to see how mod_relu operates on the input\n",
    "            # print(f'{input}\\n{mod_relu}')\n",
    "\n",
    "            return mod_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13aba4-2e62-4804-92dd-5e3f273a3fb5",
   "metadata": {},
   "source": [
    "Specify our neural network archetecture\n",
    "- layers\n",
    "- modified activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56941a3-41ef-40b2-9afe-e7da4a5ed82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedNN(torch.nn.Module):\n",
    "    \"\"\" Modified Neural Network.\n",
    "\n",
    "        This class defines a simple feedforward neural network with \n",
    "        one hidden layer and uses the ModifiedReLU activation function.\n",
    "\n",
    "        Attributes:\n",
    "            input_width (int): number of nodes in the input layer.\n",
    "            hidden_width (int): number of nodes in the hidden layer.\n",
    "            output_width (int): number of nodes in the output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_width: int, hidden_width: int, output_width: int):\n",
    "        ''' Initialize the attributes (i.e. variables).\n",
    "\n",
    "            Defines fully connected layers 1 and 2's input and output width,\n",
    "            and the modified activation function.\n",
    "        '''        \n",
    "        if not all(isinstance(param, int) for param in [input_width, hidden_width, output_width]):\n",
    "            raise TypeError(\"All input parameters must be an integer\")\n",
    "        else:\n",
    "            super(ModifiedNN, self).__init__()\n",
    "            \n",
    "            self.fc1 = torch.nn.Linear(input_width, hidden_width)\n",
    "            self.fc2 = torch.nn.Linear(hidden_width, output_width)\n",
    "            self.modified_relu = ModifiedRelu()\n",
    "\n",
    "    def forward(self, in_data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Forward pass of the ModifiedNN.\n",
    "\n",
    "            Args:\n",
    "                in_data: Input data tensor (i.e., features)\n",
    "\n",
    "            Returns:\n",
    "                forward_data: Output data tensor after neural network forward pass\n",
    "        \"\"\"\n",
    "        if not isinstance(in_data, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "        else:\n",
    "            forward_data = self.fc1(in_data)\n",
    "            forward_data = self.modified_relu(forward_data)\n",
    "            forward_data = self.fc2(forward_data)\n",
    "\n",
    "            return forward_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12455b-f0cd-45f4-8975-94d05cf623db",
   "metadata": {},
   "source": [
    "1. Create a new model using the `ModifiedNN` architecture\n",
    "2. Specify the type of optimizer to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b6524-dacc-4ae0-9329-cb33694e203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = ModifiedNN(input_width=input_width, hidden_width=hidden_width, output_width=output_width)\n",
    "optimizer = optim.SGD(params=new_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa0c829-9ccd-4d82-8033-4619d3f83095",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6408b2a-ea13-40b7-9c10-13317c316ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    output_Y2 = new_model(input_X1)\n",
    "\n",
    "    loss = loss_function(output_Y2, target_Y2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}: Loss = {loss.item():.3f}')\n",
    "\n",
    "print(f'\\nFinal Output: \\n {output_Y2}\\n')\n",
    "\n",
    "objects_ini = model.state_dict()\n",
    "print_array_specs(in_arrays=objects_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d32d8-d073-4c63-b79b-567f2cb5616f",
   "metadata": {},
   "source": [
    "#### Training and Testing\n",
    "Recall that in our shallow learning lecture, we discussed the concept of splitting a dataset into a training dataset and a test dataset. The same things idea is still utilized for neural networks.\n",
    "\n",
    "<hr style=\"border:2px solid gray\"></hr>\n",
    "\n",
    "#### Summary of PyTorch Example:\n",
    "- A <font color='dodgerblue'>class</font> (like a blueprint) and <font color='dodgerblue'>`nn.Module`</font>: a structured PyTorch approach for **defining a neural network**\n",
    "    - e.g., architecture, activation functions\n",
    "    - allows for easy/better organization and code reusability\n",
    "    - module vs. function\n",
    "    - PEP8 rules for naming and blank lines\n",
    "- Built-in <font color='dodgerblue'>Activation</font>: `torch.nn.ReLU`\n",
    "- Built-in <font color='dodgerblue'>Loss</font>: `torch.nn.MSELoss` for mean squared loss (i.e., Loss2; L2)\n",
    "- All <font color='dodgerblue'>gradients</font> needed in backward propagation done using `autograd.backwards()`\n",
    "- Built-in <font color='dodgerblue'>Optimizer</font>: `optim.SGD` for gradient descent and usage of `.step()`\n",
    "- Model training and using\n",
    "- Create a <font color='dodgerblue'>customized activation</font> function and implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d5a5f-8002-4495-a62d-afab4bded4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
